{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regresión Lineal\n",
    "================\n",
    "\n",
    "La regresión lineal es una técnica estadística que nos permite modelar la relación entre una variable dependiente $y$ y una o más variables independientes $x$. Es un modelo lineal, asume una relación lineal entre las variables de entrada y una única variable de salida. Esto significa que la variable de salida se puede calcular como una combinación lineal de las variables de entrada.\n",
    "\n",
    "Dependiendo de si las variables de entrada son una o más, hablamos de regresión lineal simple o regresión lineal múltiple. Y si el entrenamiento se realiza mediante minimos cuadrados ordinarios, hablamos de regresión lineal por minimos cuadrados ordinarios, o regresion por minimos cuadrados.\n",
    "\n",
    "## Representación del Modelo\n",
    "\n",
    "La regresión lineal es un modelo lineal, es decir, un modelo que asume una relación lineal entre las variables de entrada y la variable de salida. En el caso de la regresión lineal simple, el modelo lineal se puede representar como:\n",
    "\n",
    "$$y = b_0 + b_1 \\cdot x_1 + b_2 \\cdot x_2 + \\dots + b_n \\cdot x_n$$\n",
    "\n",
    "Aquí, $y$ es la variable de salida, $x_1, x_2, \\dots, x_n$ son las variables de entrada, y $b_0, b_1, b_2, \\dots, b_n$ son los coeficientes del modelo lineal. Las entradas y salidas son numéricas, y cada entrada tiene un coeficiente que es aprendido por el modelo durante la etapa de aprendizaje. Si tenemos una entrada, la representacion es una linea, si tenemos dos entradas, la representacion es un plano, y si tenemos tres o más entradas, la representacion es un hiperplano. Cuando el coeficiente de una entrada es cero, la entrada no afecta a la salida, y no es utilizada por el modelo.\n",
    "\n",
    "La representación del modelo en si, es el conjunto de coeficientes del modelo lineal.\n",
    "\n",
    "## Entrenamiento del Modelo\n",
    "\n",
    "El aprendizaje de la regresión lineal consiste en encontrar los coeficientes del modelo lineal que mejor se ajustan a los datos de entrenamiento. Para ello, es necesario estimar los coeficientes mediante los datos disponibles. En este documento veremos 4 metodos:\n",
    "\n",
    "- Regresión lineal simple\n",
    "- Minimos cuadrados ordinarios\n",
    "- Descenso de gradiente\n",
    "- Regresión lineal regularizada\n",
    "\n",
    "### Regresión Lineal Simple\n",
    "\n",
    "La regresión lineal simple es un caso especial de la regresión lineal donde solo tenemos una variable de entrada. El proceso de aprendizaje consiste en utilizar estadísticas de los datos de entrenamiento para estimar los coeficientes del modelo lineal. Para esto se utilizan el promedio, la varianza, la covarianza, la correlación y la desviación estándar, en conjunto con acceso a todos los datos de entrenamiento.\n",
    "\n",
    "### Minimos Cuadrados Ordinarios\n",
    "\n",
    "Los mínimos cuadrados ordinarios (OLS) es un método de optimización que encuentra los coeficientes del modelo lineal que minimizan la suma de los cuadrados de los residuos entre las predicciones y los valores reales. Calculamos las distancias entre cada punto y la linea, y sumamos los cuadrados de esas distancias. El objetivo es encontrar la linea que minimice esa suma. Para ello, se requiere acceso a todos los datos de entrenamiento, y la utilizacion de operaciones matriciales.\n",
    "\n",
    "### Descenso de Gradiente\n",
    "\n",
    "El descenso de gradiente es un método de optimización que encuentra los coeficientes del modelo lineal que minimizan una función de costo. En cada iteración, se calcula el gradiente de la función de costo, y se actualizan los coeficientes en la dirección opuesta al gradiente. El proceso se repite hasta que se alcanza un número máximo de iteraciones, o hasta que el gradiente es suficientemente pequeño.\n",
    "\n",
    "[Articulo sobre descenso de gradiente](01-descenso-gradiente.ipynb)\n",
    "\n",
    "### Regresión Lineal Regularizada\n",
    "\n",
    "La regresión lineal regularizada utiliza el mismo proceso de aprendizaje que la regresión lineal por minimos cuadrados ordinarios, pero también intenta reducir la complejidad del modelo. Para ello, se utilizan dos metodos distintos:\n",
    "\n",
    "- **Regresión lineal con regularización L1**: Se minimiza también la suma absoluta de los coeficientes. Este método se conoce como regularización L1, o regularización Lasso.\n",
    "- **Regresión lineal con regularización L2**: Se minimiza también la suma de los cuadrados de los coeficientes. Este método se conoce como regularización L2, o regularización Ridge.\n",
    "\n",
    "## Predicción del Modelo\n",
    "\n",
    "Una vez que el modelo ha sido entrenado, el proceso de realizar predicciones es tan sencillo como introducir los valores de las variables de entrada en el modelo lineal, y calcular el valor de la variable de salida.\n",
    "\n",
    "## Preparación de los Datos\n",
    "\n",
    "Veremos algunas guías para preparar los datos para la regresión lineal. En general, uno deberia probar cada una de estas técnicas, y ver cual funciona mejor para el problema en cuestión.\n",
    "\n",
    "- **Suposicion de linealidad**: La regresión lineal asume una relación lineal entre las variables de entrada y la variable de salida. Si esto no se cumple, el algoritmo no es aplicable. Puede llegar a ser necesario transformar los datos para que cumplan esta suposición.\n",
    "- **Remover ruido**: La regresión lineal es sensible al ruido en los datos. Puede llegar a ser necesario remover los outliers, o los datos que no siguen la tendencia general de los datos. Se recomienda limpiar los datos antes de aplicar la regresión lineal.\n",
    "- **Remover colinearidad**: La regresión lineal resultara en un modelo sobreajustado si hay variables de entrada que están altamente correlacionadas. Se recomienda remover estas variables utilizando una matriz de correlación.\n",
    "- **Distribución Gaussiana**: La regresión lineal resultara en predicciones mas confiables si las variables de entrada y la variable de salida tienen una distribución Gaussiana. Se recomienda visualizar los datos utilizando histogramas, y transformar los datos si no tienen esta distribución.\n",
    "- **Escalado de datos**: La regresión lineal puede verse afectada por la escala de los datos. Se recomienda escalar los datos utilizando una normalización o una estandarización.\n",
    "\n",
    "## Guía de Regresión Lineal Simple\n",
    "\n",
    "Veremos una guía paso a paso para aplicar la regresión lineal simple a un problema de regresión.\n",
    "\n",
    "1. **Gráfico de dispersión**: Utilizar un gráfico de dispersión para visualizar la relación entre las variables de entrada y la variable de salida. Esto nos permite ver si la relación es lineal, y si hay outliers. En caso de encontrar una relación lineal, verificamos que podemos utilizar la regresión lineal simple.\n",
    "2. **Inicializar función de estimación**: Inicializar la función de estimación siguiendo las siguientes ecuaciones:\n",
    "\n",
    "$$\\begin{align} & b_1 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x}) \\cdot (y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2} \\\\ & b_0 = \\bar{y} - b_1 \\cdot \\bar{x} \\end{align}$$\n",
    "\n",
    "3. **Estimación de error**: Estimar el error del modelo utilizando la raíz cuadrada del error cuadrático medio (RMSE). Esto nos permite comparar el error del modelo con otros modelos de regresión lineal. Esto lo calculamos utilizando la siguiente ecuación:\n",
    "\n",
    "$$\\begin{align} & RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (p_i - y_i)^2} \\\\ & p_i = (predicción\\ de\\ y\\ para\\ x_i) \\\\ & y_i = (valor\\ real\\ de\\ y\\ para\\ x_i) \\end{align}$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
