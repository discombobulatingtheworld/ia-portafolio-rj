{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descenso de Gradiente\n",
    "=====================\n",
    "\n",
    "El descenso de gradiente es un algoritmo de optimización simple que permite encontrar valores de parametros para una funcion $f$ que minimicen una funcion de costo. Se puede utilizar en conjunto con cualquier algoritmo de machine learning, pero es mejor usado cuando los parametros no pueden ser calculados analíticamente, requiriendo la utilización de un algoritmo de optimización. \n",
    "\n",
    "## Proceso\n",
    "\n",
    "El proceso esta basado en la idea de seleccionar parametros aleatorios y luego ir actualizandolos en la dirección que minimice la funcion de costo. La actualización de los parametros se realiza de la siguiente manera:\n",
    "\n",
    "1. Comenzamos por valores iniciales para todos los parametros, tomando valores iguales a 0.0 o valores aleatorios pequeños.\n",
    "2. Introducimos esos parametros en la funcion $f$ con los parametros elegidos, y calculamos el costo.\n",
    "3. Calculamos la derivada de la funcion de costo y encontramos la dirección hacia la cual esta decrece mas rapidamente.\n",
    "4. Actualizamos los parametros en la dirección encontrada, multiplicando la derivada por una tasa de aprendizaje y restando el resultado a los parametros.\n",
    "5. Repetimos el proceso hasta que la funcion de costo deje de decrecer o alcance un mínimo.\n",
    "\n",
    "Este proceso requiere conocer la funcion que buscamos optimizar, o la derivada de la funcion de costo.\n",
    "\n",
    "## Descenso de Gradiente por Lotes\n",
    "\n",
    "Un algoritmo de aprendizaje supervisado intenta encontrar una funcion $f$ que mapee entrada $X$ a salida $Y$, tanto para algoritmos de clasificación como de regresion. Algunos de estos algoritmos utilizan coeficientes para la estimación de $f$, y pueden requerir la optimización de estos coeficientes para encontrar la mejor estimación de $f$. Para esto es necesario evaluar que tan cercana es la estimación a $f$. Esto se realiza utilizando una funcion de costo, que mide la diferencia entre la estimación y la salida real. Luego calculamos la suma o el promedio del error y lo utilizamos como el costo de la estimación. Aplicamos este proceso a todos los coeficientes de la funcion y obtenemos la derivada para cada uno de ellos. Luego actualizamos los coeficientes con el proceso anteriormente descrito. \n",
    "\n",
    "En cada iteración del proceso, procesamos todos los ejemplos en el conjunto de datos de entrenamiento, hasta que el costo deje de decrecer o alcance un mínimo. Este proceso es llamado descenso de gradiente por lotes, ya que los coeficientes son actualizados basados en el error de todos los ejemplos en el conjunto de datos de entrenamiento.\n",
    "\n",
    "## Descenso de Gradiente Estocástico\n",
    "\n",
    "A medida crece el conjunto de entrenamiento, el costo computacional del descenso de gradiente crece, debido a que debe calcular el costo de cada ejemplo en el conjunto de datos de entrenamiento para cada iteración. Una alternativa es el actualizar los coeficientes luego de cada ejemplo en el conjunto de datos de entrenamiento.\n",
    "\n",
    "Este proceso es llamado descenso de gradiente estocástico debido a que los ejemplos del conjunto de datos de entrenamiento son procesados en orden aleatorio. Cada ejemplo es procesado individualmente, y el costo es calculado para cada ejemplo. Los coeficientes son actualizados basados en el error de cada ejemplo. Este proceso suele obtener buenos resultados luego de pocas iteraciones por el conjunto completo.\n",
    "\n",
    "## Consejos para el Uso\n",
    "\n",
    "- Graficar el costo a lo largo de las iteraciones para asegurarse que esta decreciendo. Si no lo hace, reducir la tasa de aprendizaje.\n",
    "- Probar tasas de aprendizaje variadas, como 0.1, 0.01, 0.001, 0.0001, etc. Si la tasa de aprendizaje es muy pequeña, el algoritmo puede demorar mucho en converger. Si es muy grande, el algoritmo puede incrementar el costo.\n",
    "- Escalar los datos de entrada. El descenso de gradiente funciona mejor cuando los datos de entrada están escalados a rangos similares, como por ejemplo entre 0 y 1. De lo contrario, el algoritmo puede demorar mucho en converger.\n",
    "- El descenso de gradiente estocástico no precisa muchas pasadas por el conjunto de datos de entrenamiento completo, de 1 a 10 pasadas suele ser suficiente.\n",
    "- En el caso de utilizar el descenso de gradiente estocástico, el graficar el costo despues de cada iteración puede ser muy ruidoso. Una alternativa es graficar el costo despues de cierto intervalo de iteraciones, y tomar el promedio de los costos en ese intervalo."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
