{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesiano Ingenuo\n",
    "=================\n",
    "\n",
    "El algoritmo Bayesiano Ingenuo, o Naive Bayes, es un algoritmo de aprendizaje supervisado enfocado a la clasificación. En su núcleo, es un clasificador probabilístico que se basa en el teorema de Bayes, de ahi su nombre. Utiliza las relaciones probabilisticas entre los atributos de entrada y de salida para predecir la clase de una instancia.\n",
    "\n",
    "Este algoritmo asume la independencia entre los atributos de entrada, por lo que se le llama ingenuo. A pesar de esta suposición, el algoritmo funciona lo suficiente bien en la mayoría de los casos donde se presente una ligera correlación entre los atributos. Se requiere igualmente remover aquellos atributos que estén altamente correlacionados.\n",
    "\n",
    "## Teorema de Bayes\n",
    "\n",
    "El teorema de Bayes es una ecuación que describe la relación de probabilidades condicionales. Su formulación es la siguiente:\n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A)\\cdot P(A)}{P(B)}$$\n",
    "\n",
    "Generalizando para una entrada $X$ con $n$ atributos, se tiene:\n",
    "\n",
    "$$P(Y|X) = \\frac{P(Y)\\cdot P(X_1|Y)\\cdot P(X_2|Y)\\cdot ... \\cdot P(X_n|Y)}{P(X)}$$\n",
    "\n",
    "## Representación del Modelo\n",
    "\n",
    "El modelo de Naive Bayes se representa como un conjunto de tablas de frecuencias. En esta se almacenan las probabilidades de cada atributo para cada clase de salida, asi como la probabilidad de cada clase de salida.\n",
    "\n",
    "## Entrenamiento del Modelo\n",
    "\n",
    "El entrenamiento del modelo consiste en calcular las probabilidades de cada atributo para cada clase de salida, asi como la probabilidad de cada clase de salida. Para esto, se calcula cada dato de la siguiente manera:\n",
    "\n",
    "$$\\begin{align} & P(Y) = \\frac{N_Y}{N} \\\\ & P(X_i|Y) = \\frac{N_{Y,X_i}}{N_Y} \\end{align}$$\n",
    "\n",
    "Donde $N_Y$ es el número de instancias de la clase $Y$, $N$ es el número total de instancias, y $N_{Y,X_i}$ es el número de instancias de la clase $Y$ que tienen el valor $X_i$ en el atributo $i$.\n",
    "\n",
    "Con estos datos se construyen tablas de frecuencia para cada atributo, para cada valor posible, para cada clase de salida.\n",
    "\n",
    "## Predicción del modelo\n",
    "\n",
    "Para predecir la clase de una instancia, se calcula la probabilidad de cada clase de salida para la instancia, y se elige la clase con mayor probabilidad. Esto se calcula de la siguiente manera:\n",
    "\n",
    "$$P(Y=k|X) = \\frac{P(Y=k)\\cdot P(X_1|Y=k)\\cdot P(X_2|Y=k)\\cdot ... \\cdot P(X_n|Y=k)}{P(X)}$$\n",
    "\n",
    "Donde $k$ es la clase de salida, $X$ es la instancia, $P(Y=k)$ es la probabilidad de la clase $k$, $P(X_i|Y=k)$ es la probabilidad del atributo $i$ para la clase $k$, y $P(X)$ es la probabilidad de la instancia $X$.\n",
    "\n",
    "Dadas 2 clases de salida, se calcula la probabilidad de cada clase, y se obtiene un conjunto de 2 probabilidades:\n",
    "\n",
    "$$\\begin{align} & P(Y=k_1|X) = \\frac{\\alpha}{P(X)} \\\\ & P(Y=k_2|X) = \\frac{\\beta}{P(X)} \\end{align}$$\n",
    "\n",
    "Aqui, $P(X)$ resulta ser un termino normalizador, por lo que se puede ignorar. Para obtener las probabilidades reales, se ignora $P(X)$ y se normaliza $\\alpha$ y $\\beta$ de la siguiente manera:\n",
    "\n",
    "$$\\begin{align} & P(Y=k_1|X) = \\frac{\\alpha}{\\alpha + \\beta} \\\\ & P(Y=k_2|X) = \\frac{\\beta}{\\alpha + \\beta} \\end{align}$$\n",
    "\n",
    "Habiendo obtenido las probabilidades de cada clase, se elige la clase con mayor probabilidad.\n",
    "\n",
    "## Preparación de los Datos\n",
    "\n",
    "Este algoritmo requiere ciertas características de las datos para funcionar correctamente:\n",
    "\n",
    "- **Conjunto de entrenamiento incompleto**\n",
    "\n",
    "El conjunto de entrenamiento debe contener todas las clases para todos los atributos. En el caso de un atributo no tenga una clase, el algoritmo asume que la probabilidad de ese atributo para esa clase es 0. Debido a esto, es necesario que el conjunto de entrenamiento sea representativo de la población.\n",
    "\n",
    "Es posible tolerar la ausencia de clases en el conjunto de entrenamiento, pero esto requiere de un tratamiento especial. Se debe asignar probabilidades pequeñas a las clases faltantes, y se debe tener cuidado de no asignar probabilidades de 0 a ninguna clase, para no descartarlos por completo. Realizamos esto tomando $p_1$, $p_2$, ..., $p_n$ tales que su suma sea 1, y $\\mu$ una probabilidad pequeña. Entonces, la probabilidad de cada clase es:\n",
    "\n",
    "$$P(X_i|Y) = \\frac{x_i + \\mu \\cdot p_1}{y_i + \\mu}$$\n",
    "\n",
    "Donde $x_i$ es el numerador de la probabilidad sin corrección, y $y_i$ es el denominador. Esto se llama corrección de Laplace.\n",
    "\n",
    "- **Atributos continuos**\n",
    "\n",
    "Este algoritmo, debido a su utilización de probabilidades, si es aplicado a atributos continuos, se llega al resultado de que la probabilidad de un valor concreto es 0. Esto se debe a que la probabilidad de un valor concreto es 0 en una distribución continua. Para evitar esto, se puede discretizar los atributos continuos, pero esto resulta en el problema de que una decision de discretización arbitraria puede afectar la precisión de la predicción. Para solucionar esto, se recomienda utilizar una funcion de densidad de probabilidad.\n",
    "\n",
    "Asumimos una distribución normal de los datos en la probabilidad del atributo numérico y aplicamos la función siguiente:\n",
    "\n",
    "$$f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma}}e^{\\frac{(x-\\mu)^2}{2\\sigma^2}}$$\n",
    "\n",
    "Donde $\\mu$ es la media de los datos, y $\\sigma$ es la desviación estándar.\n",
    "\n",
    "En caso de que la distribución de los datos no sea normal, se puede utilizar una función de densidad de probabilidad diferente.\n",
    "\n",
    "- **Atributos independientes**\n",
    "\n",
    "El teorema de Bayes asume que los atributos son independientes entre si. Esto no es cierto en la mayoría de los casos, pero el algoritmo funciona lo suficientemente bien en la mayoría de los casos donde se presente una ligera correlación entre los atributos. Se requiere igualmente remover aquellos atributos que estén altamente correlacionados.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
