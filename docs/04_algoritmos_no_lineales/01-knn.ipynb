{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Vecinos Más Cercanos (k-NN)\n",
    "=============================\n",
    "\n",
    "El algoritmo de los k vecinos más cercanos (k-NN) es un algoritmo de aprendizaje supervisado que intenta predecir el valor de una variable objetivo a partir de su similitud con otros datos existentes en el conjunto de entrenamiento. El algoritmo k-NN asume que los datos similares existen en proximidad cercana. En otras palabras, objetos similares están cerca unos de otros.\n",
    "\n",
    "En comparación con otros algoritmos de aprendizaje supervisado que intentan hallar la mejor aproximacion de una funcion $y = f(x)$, el algoritmo k-NN no intenta construir un modelo interno, sino que simplemente almacena instancias de entrenamiento. La clasificación de una nueva instancia se realiza en base a la similitud con las instancias almacenadas.\n",
    "\n",
    "Es un método no paramétrico que no hace suposiciones sobre la distribución de los datos.\n",
    "\n",
    "## Cercanía\n",
    "\n",
    "Existen varias formas de medida de cercanía. La idea utilizada por el algoritmo, es representar cada dato como un punto en un espacion $n$-dimensional, dada $n$ cantidad de atributos. Pero incluso utilizando esta idea surgen problemas como lo es la proximidad a varios puntos de distinta clasificación. Para esto se pone un limite a la cantidad de vecinos a considerar, y se clasifica en base a la clase mayoritaria de los $k$ vecinos más cercanos. Si este numero $k$ es par, se puede producir un empate, por lo que se suele elegir un numero impar.\n",
    "\n",
    "Para comparar la similitud entre dos puntos, pueden utilizar varios tipos de calculos de distancia. La más común es la distancia euclidiana, pero se pueden utilizar otros.\n",
    "\n",
    "### Distancia Euclidiana \n",
    "\n",
    "Dada por la siguiente ecuación:\n",
    "\n",
    "$$d(x,y) = \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2}$$\n",
    "\n",
    "Es la distancia más comúnmente utilizada, pero es sensible a la escala de los datos. Para evitar esto, se puede normalizar los datos, es decir, llevarlos a una escala común. Otras distancias que se pueden utilizar son:\n",
    "\n",
    "- **Distancia Manhattan**: \n",
    "$$d(x,y) = \\sum_{i=1}^{n}|x_i - y_i|$$\n",
    "- **Distancia Chebyshev**: \n",
    "$$d(x,y) = \\max_{i=1}^{n}|x_i - y_i|$$\n",
    "- **Distancia Minkowski**: \n",
    "$$\\begin{align} & d(x,y) = \\left(\\sum_{i=1}^{n}|x_i - y_i|^p\\right)^{1/p} \\\\ & p = 1 \\rightarrow manhattan\n",
    " \\\\ & p = 2 \\rightarrow euclidiana \\\\ & p = \\infty \\rightarrow chebyshev\\end{align}$$\n",
    "\n",
    "Una corrección que se puede hacer a la distancia euclidiana es la de considerar las instancias mas cercanas como las que tienen mayor influencia en la clasificación. Para esto se puede asignar pesos a la distancia de cada vecino, de forma que los vecinos más cercanos tengan mayor influencia en la clasificación. Una forma de hacer esto es la siguiente: consideramos que la suma de los pesos debe ser 1, y que el peso de un vecino es inversamente proporcional a su distancia. Usando caída exponencial, el peso de un vecino $i$ es:\n",
    "\n",
    "$$w_i = \\frac{e^{-d(x,x_i)}}{\\sum_{j=1}^{k}e^{-d(x,x_j)}}$$\n",
    "\n",
    "El calculo de clasificación resultante es:\n",
    "\n",
    "$$y = {max\\ class} (w_1 \\cdot y_1, \\dots, w_k \\cdot y_k)$$\n",
    "\n",
    "### Similitud por Correlación\n",
    "\n",
    "Otra forma de medir la similitud entre dos puntos es la correlación. La correlación es una medida de la relación lineal entre dos puntos. La correlación de Pearson varía entre -1 y 1, donde 1 es una correlación positiva perfecta y -1 es una correlación negativa perfecta. Una correlación de 0 indica que no hay relación entre los puntos, o que existe una relación no lineal.\n",
    "\n",
    "La correlación se calcula entre puntos, no las variables en si. Se calcula como:\n",
    "\n",
    "$$corr(X,Y) = \\frac{cov(X,Y)}{\\sigma_X \\cdot \\sigma_Y}$$\n",
    "\n",
    "Donde $cov(X,Y)$ es la covarianza entre $X$ e $Y$, y $\\sigma_X$ y $\\sigma_Y$ son las desviaciones estandar de $X$ e $Y$ respectivamente. Estas son calculadas de la siguiente forma:\n",
    "\n",
    "$$\\begin{align} & cov(X,Y) = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{n-1} \\\\ & \\sigma_X = \\sqrt{\\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}{n-1}} \\\\ & \\sigma_Y = \\sqrt{\\frac{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}{n-1}}\\end{align}$$\n",
    "\n",
    "### Coeficiente de Coincidencia Simple\n",
    "\n",
    "El coeficiente de coincidencia simple es una medida de similitud entre dos puntos cuando los atributos son binarios. Se calcula como la proporcion de aquellos atributos que coinciden entre los dos puntos.\n",
    "\n",
    "$$SMC = \\frac{m_{11} + m_{00}}{m_{11} + m_{10} + m_{01} + m_{00}}$$\n",
    "\n",
    "Donde $m_{ij}$ es la cantidad de atributos cuyo valor en el punto $X$ es $i$ y en el punto $Y$ es $j$.\n",
    "\n",
    "### Similitud de Jaccard\n",
    "\n",
    "La similitud de Jaccard es una medida de similitud entre dos puntos cuando los atributos son binarios. Se calcula de forma similar al coeficiente de coincidencia simple, pero se excluyen los atributos donde ambos puntos tienen valor 0.\n",
    "\n",
    "$$J(X,Y) = \\frac{m_{11}}{m_{11} + m_{10} + m_{01}}$$\n",
    "\n",
    "Es utilizado, por ejemplo, para medir la similitud entre documentos, donde cada atributo es una palabra, y el valor es 1 si la palabra aparece en el documento, y 0 si no, con la consideración de que dos documentos son similares si comparten palabras.\n",
    "\n",
    "### Similitud por Coseno\n",
    "\n",
    "La similitud por coseno es una medida de similitud entre dos puntos cuando los atributos son valores reales. Considera los puntos como vectores, y calcula el coseno del angulo entre ellos. Se calcula como:\n",
    "\n",
    "$$cos(X,Y) = \\frac{X \\cdot Y}{||X|| \\cdot ||Y||}$$\n",
    "\n",
    "Donde $X \\cdot Y$ es el producto punto entre los vectores $X$ e $Y$, y $||X||$ e $||Y||$ son las normas de los vectores $X$ e $Y$ respectivamente. Estas se calculan como:\n",
    "\n",
    "$$\\begin{align} & ||X|| = \\sqrt{\\sum_{i=1}^{n}x_i^2} \\\\ & ||Y|| = \\sqrt{\\sum_{i=1}^{n}y_i^2}\\end{align}$$\n",
    "\n",
    "## Representación del Modelo\n",
    "\n",
    "El algoritmo k-NN es simple de implementar, pues su representación interna es simplemente el conjunto de entrenamiento. \n",
    "\n",
    "## Entrenamiento del Modelo\n",
    "\n",
    "El algoritmo k-NN no tiene un entrenamiento propiamente dicho, pues su representación interna es simplemente el conjunto de entrenamiento.\n",
    "\n",
    "## Predicción del Modelo\n",
    "\n",
    "Para predecir la clase de un nuevo punto, se calcula la distancia entre este y todos los puntos del conjunto de entrenamiento. Luego se seleccionan los $k$ puntos más cercanos, y se clasifica el nuevo punto en base a la clase mayoritaria de estos $k$ puntos. Esto requiere primero seleccionar una medida de distancia, y luego un valor para $k$.\n",
    "\n",
    "## Preparación de los Datos\n",
    "\n",
    "Para la utilización del algoritmo k-NN, es necesario que los datos esten en una escala común, pues algunas medidas de distancia son sensibles a la escala de los datos. Para esto se recomienda normalizar los datos."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
