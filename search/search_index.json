{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"algoritmos-lineales/00-introduccion/","title":"Introducci\u00f3n","text":""},{"location":"algoritmos-lineales/00-introduccion/#introduccion","title":"Introducci\u00f3n\u00b6","text":"<p>Los algoritmos lineales de aprendizaje automatizado son aquellos que asumen una relaci\u00f3n lineal entre las variables de entrada y la variable de salida.</p> <p>Su funcionamiento se basa en aproximar la funci\u00f3n de salida como una combinaci\u00f3n lineal de las variables de entrada.</p> <p>Dependiendo del algoritmo en concreto, se pueden utilizar tanto para problemas de regresi\u00f3n como de clasificaci\u00f3n.</p>"},{"location":"algoritmos-lineales/00-introduccion/#contenidos","title":"Contenidos\u00b6","text":"<ol> <li>Descenso de Gradiente</li> <li>Regresi\u00f3n Lineal</li> <li>Regresi\u00f3n Log\u00edstica</li> <li>An\u00e1lisis Discrimitante Lineal</li> </ol>"},{"location":"algoritmos-lineales/01-descenso-gradiente/","title":"Descenso de Gradiente","text":""},{"location":"algoritmos-lineales/01-descenso-gradiente/#descenso-de-gradiente","title":"Descenso de Gradiente\u00b6","text":"<p>El descenso de gradiente es un algoritmo de optimizaci\u00f3n simple que permite encontrar valores de parametros para una funcion $f$ que minimicen una funcion de costo. Se puede utilizar en conjunto con cualquier algoritmo de machine learning, pero es mejor usado cuando los parametros no pueden ser calculados anal\u00edticamente, requiriendo la utilizaci\u00f3n de un algoritmo de optimizaci\u00f3n.</p>"},{"location":"algoritmos-lineales/01-descenso-gradiente/#proceso","title":"Proceso\u00b6","text":"<p>El proceso esta basado en la idea de seleccionar parametros aleatorios y luego ir actualizandolos en la direcci\u00f3n que minimice la funcion de costo. La actualizaci\u00f3n de los parametros se realiza de la siguiente manera:</p> <ol> <li>Comenzamos por valores iniciales para todos los parametros, tomando valores iguales a 0.0 o valores aleatorios peque\u00f1os.</li> <li>Introducimos esos parametros en la funcion $f$ con los parametros elegidos, y calculamos el costo.</li> <li>Calculamos la derivada de la funcion de costo y encontramos la direcci\u00f3n hacia la cual esta decrece mas rapidamente.</li> <li>Actualizamos los parametros en la direcci\u00f3n encontrada, multiplicando la derivada por una tasa de aprendizaje y restando el resultado a los parametros.</li> <li>Repetimos el proceso hasta que la funcion de costo deje de decrecer o alcance un m\u00ednimo.</li> </ol> <p>Este proceso requiere conocer la funcion que buscamos optimizar, o la derivada de la funcion de costo.</p>"},{"location":"algoritmos-lineales/01-descenso-gradiente/#descenso-de-gradiente-por-lotes","title":"Descenso de Gradiente por Lotes\u00b6","text":"<p>Un algoritmo de aprendizaje supervisado intenta encontrar una funcion $f$ que mapee entrada $X$ a salida $Y$, tanto para algoritmos de clasificaci\u00f3n como de regresion. Algunos de estos algoritmos utilizan coeficientes para la estimaci\u00f3n de $f$, y pueden requerir la optimizaci\u00f3n de estos coeficientes para encontrar la mejor estimaci\u00f3n de $f$. Para esto es necesario evaluar que tan cercana es la estimaci\u00f3n a $f$. Esto se realiza utilizando una funcion de costo, que mide la diferencia entre la estimaci\u00f3n y la salida real. Luego calculamos la suma o el promedio del error y lo utilizamos como el costo de la estimaci\u00f3n. Aplicamos este proceso a todos los coeficientes de la funcion y obtenemos la derivada para cada uno de ellos. Luego actualizamos los coeficientes con el proceso anteriormente descrito.</p> <p>En cada iteraci\u00f3n del proceso, procesamos todos los ejemplos en el conjunto de datos de entrenamiento, hasta que el costo deje de decrecer o alcance un m\u00ednimo. Este proceso es llamado descenso de gradiente por lotes, ya que los coeficientes son actualizados basados en el error de todos los ejemplos en el conjunto de datos de entrenamiento.</p>"},{"location":"algoritmos-lineales/01-descenso-gradiente/#descenso-de-gradiente-estocastico","title":"Descenso de Gradiente Estoc\u00e1stico\u00b6","text":"<p>A medida crece el conjunto de entrenamiento, el costo computacional del descenso de gradiente crece, debido a que debe calcular el costo de cada ejemplo en el conjunto de datos de entrenamiento para cada iteraci\u00f3n. Una alternativa es el actualizar los coeficientes luego de cada ejemplo en el conjunto de datos de entrenamiento.</p> <p>Este proceso es llamado descenso de gradiente estoc\u00e1stico debido a que los ejemplos del conjunto de datos de entrenamiento son procesados en orden aleatorio. Cada ejemplo es procesado individualmente, y el costo es calculado para cada ejemplo. Los coeficientes son actualizados basados en el error de cada ejemplo. Este proceso suele obtener buenos resultados luego de pocas iteraciones por el conjunto completo.</p>"},{"location":"algoritmos-lineales/01-descenso-gradiente/#consejos-para-el-uso","title":"Consejos para el Uso\u00b6","text":"<ul> <li>Graficar el costo a lo largo de las iteraciones para asegurarse que esta decreciendo. Si no lo hace, reducir la tasa de aprendizaje.</li> <li>Probar tasas de aprendizaje variadas, como 0.1, 0.01, 0.001, 0.0001, etc. Si la tasa de aprendizaje es muy peque\u00f1a, el algoritmo puede demorar mucho en converger. Si es muy grande, el algoritmo puede incrementar el costo.</li> <li>Escalar los datos de entrada. El descenso de gradiente funciona mejor cuando los datos de entrada est\u00e1n escalados a rangos similares, como por ejemplo entre 0 y 1. De lo contrario, el algoritmo puede demorar mucho en converger.</li> <li>El descenso de gradiente estoc\u00e1stico no precisa muchas pasadas por el conjunto de datos de entrenamiento completo, de 1 a 10 pasadas suele ser suficiente.</li> <li>En el caso de utilizar el descenso de gradiente estoc\u00e1stico, el graficar el costo despues de cada iteraci\u00f3n puede ser muy ruidoso. Una alternativa es graficar el costo despues de cierto intervalo de iteraciones, y tomar el promedio de los costos en ese intervalo.</li> </ul>"},{"location":"algoritmos-lineales/02-regresion-lineal/","title":"Regresi\u00f3n Lineal","text":""},{"location":"algoritmos-lineales/02-regresion-lineal/#regresion-lineal","title":"Regresi\u00f3n Lineal\u00b6","text":"<p>La regresi\u00f3n lineal es una t\u00e9cnica estad\u00edstica que nos permite modelar la relaci\u00f3n entre una variable dependiente $y$ y una o m\u00e1s variables independientes $x$. Es un modelo lineal, asume una relaci\u00f3n lineal entre las variables de entrada y una \u00fanica variable de salida. Esto significa que la variable de salida se puede calcular como una combinaci\u00f3n lineal de las variables de entrada.</p> <p>Dependiendo de si las variables de entrada son una o m\u00e1s, hablamos de regresi\u00f3n lineal simple o regresi\u00f3n lineal m\u00faltiple. Y si el entrenamiento se realiza mediante minimos cuadrados ordinarios, hablamos de regresi\u00f3n lineal por minimos cuadrados ordinarios, o regresion por minimos cuadrados.</p>"},{"location":"algoritmos-lineales/02-regresion-lineal/#representacion-del-modelo","title":"Representaci\u00f3n del Modelo\u00b6","text":"<p>La regresi\u00f3n lineal es un modelo lineal, es decir, un modelo que asume una relaci\u00f3n lineal entre las variables de entrada y la variable de salida. En el caso de la regresi\u00f3n lineal simple, el modelo lineal se puede representar como:</p> <p>$$y = b_0 + b_1 \\cdot x_1 + b_2 \\cdot x_2 + \\dots + b_n \\cdot x_n$$</p> <p>Aqu\u00ed, $y$ es la variable de salida, $x_1, x_2, \\dots, x_n$ son las variables de entrada, y $b_0, b_1, b_2, \\dots, b_n$ son los coeficientes del modelo lineal. Las entradas y salidas son num\u00e9ricas, y cada entrada tiene un coeficiente que es aprendido por el modelo durante la etapa de aprendizaje. Si tenemos una entrada, la representacion es una linea, si tenemos dos entradas, la representacion es un plano, y si tenemos tres o m\u00e1s entradas, la representacion es un hiperplano. Cuando el coeficiente de una entrada es cero, la entrada no afecta a la salida, y no es utilizada por el modelo.</p> <p>La representaci\u00f3n del modelo en si, es el conjunto de coeficientes del modelo lineal.</p>"},{"location":"algoritmos-lineales/02-regresion-lineal/#entrenamiento-del-modelo","title":"Entrenamiento del Modelo\u00b6","text":"<p>El aprendizaje de la regresi\u00f3n lineal consiste en encontrar los coeficientes del modelo lineal que mejor se ajustan a los datos de entrenamiento. Para ello, es necesario estimar los coeficientes mediante los datos disponibles. En este documento veremos 4 metodos:</p> <ul> <li>Regresi\u00f3n lineal simple</li> <li>Minimos cuadrados ordinarios</li> <li>Descenso de gradiente</li> <li>Regresi\u00f3n lineal regularizada</li> </ul>"},{"location":"algoritmos-lineales/02-regresion-lineal/#regresion-lineal-simple","title":"Regresi\u00f3n Lineal Simple\u00b6","text":"<p>La regresi\u00f3n lineal simple es un caso especial de la regresi\u00f3n lineal donde solo tenemos una variable de entrada. El proceso de aprendizaje consiste en utilizar estad\u00edsticas de los datos de entrenamiento para estimar los coeficientes del modelo lineal. Para esto se utilizan el promedio, la varianza, la covarianza, la correlaci\u00f3n y la desviaci\u00f3n est\u00e1ndar, en conjunto con acceso a todos los datos de entrenamiento.</p>"},{"location":"algoritmos-lineales/02-regresion-lineal/#minimos-cuadrados-ordinarios","title":"Minimos Cuadrados Ordinarios\u00b6","text":"<p>Los m\u00ednimos cuadrados ordinarios (OLS) es un m\u00e9todo de optimizaci\u00f3n que encuentra los coeficientes del modelo lineal que minimizan la suma de los cuadrados de los residuos entre las predicciones y los valores reales. Calculamos las distancias entre cada punto y la linea, y sumamos los cuadrados de esas distancias. El objetivo es encontrar la linea que minimice esa suma. Para ello, se requiere acceso a todos los datos de entrenamiento, y la utilizacion de operaciones matriciales.</p>"},{"location":"algoritmos-lineales/02-regresion-lineal/#descenso-de-gradiente","title":"Descenso de Gradiente\u00b6","text":"<p>El descenso de gradiente es un m\u00e9todo de optimizaci\u00f3n que encuentra los coeficientes del modelo lineal que minimizan una funci\u00f3n de costo. En cada iteraci\u00f3n, se calcula el gradiente de la funci\u00f3n de costo, y se actualizan los coeficientes en la direcci\u00f3n opuesta al gradiente. El proceso se repite hasta que se alcanza un n\u00famero m\u00e1ximo de iteraciones, o hasta que el gradiente es suficientemente peque\u00f1o.</p> <p>Articulo sobre descenso de gradiente</p>"},{"location":"algoritmos-lineales/02-regresion-lineal/#regresion-lineal-regularizada","title":"Regresi\u00f3n Lineal Regularizada\u00b6","text":"<p>La regresi\u00f3n lineal regularizada utiliza el mismo proceso de aprendizaje que la regresi\u00f3n lineal por minimos cuadrados ordinarios, pero tambi\u00e9n intenta reducir la complejidad del modelo. Para ello, se utilizan dos metodos distintos:</p> <ul> <li>Regresi\u00f3n lineal con regularizaci\u00f3n L1: Se minimiza tambi\u00e9n la suma absoluta de los coeficientes. Este m\u00e9todo se conoce como regularizaci\u00f3n L1, o regularizaci\u00f3n Lasso.</li> <li>Regresi\u00f3n lineal con regularizaci\u00f3n L2: Se minimiza tambi\u00e9n la suma de los cuadrados de los coeficientes. Este m\u00e9todo se conoce como regularizaci\u00f3n L2, o regularizaci\u00f3n Ridge.</li> </ul>"},{"location":"algoritmos-lineales/02-regresion-lineal/#prediccion-del-modelo","title":"Predicci\u00f3n del Modelo\u00b6","text":"<p>Una vez que el modelo ha sido entrenado, el proceso de realizar predicciones es tan sencillo como introducir los valores de las variables de entrada en el modelo lineal, y calcular el valor de la variable de salida.</p>"},{"location":"algoritmos-lineales/02-regresion-lineal/#preparacion-de-los-datos","title":"Preparaci\u00f3n de los Datos\u00b6","text":"<p>Veremos algunas gu\u00edas para preparar los datos para la regresi\u00f3n lineal. En general, uno deberia probar cada una de estas t\u00e9cnicas, y ver cual funciona mejor para el problema en cuesti\u00f3n.</p> <ul> <li>Suposicion de linealidad: La regresi\u00f3n lineal asume una relaci\u00f3n lineal entre las variables de entrada y la variable de salida. Si esto no se cumple, el algoritmo no es aplicable. Puede llegar a ser necesario transformar los datos para que cumplan esta suposici\u00f3n.</li> <li>Remover ruido: La regresi\u00f3n lineal es sensible al ruido en los datos. Puede llegar a ser necesario remover los outliers, o los datos que no siguen la tendencia general de los datos. Se recomienda limpiar los datos antes de aplicar la regresi\u00f3n lineal.</li> <li>Remover colinearidad: La regresi\u00f3n lineal resultara en un modelo sobreajustado si hay variables de entrada que est\u00e1n altamente correlacionadas. Se recomienda remover estas variables utilizando una matriz de correlaci\u00f3n.</li> <li>Distribuci\u00f3n Gaussiana: La regresi\u00f3n lineal resultara en predicciones mas confiables si las variables de entrada y la variable de salida tienen una distribuci\u00f3n Gaussiana. Se recomienda visualizar los datos utilizando histogramas, y transformar los datos si no tienen esta distribuci\u00f3n.</li> <li>Escalado de datos: La regresi\u00f3n lineal puede verse afectada por la escala de los datos. Se recomienda escalar los datos utilizando una normalizaci\u00f3n o una estandarizaci\u00f3n.</li> </ul>"},{"location":"algoritmos-lineales/02-regresion-lineal/#guia-de-regresion-lineal-simple","title":"Gu\u00eda de Regresi\u00f3n Lineal Simple\u00b6","text":"<p>Veremos una gu\u00eda paso a paso para aplicar la regresi\u00f3n lineal simple a un problema de regresi\u00f3n.</p> <ol> <li>Gr\u00e1fico de dispersi\u00f3n: Utilizar un gr\u00e1fico de dispersi\u00f3n para visualizar la relaci\u00f3n entre las variables de entrada y la variable de salida. Esto nos permite ver si la relaci\u00f3n es lineal, y si hay outliers. En caso de encontrar una relaci\u00f3n lineal, verificamos que podemos utilizar la regresi\u00f3n lineal simple.</li> <li>Inicializar funci\u00f3n de estimaci\u00f3n: Inicializar la funci\u00f3n de estimaci\u00f3n siguiendo las siguientes ecuaciones:</li> </ol> <p>$$\\begin{align} &amp; b_1 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x}) \\cdot (y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2} \\\\ &amp; b_0 = \\bar{y} - b_1 \\cdot \\bar{x} \\end{align}$$</p> <ol> <li>Estimaci\u00f3n de error: Estimar el error del modelo utilizando la ra\u00edz cuadrada del error cuadr\u00e1tico medio (RMSE). Esto nos permite comparar el error del modelo con otros modelos de regresi\u00f3n lineal. Esto lo calculamos utilizando la siguiente ecuaci\u00f3n:</li> </ol> <p>$$\\begin{align} &amp; RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (p_i - y_i)^2} \\\\ &amp; p_i = (predicci\u00f3n\\ de\\ y\\ para\\ x_i) \\\\ &amp; y_i = (valor\\ real\\ de\\ y\\ para\\ x_i) \\end{align}$$</p>"},{"location":"algoritmos-lineales/03-regresion-logistica/","title":"Regresi\u00f3n Log\u00edstica","text":""},{"location":"algoritmos-lineales/03-regresion-logistica/#regresion-logistica","title":"Regresi\u00f3n Log\u00edstica\u00b6","text":"<p>La regresi\u00f3n log\u00edstica es un modelo de clasificaci\u00f3n utilizado para problemas de clasificaci\u00f3n de dos clases. En su centro tiene a la funci\u00f3n log\u00edstica, tambien conocida como funcion sigmoide, que es una funci\u00f3n que toma valores reales y los comprime en un rango entre 0 y 1. La funci\u00f3n log\u00edstica se define como:</p> <p>$$y = \\frac{1}{1 + e^{-x}}$$</p> <p>Donde $x$ es el valor de entrada y $y$ es el valor transformado.</p>"},{"location":"algoritmos-lineales/03-regresion-logistica/#representacion-del-modelo","title":"Representaci\u00f3n del Modelo\u00b6","text":"<p>La funci\u00f3n log\u00edstica es utlizada por el algoitmo de regresi\u00f3n log\u00edstica para predecir la probabilidad de que una instancia pertenezca a una clase en particular. Para esto se utiliza una combinaci\u00f3n lineal de las variables de entrada, y se aplica la funci\u00f3n log\u00edstica a esta combinaci\u00f3n lineal. El resultado es una funci\u00f3n que sigue la siguiente ecuaci\u00f3n:</p> <p>$$y = \\frac{\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n}{1 + e^{-(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n)}}$$</p> <p>La representaci\u00f3n del modelo de regresi\u00f3n log\u00edstica en si, es el conjunto de coeficientes $\\beta_0, \\beta_1, \\beta_2, ..., \\beta_n$ que fueron aprendidos durante el entrenamiento del modelo.</p>"},{"location":"algoritmos-lineales/03-regresion-logistica/#prediccion-de-probabilidades","title":"Predicci\u00f3n de Probabilidades\u00b6","text":"<p>La salida de la funci\u00f3n log\u00edstica es un valor entre 0 y 1, que representa la probabilidad de que una instancia pertenezca a la clase positiva. Para convertir este valor en una predicci\u00f3n binaria, se utiliza un umbral, que por defecto es 0.5. Si el valor de la funci\u00f3n log\u00edstica es mayor a 0.5, la predicci\u00f3n es 1, y si es menor a 0.5, la predicci\u00f3n es 0.</p>"},{"location":"algoritmos-lineales/03-regresion-logistica/#entrenamiento-del-modelo","title":"Entrenamiento del Modelo\u00b6","text":"<p>El entrenamiento del modelo de regresi\u00f3n log\u00edstica consiste en estimar los valores de los coeficientes $\\beta_0, \\beta_1, \\beta_2, ..., \\beta_n$. Para esto se utiliza el algoritmo de maxima verosimilitud, un algoritmo com\u00fanmente utilizado en aprendizaje automatizado, pero que realiza varias suposiciones sobre los datos. La idea es que los mejores coeficientes son aquellos que predicen valores cercanos a 1 para las instancias positivas, y valores cercanos a 0 para las instancias negativas. Aqu\u00ed se busca reducir el error en el modelo para las probabilidades que predice. Esto se implementa com\u00fanmente utilizando algoritmos de optimizaci\u00f3n como el m\u00e9todo Cuasi-Newton.</p>"},{"location":"algoritmos-lineales/03-regresion-logistica/#preparacion-de-los-datos","title":"Preparaci\u00f3n de los Datos\u00b6","text":"<p>El modelo de regresi\u00f3n log\u00edstica realiza varias suposiciones acerca de los datos, incluyendo la distribuci\u00f3n y las relaciones entre las variables. Como tal, es importante preparar los datos para que se ajusten a estas suposiciones. Esto incluye:</p> <ul> <li>Variable de salida binaria: El modelo de regresi\u00f3n log\u00edstica hace predicciones sobre la probabilidad de que una instancia pertenezca a la clase positiva.</li> <li>Remover ruido: Se asume que no existen errores en la clasificacion de las instancias en el conjunto de entrenamiento. Se recomienda remover los ejemplos que est\u00e9n mal clasificados utilizando operaciones de limpieza de datos.</li> <li>Distribuci\u00f3n Gaussiana: El modelo de regresi\u00f3n log\u00edstica es un algoritmo lineal. Resultara en mejores resultados si las variables de entrada tienen una distribuci\u00f3n Gaussiana.</li> <li>Remover entradas correlacionadas: El modelo de regresi\u00f3n log\u00edstica sufrir\u00e1 de un sobreajuste si las variables de entrada estan altamente correlacionadas. Se recomienda remover las variables que esten altamente correlacionadas entre si.</li> <li>Falla en converger: Es posible que el algoritmo de optimizaci\u00f3n no converja. Esto puede suceder si hay entradas altamente correlacionadas en los datos de entrenamiento.</li> </ul>"},{"location":"algoritmos-lineales/04-analisis-discriminante-lineal/","title":"An\u00e1lisis de Discriminante Lineal","text":""},{"location":"algoritmos-lineales/04-analisis-discriminante-lineal/#analisis-de-discriminante-lineal","title":"An\u00e1lisis de Discriminante Lineal\u00b6","text":"<p>El an\u00e1lisis de discriminante lineal (LDA) es un m\u00e9todo de clasificaci\u00f3n para problemas de multiples clases. Es una alternativa a la regresi\u00f3n log\u00edstica para 2 clases, esta siendo tambien sensible a clases demasiado separaads y conjuntos de datos peque\u00f1os.</p>"},{"location":"algoritmos-lineales/04-analisis-discriminante-lineal/#representacion-del-modelo","title":"Representaci\u00f3n del Modelo\u00b6","text":"<p>El modelo de LDA tiene una representacion muy directa. Esta compuesta por las propiedades estadisticas de cada variable, y calculadas para cada clase. Estas propiedades son la media y la varianza de la variable para cada clase.</p>"},{"location":"algoritmos-lineales/04-analisis-discriminante-lineal/#entrenamiento-del-modelo","title":"Entrenamiento del Modelo\u00b6","text":"<p>El entrenamiento del modelo LDA consiste en calcular la media y la varianza de cada variable para cada clase. Estas propiedades se calculan a partir de los datos de entrenamiento. Esto se hace partiendo de la suposicion de que los datos de entrenamiento tienen una distribucion Gaussiana (normal).</p>"},{"location":"algoritmos-lineales/04-analisis-discriminante-lineal/#prediccion-del-modelo","title":"Predicci\u00f3n del Modelo\u00b6","text":"<p>La predicci\u00f3n del modelo LDA consiste en calcular la probabilidad de que un dato pertenezca a cada clase, y asignarle la clase con mayor probabilidad. La probabilidad se calcula a partir del teorema de Bayes, intentando estimar la probabilidad de que un dato pertenezca a una clase, dadas las probabilidades de que cada valor de entrada pertenezca a cada clase.</p> <p>Podemos estimar algunos valores utilizando la funci\u00f3n de la distribucion Gaussiana. Simplificando, obtenemos la siguiente ecuaci\u00f3n:</p> <p>$$D_k(x) = x \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2 \\sigma^2} + \\log(P(k))$$</p> <p>Donde $D_k(x)$ es la funci\u00f3n discriminante para la clase $k$, $x$ es el dato de entrada, $\\mu_k$ es la media de la variable para la clase $k$, $\\sigma$ es la desviaci\u00f3n est\u00e1ndar de la variable para todas las clases, y $P(k)$ es la probabilidad de que un dato pertenezca a la clase $k$.</p>"},{"location":"algoritmos-lineales/04-analisis-discriminante-lineal/#preparacion-de-los-datos","title":"Preparaci\u00f3n de los Datos\u00b6","text":"<p>Dadas las caracter\u00edsticas del algoritmo podemos determinar varias reglas o gu\u00edas para preparar los datos de entrada. Estas son:</p> <ul> <li>Problemas de Clasificaci\u00f3n: LDA es un algoritmo de clasificaci\u00f3n, por lo que solo se debe usar para problemas donde la variable de salida sea categ\u00f3rica.</li> <li>Distribuci\u00f3n Gaussiana: El algoritmo asume que los datos de entrada tienen una distribuci\u00f3n Gaussiana, por lo que se debe verificar que esto sea cierto. Se puede verificar esto utilizando un histograma de los datos para cada variable en aisalaci\u00f3n. En caso de que la distribuci\u00f3n no sea Gaussiana, se puede transformar la variable para que lo sea.</li> <li>Remover Outliers: LDA es sensible a outliers, por lo que se deben remover antes de entrenar el modelo.</li> <li>Misma varianza: LDA asume que las variables tienen la misma varianza. Se recomienda siempre estandarizar las variables antes de entrenar el modelo.</li> </ul>"},{"location":"algoritmos-lineales/04-analisis-discriminante-lineal/#extensiones-del-modelo","title":"Extensiones del Modelo\u00b6","text":"<p>Dada la simpleza del algoritmo, existen varias extensiones que se pueden aplicar para mejorar el rendimiento del modelo. Algunas de estas son:</p> <ul> <li>An\u00e1lisis de Discriminante Cuadr\u00e1tico: El modelo LDA asume que las variables tienen la misma covarianza. El modelo QDA relaja esta suposici\u00f3n, permitiendo que cada variable tenga su propia varianza.</li> <li>An\u00e1lisis de Discriminante Flexible: Usa una combinaci\u00f3n no lineal de las entradas, como lo es un spline.</li> <li>An\u00e1lisis de Discriminante Regularizado: Utiliza regularizacion en la estimaci\u00f3n de la varianza, moderando la influencia de distintas variables en LDA.</li> </ul>"},{"location":"algoritmos-no-lineales/00-introduccion/","title":"Introducci\u00f3n","text":""},{"location":"algoritmos-no-lineales/00-introduccion/#introduccion","title":"Introducci\u00f3n\u00b6","text":"<p>Los algoritmos de aprendizaje automatizado no lineales son aquellos usados para modelar relaciones no lineales entre las variables de entrada y salida. En este caso, la salida no es una combinaci\u00f3n lineal de las entradas.</p>"},{"location":"algoritmos-no-lineales/00-introduccion/#contenidos","title":"Contenidos\u00b6","text":"<ol> <li>k Vecinos m\u00e1s Cercanos</li> <li>Bayesiano Ingenuo</li> </ol>"},{"location":"algoritmos-no-lineales/01-knn/","title":"K Vecinos M\u00e1s Cercanos (k-NN)","text":""},{"location":"algoritmos-no-lineales/01-knn/#k-vecinos-mas-cercanos-k-nn","title":"K Vecinos M\u00e1s Cercanos (k-NN)\u00b6","text":"<p>El algoritmo de los k vecinos m\u00e1s cercanos (k-NN) es un algoritmo de aprendizaje supervisado que intenta predecir el valor de una variable objetivo a partir de su similitud con otros datos existentes en el conjunto de entrenamiento. El algoritmo k-NN asume que los datos similares existen en proximidad cercana. En otras palabras, objetos similares est\u00e1n cerca unos de otros.</p> <p>En comparaci\u00f3n con otros algoritmos de aprendizaje supervisado que intentan hallar la mejor aproximacion de una funcion $y = f(x)$, el algoritmo k-NN no intenta construir un modelo interno, sino que simplemente almacena instancias de entrenamiento. La clasificaci\u00f3n de una nueva instancia se realiza en base a la similitud con las instancias almacenadas.</p> <p>Es un m\u00e9todo no param\u00e9trico que no hace suposiciones sobre la distribuci\u00f3n de los datos.</p>"},{"location":"algoritmos-no-lineales/01-knn/#cercania","title":"Cercan\u00eda\u00b6","text":"<p>Existen varias formas de medida de cercan\u00eda. La idea utilizada por el algoritmo, es representar cada dato como un punto en un espacion $n$-dimensional, dada $n$ cantidad de atributos. Pero incluso utilizando esta idea surgen problemas como lo es la proximidad a varios puntos de distinta clasificaci\u00f3n. Para esto se pone un limite a la cantidad de vecinos a considerar, y se clasifica en base a la clase mayoritaria de los $k$ vecinos m\u00e1s cercanos. Si este numero $k$ es par, se puede producir un empate, por lo que se suele elegir un numero impar.</p> <p>Para comparar la similitud entre dos puntos, pueden utilizar varios tipos de calculos de distancia. La m\u00e1s com\u00fan es la distancia euclidiana, pero se pueden utilizar otros.</p>"},{"location":"algoritmos-no-lineales/01-knn/#distancia-euclidiana","title":"Distancia Euclidiana\u00b6","text":"<p>Dada por la siguiente ecuaci\u00f3n:</p> <p>$$d(x,y) = \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2}$$</p> <p>Es la distancia m\u00e1s com\u00fanmente utilizada, pero es sensible a la escala de los datos. Para evitar esto, se puede normalizar los datos, es decir, llevarlos a una escala com\u00fan. Otras distancias que se pueden utilizar son:</p> <ul> <li>Distancia Manhattan: $$d(x,y) = \\sum_{i=1}^{n}|x_i - y_i|$$</li> <li>Distancia Chebyshev: $$d(x,y) = \\max_{i=1}^{n}|x_i - y_i|$$</li> <li>Distancia Minkowski: $$\\begin{align} &amp; d(x,y) = \\left(\\sum_{i=1}^{n}|x_i - y_i|^p\\right)^{1/p} \\\\ &amp; p = 1 \\rightarrow manhattan  \\\\ &amp; p = 2 \\rightarrow euclidiana \\\\ &amp; p = \\infty \\rightarrow chebyshev\\end{align}$$</li> </ul> <p>Una correcci\u00f3n que se puede hacer a la distancia euclidiana es la de considerar las instancias mas cercanas como las que tienen mayor influencia en la clasificaci\u00f3n. Para esto se puede asignar pesos a la distancia de cada vecino, de forma que los vecinos m\u00e1s cercanos tengan mayor influencia en la clasificaci\u00f3n. Una forma de hacer esto es la siguiente: consideramos que la suma de los pesos debe ser 1, y que el peso de un vecino es inversamente proporcional a su distancia. Usando ca\u00edda exponencial, el peso de un vecino $i$ es:</p> <p>$$w_i = \\frac{e^{-d(x,x_i)}}{\\sum_{j=1}^{k}e^{-d(x,x_j)}}$$</p> <p>El calculo de clasificaci\u00f3n resultante es:</p> <p>$$y = {max\\ class} (w_1 \\cdot y_1, \\dots, w_k \\cdot y_k)$$</p>"},{"location":"algoritmos-no-lineales/01-knn/#similitud-por-correlacion","title":"Similitud por Correlaci\u00f3n\u00b6","text":"<p>Otra forma de medir la similitud entre dos puntos es la correlaci\u00f3n. La correlaci\u00f3n es una medida de la relaci\u00f3n lineal entre dos puntos. La correlaci\u00f3n de Pearson var\u00eda entre -1 y 1, donde 1 es una correlaci\u00f3n positiva perfecta y -1 es una correlaci\u00f3n negativa perfecta. Una correlaci\u00f3n de 0 indica que no hay relaci\u00f3n entre los puntos, o que existe una relaci\u00f3n no lineal.</p> <p>La correlaci\u00f3n se calcula entre puntos, no las variables en si. Se calcula como:</p> <p>$$corr(X,Y) = \\frac{cov(X,Y)}{\\sigma_X \\cdot \\sigma_Y}$$</p> <p>Donde $cov(X,Y)$ es la covarianza entre $X$ e $Y$, y $\\sigma_X$ y $\\sigma_Y$ son las desviaciones estandar de $X$ e $Y$ respectivamente. Estas son calculadas de la siguiente forma:</p> <p>$$\\begin{align} &amp; cov(X,Y) = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{n-1} \\\\ &amp; \\sigma_X = \\sqrt{\\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}{n-1}} \\\\ &amp; \\sigma_Y = \\sqrt{\\frac{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}{n-1}}\\end{align}$$</p>"},{"location":"algoritmos-no-lineales/01-knn/#coeficiente-de-coincidencia-simple","title":"Coeficiente de Coincidencia Simple\u00b6","text":"<p>El coeficiente de coincidencia simple es una medida de similitud entre dos puntos cuando los atributos son binarios. Se calcula como la proporcion de aquellos atributos que coinciden entre los dos puntos.</p> <p>$$SMC = \\frac{m_{11} + m_{00}}{m_{11} + m_{10} + m_{01} + m_{00}}$$</p> <p>Donde $m_{ij}$ es la cantidad de atributos cuyo valor en el punto $X$ es $i$ y en el punto $Y$ es $j$.</p>"},{"location":"algoritmos-no-lineales/01-knn/#similitud-de-jaccard","title":"Similitud de Jaccard\u00b6","text":"<p>La similitud de Jaccard es una medida de similitud entre dos puntos cuando los atributos son binarios. Se calcula de forma similar al coeficiente de coincidencia simple, pero se excluyen los atributos donde ambos puntos tienen valor 0.</p> <p>$$J(X,Y) = \\frac{m_{11}}{m_{11} + m_{10} + m_{01}}$$</p> <p>Es utilizado, por ejemplo, para medir la similitud entre documentos, donde cada atributo es una palabra, y el valor es 1 si la palabra aparece en el documento, y 0 si no, con la consideraci\u00f3n de que dos documentos son similares si comparten palabras.</p>"},{"location":"algoritmos-no-lineales/01-knn/#similitud-por-coseno","title":"Similitud por Coseno\u00b6","text":"<p>La similitud por coseno es una medida de similitud entre dos puntos cuando los atributos son valores reales. Considera los puntos como vectores, y calcula el coseno del angulo entre ellos. Se calcula como:</p> <p>$$cos(X,Y) = \\frac{X \\cdot Y}{||X|| \\cdot ||Y||}$$</p> <p>Donde $X \\cdot Y$ es el producto punto entre los vectores $X$ e $Y$, y $||X||$ e $||Y||$ son las normas de los vectores $X$ e $Y$ respectivamente. Estas se calculan como:</p> <p>$$\\begin{align} &amp; ||X|| = \\sqrt{\\sum_{i=1}^{n}x_i^2} \\\\ &amp; ||Y|| = \\sqrt{\\sum_{i=1}^{n}y_i^2}\\end{align}$$</p>"},{"location":"algoritmos-no-lineales/01-knn/#representacion-del-modelo","title":"Representaci\u00f3n del Modelo\u00b6","text":"<p>El algoritmo k-NN es simple de implementar, pues su representaci\u00f3n interna es simplemente el conjunto de entrenamiento.</p>"},{"location":"algoritmos-no-lineales/01-knn/#entrenamiento-del-modelo","title":"Entrenamiento del Modelo\u00b6","text":"<p>El algoritmo k-NN no tiene un entrenamiento propiamente dicho, pues su representaci\u00f3n interna es simplemente el conjunto de entrenamiento.</p>"},{"location":"algoritmos-no-lineales/01-knn/#prediccion-del-modelo","title":"Predicci\u00f3n del Modelo\u00b6","text":"<p>Para predecir la clase de un nuevo punto, se calcula la distancia entre este y todos los puntos del conjunto de entrenamiento. Luego se seleccionan los $k$ puntos m\u00e1s cercanos, y se clasifica el nuevo punto en base a la clase mayoritaria de estos $k$ puntos. Esto requiere primero seleccionar una medida de distancia, y luego un valor para $k$.</p>"},{"location":"algoritmos-no-lineales/01-knn/#preparacion-de-los-datos","title":"Preparaci\u00f3n de los Datos\u00b6","text":"<p>Para la utilizaci\u00f3n del algoritmo k-NN, es necesario que los datos esten en una escala com\u00fan, pues algunas medidas de distancia son sensibles a la escala de los datos. Para esto se recomienda normalizar los datos.</p>"},{"location":"algoritmos-no-lineales/02-naive-bayes/","title":"Bayesiano Ingenuo","text":""},{"location":"algoritmos-no-lineales/02-naive-bayes/#bayesiano-ingenuo","title":"Bayesiano Ingenuo\u00b6","text":"<p>El algoritmo Bayesiano Ingenuo, o Naive Bayes, es un algoritmo de aprendizaje supervisado enfocado a la clasificaci\u00f3n. En su n\u00facleo, es un clasificador probabil\u00edstico que se basa en el teorema de Bayes, de ahi su nombre. Utiliza las relaciones probabilisticas entre los atributos de entrada y de salida para predecir la clase de una instancia.</p> <p>Este algoritmo asume la independencia entre los atributos de entrada, por lo que se le llama ingenuo. A pesar de esta suposici\u00f3n, el algoritmo funciona lo suficiente bien en la mayor\u00eda de los casos donde se presente una ligera correlaci\u00f3n entre los atributos. Se requiere igualmente remover aquellos atributos que est\u00e9n altamente correlacionados.</p>"},{"location":"algoritmos-no-lineales/02-naive-bayes/#teorema-de-bayes","title":"Teorema de Bayes\u00b6","text":"<p>El teorema de Bayes es una ecuaci\u00f3n que describe la relaci\u00f3n de probabilidades condicionales. Su formulaci\u00f3n es la siguiente:</p> <p>$$P(A|B) = \\frac{P(B|A)\\cdot P(A)}{P(B)}$$</p> <p>Generalizando para una entrada $X$ con $n$ atributos, se tiene:</p> <p>$$P(Y|X) = \\frac{P(Y)\\cdot P(X_1|Y)\\cdot P(X_2|Y)\\cdot ... \\cdot P(X_n|Y)}{P(X)}$$</p>"},{"location":"algoritmos-no-lineales/02-naive-bayes/#representacion-del-modelo","title":"Representaci\u00f3n del Modelo\u00b6","text":"<p>El modelo de Naive Bayes se representa como un conjunto de tablas de frecuencias. En esta se almacenan las probabilidades de cada atributo para cada clase de salida, asi como la probabilidad de cada clase de salida.</p>"},{"location":"algoritmos-no-lineales/02-naive-bayes/#entrenamiento-del-modelo","title":"Entrenamiento del Modelo\u00b6","text":"<p>El entrenamiento del modelo consiste en calcular las probabilidades de cada atributo para cada clase de salida, asi como la probabilidad de cada clase de salida. Para esto, se calcula cada dato de la siguiente manera:</p> <p>$$\\begin{align} &amp; P(Y) = \\frac{N_Y}{N} \\\\ &amp; P(X_i|Y) = \\frac{N_{Y,X_i}}{N_Y} \\end{align}$$</p> <p>Donde $N_Y$ es el n\u00famero de instancias de la clase $Y$, $N$ es el n\u00famero total de instancias, y $N_{Y,X_i}$ es el n\u00famero de instancias de la clase $Y$ que tienen el valor $X_i$ en el atributo $i$.</p> <p>Con estos datos se construyen tablas de frecuencia para cada atributo, para cada valor posible, para cada clase de salida.</p>"},{"location":"algoritmos-no-lineales/02-naive-bayes/#prediccion-del-modelo","title":"Predicci\u00f3n del modelo\u00b6","text":"<p>Para predecir la clase de una instancia, se calcula la probabilidad de cada clase de salida para la instancia, y se elige la clase con mayor probabilidad. Esto se calcula de la siguiente manera:</p> <p>$$P(Y=k|X) = \\frac{P(Y=k)\\cdot P(X_1|Y=k)\\cdot P(X_2|Y=k)\\cdot ... \\cdot P(X_n|Y=k)}{P(X)}$$</p> <p>Donde $k$ es la clase de salida, $X$ es la instancia, $P(Y=k)$ es la probabilidad de la clase $k$, $P(X_i|Y=k)$ es la probabilidad del atributo $i$ para la clase $k$, y $P(X)$ es la probabilidad de la instancia $X$.</p> <p>Dadas 2 clases de salida, se calcula la probabilidad de cada clase, y se obtiene un conjunto de 2 probabilidades:</p> <p>$$\\begin{align} &amp; P(Y=k_1|X) = \\frac{\\alpha}{P(X)} \\\\ &amp; P(Y=k_2|X) = \\frac{\\beta}{P(X)} \\end{align}$$</p> <p>Aqui, $P(X)$ resulta ser un termino normalizador, por lo que se puede ignorar. Para obtener las probabilidades reales, se ignora $P(X)$ y se normaliza $\\alpha$ y $\\beta$ de la siguiente manera:</p> <p>$$\\begin{align} &amp; P(Y=k_1|X) = \\frac{\\alpha}{\\alpha + \\beta} \\\\ &amp; P(Y=k_2|X) = \\frac{\\beta}{\\alpha + \\beta} \\end{align}$$</p> <p>Habiendo obtenido las probabilidades de cada clase, se elige la clase con mayor probabilidad.</p>"},{"location":"algoritmos-no-lineales/02-naive-bayes/#preparacion-de-los-datos","title":"Preparaci\u00f3n de los Datos\u00b6","text":"<p>Este algoritmo requiere ciertas caracter\u00edsticas de las datos para funcionar correctamente:</p> <ul> <li>Conjunto de entrenamiento incompleto</li> </ul> <p>El conjunto de entrenamiento debe contener todas las clases para todos los atributos. En el caso de un atributo no tenga una clase, el algoritmo asume que la probabilidad de ese atributo para esa clase es 0. Debido a esto, es necesario que el conjunto de entrenamiento sea representativo de la poblaci\u00f3n.</p> <p>Es posible tolerar la ausencia de clases en el conjunto de entrenamiento, pero esto requiere de un tratamiento especial. Se debe asignar probabilidades peque\u00f1as a las clases faltantes, y se debe tener cuidado de no asignar probabilidades de 0 a ninguna clase, para no descartarlos por completo. Realizamos esto tomando $p_1$, $p_2$, ..., $p_n$ tales que su suma sea 1, y $\\mu$ una probabilidad peque\u00f1a. Entonces, la probabilidad de cada clase es:</p> <p>$$P(X_i|Y) = \\frac{x_i + \\mu \\cdot p_1}{y_i + \\mu}$$</p> <p>Donde $x_i$ es el numerador de la probabilidad sin correcci\u00f3n, y $y_i$ es el denominador. Esto se llama correcci\u00f3n de Laplace.</p> <ul> <li>Atributos continuos</li> </ul> <p>Este algoritmo, debido a su utilizaci\u00f3n de probabilidades, si es aplicado a atributos continuos, se llega al resultado de que la probabilidad de un valor concreto es 0. Esto se debe a que la probabilidad de un valor concreto es 0 en una distribuci\u00f3n continua. Para evitar esto, se puede discretizar los atributos continuos, pero esto resulta en el problema de que una decision de discretizaci\u00f3n arbitraria puede afectar la precisi\u00f3n de la predicci\u00f3n. Para solucionar esto, se recomienda utilizar una funcion de densidad de probabilidad.</p> <p>Asumimos una distribuci\u00f3n normal de los datos en la probabilidad del atributo num\u00e9rico y aplicamos la funci\u00f3n siguiente:</p> <p>$$f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma}}e^{\\frac{(x-\\mu)^2}{2\\sigma^2}}$$</p> <p>Donde $\\mu$ es la media de los datos, y $\\sigma$ es la desviaci\u00f3n est\u00e1ndar.</p> <p>En caso de que la distribuci\u00f3n de los datos no sea normal, se puede utilizar una funci\u00f3n de densidad de probabilidad diferente.</p> <ul> <li>Atributos independientes</li> </ul> <p>El teorema de Bayes asume que los atributos son independientes entre si. Esto no es cierto en la mayor\u00eda de los casos, pero el algoritmo funciona lo suficientemente bien en la mayor\u00eda de los casos donde se presente una ligera correlaci\u00f3n entre los atributos. Se requiere igualmente remover aquellos atributos que est\u00e9n altamente correlacionados.</p>"},{"location":"casos-de-estudio/altura-osea/01-introduccion/","title":"Introducci\u00f3n","text":""},{"location":"casos-de-estudio/altura-osea/01-introduccion/#introduccion","title":"Introducci\u00f3n\u00b6","text":"<p>En este caso de estudo, vamos a enfocarnos en la predicci\u00f3n de la altura de una persona a partir de los restos \u00f3seos de su esqueleto. Para ello vamos a utilizar un conjunto de datos conteniendo informacion de esqueletos excavados de un cementerio medieval en Ribe, Dinamarca. El l\u00edder de la excavaci\u00f3n fue Jacob Kieffer-Olsen. Dentro de este conjunto de datos solo se cuentan con aquellos correspondientes a adultos. Sexo y edad fueron estimados para poder ser utilizados en an\u00e1lisis posteriores.</p> <p>La estimacion de altura basada en restos \u00f3seos es una herramienta util para la antropolog\u00eda y medicina forense. Es nuestro objetivo analizar los datos aqu\u00ed presentados y explorar la posibilidad de predecir la altura de una persona utilizando modelos de aprendizaje automatizado.</p>"},{"location":"casos-de-estudio/altura-osea/01-introduccion/#contenidos","title":"Contenidos\u00b6","text":"<ol> <li>Datos</li> <li>Modelado en RapidMiner</li> <li>Modelado en Python</li> </ol>"},{"location":"casos-de-estudio/altura-osea/02-datos/","title":"Datos","text":"In\u00a0[30]: Copied! <pre>import pandas\nimport matplotlib.pyplot as plt\nimport sqlalchemy as sa\nimport os\nimport pandas as pd\nimport numpy as np\n</pre> import pandas import matplotlib.pyplot as plt import sqlalchemy as sa import os import pandas as pd import numpy as np In\u00a0[36]: Copied! <pre>db_dir = \"datasets\\originales\\Knogler.accdb\"\nSQLALCHEMY_DATABASE_URI = os.path.abspath(db_dir).replace(\"\\\\\", \"\\\\\\\\\")\n\nconnection_string = (\n    r\"DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};\"\n    rf\"DBQ={SQLALCHEMY_DATABASE_URI};\"\n    r\"ExtendedAnsiSQL=1;\"\n)\nconnection_url = sa.engine.URL.create(\n    \"access+pyodbc\",\n    query={\"odbc_connect\": connection_string}\n)\n\nengine = sa.create_engine(connection_url)\ndf = pandas.read_sql(\"SELECT * FROM Dataindsamling\", engine)\n\nprint(df.head())\n</pre> db_dir = \"datasets\\originales\\Knogler.accdb\" SQLALCHEMY_DATABASE_URI = os.path.abspath(db_dir).replace(\"\\\\\", \"\\\\\\\\\")  connection_string = (     r\"DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};\"     rf\"DBQ={SQLALCHEMY_DATABASE_URI};\"     r\"ExtendedAnsiSQL=1;\" ) connection_url = sa.engine.URL.create(     \"access+pyodbc\",     query={\"odbc_connect\": connection_string} )  engine = sa.create_engine(connection_url) df = pandas.read_sql(\"SELECT * FROM Dataindsamling\", engine)  print(df.head()) <pre>   ID Location Site_Number  Age_Minumum  Age_Maximum   Sex Grave Number  \\\n0   1     Ribe     ASR1015         20.0         24.0  Male          G40   \n1   4     Ribe     ASR1015         35.0         45.0  Male         G312   \n2   5     Ribe     ASR1015         50.0         60.0  Male         G229   \n3   6     Ribe     ASR1015         30.0         40.0  Male         G257   \n4   7     Ribe     ASR1015         45.0         55.0  Male          G74   \n\n   Canine number  Canine largest age  Canine 2nd largest age  ...  \\\n0            2.0                 3.0                     5.0  ...   \n1            3.0                 4.0                     4.5  ...   \n2            0.0                 0.0                     0.0  ...   \n3            NaN                 NaN                     NaN  ...   \n4            NaN                 NaN                     NaN  ...   \n\n   Height in grave                            Abnormalities Vertebras  \\\n0            173.5                                               None   \n1            170.0                                               None   \n2            171.5  To hvirvler fusioneret - har ingen betydning f...   \n3            165.0                                               None   \n4            165.0                                               None   \n\n   Femur left  Femur right  Abnormalities Femur  \\\n0        49.6         50.0  Br\u00e6kket Postmortalt   \n1        48.4         48.5                 None   \n2        50.8         51.3                 None   \n3        45.4          NaN                 None   \n4        47.5         46.6                 None   \n\n                                               Notes       Date Signature  \\\n0                                               None 2008-05-05      Mwod   \n1                                      l\u00e6bedannelser 2008-05-08      MWOD   \n2                                               None 2008-05-08      MWOD   \n3  Male?, h\u00f8jre femur kan ikke m\u00e5les da nedbrudt,... 2008-05-08      MWOD   \n4  kraniet mangler - kig efter om det er p\u00e5 udsti...        NaT      None   \n\n  Hyperplasia Teeth Scorable  \n0        True           True  \n1        True           True  \n2       False           True  \n3       False          False  \n4       False          False  \n\n[5 rows x 23 columns]\n</pre> In\u00a0[32]: Copied! <pre>print(df.info())\n</pre> print(df.info()) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 117 entries, 0 to 116\nData columns (total 23 columns):\n #   Column                   Non-Null Count  Dtype         \n---  ------                   --------------  -----         \n 0   ID                       117 non-null    int64         \n 1   Location                 117 non-null    object        \n 2   Site_Number              117 non-null    object        \n 3   Age_Minumum              117 non-null    float64       \n 4   Age_Maximum              110 non-null    float64       \n 5   Sex                      115 non-null    object        \n 6   Grave Number             117 non-null    object        \n 7   Canine number            89 non-null     float64       \n 8   Canine largest age       83 non-null     float64       \n 9   Canine 2nd largest age   76 non-null     float64       \n 10  Incisor number           77 non-null     float64       \n 11  Incisor largest age      75 non-null     float64       \n 12  Incisor 2nd largest age  72 non-null     float64       \n 13  Height in grave          117 non-null    float64       \n 14  Abnormalities Vertebras  13 non-null     object        \n 15  Femur left               92 non-null     float64       \n 16  Femur right              90 non-null     float64       \n 17  Abnormalities Femur      11 non-null     object        \n 18  Notes                    65 non-null     object        \n 19  Date                     114 non-null    datetime64[ns]\n 20  Signature                114 non-null    object        \n 21  Hyperplasia              117 non-null    bool          \n 22  Teeth Scorable           117 non-null    bool          \ndtypes: bool(2), datetime64[ns](1), float64(11), int64(1), object(8)\nmemory usage: 19.5+ KB\nNone\n</pre> <p>Existen 23 atributos. Veamos en la literatura una descripci\u00f3n de los datos:</p> <ul> <li>ID: Identificador \u00fanico de cada registro.</li> <li>Location: Ubicaci\u00f3n de la muestra.</li> <li>Site_Number: N\u00famero de sitio.</li> <li>Age_Minimum: Edad m\u00ednima de la muestra.</li> <li>Age_Maximum: Edad m\u00e1xima de la muestra.</li> <li>Sex: Sexo de la muestra.</li> <li>Grave Number: N\u00famero de tumba.</li> <li>Canine Number: N\u00famero de hipopl\u00e1sias visibles en el canino.</li> <li>Canine largest age: Edad a la que se formo la hipoplasia en el primer canino m\u00e1s grandes.</li> <li>Canine 2nd largest age: Edad a la que se formo la hipoplasia en el segundo canino m\u00e1s grandes.</li> <li>Incisor Number: N\u00famero de hipopl\u00e1sias visibles en el incisivo.</li> <li>Incisor largest age: Edad a la que se formo la hipoplasia en el primer incisivo m\u00e1s grandes.</li> <li>Incisor 2nd largest age: Edad a la que se formo la hipoplasia en el segundo incisivo m\u00e1s grandes.</li> <li>Height in Grave: Altura en la tumba.</li> <li>Abnormalities Vertebras: Anormalidades notadas que pudieran haber impactado la altura en vida.</li> <li>Femur left: Longitud m\u00e1xima del f\u00e9mur izquierdo en cm.</li> <li>Femur right: Longitud m\u00e1xima del f\u00e9mur derecho en cm.</li> <li>Abnormalities Femur: Anormalidades notadas que pudieran haber impactado la altura medida en la tumba.</li> <li>Notes: Notas adicionales de inter\u00e9s.</li> <li>Date: Fecha de registro.</li> <li>Signature: Firma de la persona que recolecto la informaci\u00f3n.</li> <li>Hipopl\u00e1sia:  Marcado si se encontro hipopl\u00e1sia en la dentadura.</li> <li>Teeth Scorable: Marcado si el estado de los dientes es lo suficientemente bueno para analizar hipopl\u00e1sias.</li> </ul> In\u00a0[33]: Copied! <pre>data_df = df.drop(['ID', 'Location', 'Site_Number', 'Grave Number', 'Notes', 'Date', 'Signature'], axis=1)\n</pre> data_df = df.drop(['ID', 'Location', 'Site_Number', 'Grave Number', 'Notes', 'Date', 'Signature'], axis=1) In\u00a0[34]: Copied! <pre>statistic = pd.DataFrame([], index=['mean', 'std', 'min', 'max', 'median', 'mising'])\nfor column in statistic.columns:\n    statistic[column] = statistic[column].astype('float')\n\nfor column in data_df.columns:\n    if data_df[column].dtype in ('object', 'category', 'string'):\n        statistic[column] = [np.nan] * 5 + [data_df[column].isna().sum()]\n        continue\n\n    if data_df[column].isna().all():\n        statistic[column] = [np.nan] * 5 + [len(data_df[column])]\n        continue\n\n    statistic[column] = [\n        data_df[column].mean(),\n        data_df[column].std(),\n        data_df[column].min(),\n        data_df[column].max(),\n        data_df[column].median(),\n        np.isnan(data_df[column]).sum()\n    ]\nstatistic = statistic.T\nprint(statistic)\nprint('\\n')\n</pre> statistic = pd.DataFrame([], index=['mean', 'std', 'min', 'max', 'median', 'mising']) for column in statistic.columns:     statistic[column] = statistic[column].astype('float')  for column in data_df.columns:     if data_df[column].dtype in ('object', 'category', 'string'):         statistic[column] = [np.nan] * 5 + [data_df[column].isna().sum()]         continue      if data_df[column].isna().all():         statistic[column] = [np.nan] * 5 + [len(data_df[column])]         continue      statistic[column] = [         data_df[column].mean(),         data_df[column].std(),         data_df[column].min(),         data_df[column].max(),         data_df[column].median(),         np.isnan(data_df[column]).sum()     ] statistic = statistic.T print(statistic) print('\\n') <pre>                               mean        std    min    max median mising\nAge_Minumum               34.794872  10.675091   18.0   60.0   35.0    0.0\nAge_Maximum               43.309091  11.877622   20.0   70.0   45.0    7.0\nSex                             NaN        NaN    NaN    NaN    NaN    2.0\nCanine number              1.550562   1.883006    0.0    8.0    1.0   28.0\nCanine largest age         2.024096   1.969127    0.0    5.0    2.5   34.0\nCanine 2nd largest age     1.890789   2.010849    0.0    5.0    0.5   41.0\nIncisor number             1.584416   2.123494    0.0    7.0    0.0   40.0\nIncisor largest age            1.24   1.457413    0.0    4.0    0.0   42.0\nIncisor 2nd largest age    1.144444   1.491184    0.0    4.0    0.0   45.0\nHeight in grave          159.852991  23.246099    0.0  185.0  161.0    0.0\nAbnormalities Vertebras         NaN        NaN    NaN    NaN    NaN  104.0\nFemur left                42.504348  12.718663    0.0   54.4   45.1   25.0\nFemur right               40.436667  15.483412    0.0   53.6   45.4   27.0\nAbnormalities Femur             NaN        NaN    NaN    NaN    NaN  106.0\nHyperplasia                0.461538   0.500663  False   True    0.0      0\nTeeth Scorable             0.521368   0.501692  False   True    1.0      0\n\n\n</pre> <p>Dada la cantidad de ejemplos, podemos ver algunos atributos con muchos datos faltantes. Estos son:</p> <ul> <li>Abnormalities Vertebras</li> <li>Abnormalities Femur</li> </ul> <p>Estos atributos se pasan de 100 datos faltantes, para un conjunto de 117 ejemplos. Por otro lado, los atributos:</p> <ul> <li>Canine 2nd largest age</li> <li>Incisor Number</li> <li>Incisor largest age</li> <li>Incisor 2nd largest age</li> </ul> <p>Tienen mas de 40 datos faltantes, resultando ser de preocupacion. Veremos mas adelante en el preprocesamiento de los datos que hacemos con ellos.</p> In\u00a0[35]: Copied! <pre>num_columns = len(data_df.select_dtypes(include=['number']).columns)\nfig, axes = plt.subplots(nrows=num_columns, figsize=(10, 4 * num_columns))\n\nfor col, ax in zip(data_df.select_dtypes(include=['number']).columns, axes):\n    data_df[col].hist(ax=ax)\n    ax.set_title(f'Histogram for {col}')\n    ax.set_xlabel(col)\n    ax.set_ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n</pre> num_columns = len(data_df.select_dtypes(include=['number']).columns) fig, axes = plt.subplots(nrows=num_columns, figsize=(10, 4 * num_columns))  for col, ax in zip(data_df.select_dtypes(include=['number']).columns, axes):     data_df[col].hist(ax=ax)     ax.set_title(f'Histogram for {col}')     ax.set_xlabel(col)     ax.set_ylabel('Frequency')  plt.tight_layout() plt.show() <p>Analizando las distribuciones de los datos, podemos ver que varios de ellos presentan una distribuci\u00f3n normal, o al menos se asemejan fuera del valores en 0, aunque no todos lo hacen.</p>"},{"location":"casos-de-estudio/altura-osea/02-datos/#datos","title":"Datos\u00b6","text":"<p>A nuestra disposici\u00f3n se encuentra una base de datos en de Access. Dentro de esta nos encontramos con 2 tablas, Dataindsamling y Copy of Dataindsamling. Ambas contienen la misma cantidad de registros, los mismos atributos y los mismos valores, por lo que pasaremos a considerar solo una tabla.</p>"},{"location":"casos-de-estudio/altura-osea/02-datos/#estadistica","title":"Estadistica\u00b6","text":"<p>Investiguemos la distribuciones de los distintos atributos, y su estadistica descriptiva.</p>"},{"location":"casos-de-estudio/altura-osea/02-datos/#seleccion-de-algoritmo","title":"Selecci\u00f3n de Algoritmo\u00b6","text":"<p>Con el anterior an\u00e1lisis, nos encontramos con un problema de regresi\u00f3n donde el valor de una variable continua, la altura, es estimada a partir de otras variables continuas y discretas num\u00e9ricas. Por lo tanto, podemos presumir lo siguiente:</p> <ul> <li>Regresi\u00f3n lineal: Es un buen candidato, ya que el problema es de regresi\u00f3n, y los datos parecen tener una distribuci\u00f3n normal.</li> <li>Regresi\u00f3n log\u00edstica: No es un buen candidato, ya que el problema es de regresi\u00f3n, y la variable a predecir no es binaria.</li> <li>An\u00e1lisis discriminante lineal: Utilizado para clasificaci\u00f3n con variables de salida multi clase, no es un buen candidato.</li> <li>kNN: Se puede utilizar para regresi\u00f3n, con alguna modificaci\u00f3n.</li> <li>Naive Bayes: No es un buen candidato, ya que el varias de las variables de entrada no son discretas y precisar\u00edan de una discretizaci\u00f3n.</li> </ul> <p>Partiendo de estas observaciones, utilizaremos un modelo de regresi\u00f3n lineal y mediremos su desempe\u00f1o, tanto en Python como en RapidMiner.</p>"},{"location":"casos-de-estudio/altura-osea/03-regresion-lineal-rapidminer/","title":"Modelo de Regresi\u00f3n Lineal en RapidMiner","text":""},{"location":"casos-de-estudio/altura-osea/03-regresion-lineal-rapidminer/#modelo-de-regresion-lineal-en-rapidminer","title":"Modelo de Regresi\u00f3n Lineal en RapidMiner\u00b6","text":"<p>Previo an\u00e1lisis de los datos, llegamos a la conclusi\u00f3n de que un algoritmo de regresi\u00f3n lineal podr\u00eda ser una buena opci\u00f3n para predecir la altura de una persona seg\u00fan datos recopilados de sus restos \u00f3seos. Veremos como implementar este algoritmo en RapidMiner con este caso de estudio.</p>"},{"location":"casos-de-estudio/altura-osea/03-regresion-lineal-rapidminer/#carga-de-datos","title":"Carga de datos\u00b6","text":"<p>Utilizamos el operador <code>Read Access</code> para acceder a la base de datos del archivo <code>Knogler.accdb</code>. Nos aseguramos tambien de seleccionar la tabla <code>Dataindsamling</code> para cargar los datos.</p> <p></p>"},{"location":"casos-de-estudio/altura-osea/03-regresion-lineal-rapidminer/#preprocesamiento","title":"Preprocesamiento\u00b6","text":"<p>Cargados los datos, pasamos a etiquetarlos correctamente. Utilizamos el operador <code>Set Role</code> y marcamos la columna de altura como <code>label</code>, la columna ID como <code>id</code> y las columnas de informacion extra como Notes y Signature como <code>metadata</code>. El resto quedan como <code>regular</code>.</p> <p></p> <p>Segunda tarea es la selecci\u00f3n de atributos. Conocemos por el an\u00e1lisis exploratorio realizado anteriormente que existen algunas columnas con muchos valores faltantes. Removemos estas columnas con el operador <code>Select Attributes</code> y seleccionamos las columnas que no contengan mas de 40 valores faltantes.</p> <p></p> <p>Tercera tarea son los atributos faltantes. Utilizamos el operador <code>Replace Missing Values</code> para reemplazar los valores faltantes del atributo <code>Canine largest age</code>. Hay otros atributos con valores faltantes, pero estos no tienen tantos valores faltantes, por lo que simplemente removeremos los ejemplos donde esten presentes.</p> <p></p> <p>Utilizamos el operador <code>Filter Examples</code> para remover los ejemplos con valores faltantes en todos los atributos.</p> <p></p> <p>Luego convertimos los atributos categoricos a num\u00e9ricos con el operador <code>Nominal to Numerical</code>, seleccionando convertirlos a enteros \u00fanicos. Seleccionamos los atributos <code>Sex</code>, <code>Hyperplasia</code>, y <code>Teeth Scorable</code>.</p> <p></p> <p>Sexta tarea, es reasignar los tipos de atributo perdidos en las conversiones utilizando el operador <code>Guess Types</code> sobre los atributos numeros, esto nos restaurara los tipos reales.</p> <p></p> <p>Siguiente, removeremos atributo correlacionados utilizando el operador <code>Remove Correlated Attributes</code>. Seleccionamos un umbral de 0.35, determinado empiricamente.</p> <p></p> <p>Cerca el final, pasamos a detectar datos anomalos. Utilizamos el operador <code>Detect Outliers (LOF)</code> para detectar outliers en los datos. Este operador utiliza el algoritmo LOF (Local Outlier Factor) para detectar outliers, creando un atributo nuevo llamado <code>outlier</code> que contiene el factor de outlier para cada ejemplo. Luego utilizamos el operador <code>Filter Examples</code> para remover los ejemplos con un factor de outlier mayor a 2.</p> <p></p> <p>Finalmente, aplicamos una optimizacion de atributos utilizando los operadores <code>Optimize Selection</code> y <code>Select by Weights</code>. El primero nos permite optimizar los pesos de los atributos corriendo un algotimo de regresi\u00f3n lineal interno y mediante forwards selection. El segundo nos permite seleccionar los atributos con un peso mayor a un parametro dado. En este caso, seleccionamos los atributos con un peso mayor a 0.2, y configuraremos la optimizacion de parametros para utilizar 5 generaciones especulativas. Luego, continuaremos con una normalizaci\u00f3n de los datos utilizando el operador <code>Normalize</code>. Estos ultimos pasos los realizaremos dentro de un <code>Cross Validation</code> para prevenir contaminaci\u00f3n accidental de datos.</p>"},{"location":"casos-de-estudio/altura-osea/03-regresion-lineal-rapidminer/#entrenamiento","title":"Entrenamiento\u00b6","text":"<p>Para entrenar el modelo, utilizaremos el operador <code>Linear Regression</code>. Este operador nos permite entrenar un modelo de regresi\u00f3n lineal con los datos de entrada. Para evaluar el modelo, utilizaremos el operador <code>Cross Validation</code> que nos permite realizar una validaci\u00f3n cruzada de los datos de entrada. Dentro de este operador, utilizaremos los operadores <code>Optimize Selection</code> y <code>Select by Weights</code> para seleccionar los atributos mas importantes.</p>"},{"location":"casos-de-estudio/altura-osea/03-regresion-lineal-rapidminer/#proceso-completo","title":"Proceso Completo\u00b6","text":""},{"location":"casos-de-estudio/altura-osea/03-regresion-lineal-rapidminer/#evaluacion","title":"Evaluaci\u00f3n\u00b6","text":"<p>Una vez ejecutado el proceso, podemos ver los resultados de la evaluaci\u00f3n del modelo en la pesta\u00f1a <code>Results</code>. Aqu\u00ed podemos ver el RMSE o Root Mean Squared Error, que es una medida de la diferencia entre los valores predichos por el modelo y los valores reales.</p> <p></p> <p>En este caso, el RMSE es de 3.589, lo que significa que el modelo tiene un error promedio de ~3.6 cm en sus predicciones. Tambien podemos graficares los valores reales contra los valores predichos por el modelo, para ver como se comporta.</p> <p></p>"},{"location":"casos-de-estudio/altura-osea/03-regresion-lineal-rapidminer/#conclusiones","title":"Conclusiones\u00b6","text":"<p>Es mi opinion que dado el error para el modelo entrenado, regresi\u00f3n lineal presenta una buena opci\u00f3n para predecir la altura de una persona seg\u00fan datos recopilados de sus restos \u00f3seos. Dados los pesos reportados por el modelo de regresi\u00f3n lineal entrenado, parece ser que los unicos datos necesarios del conjunto para realizar las predicciones son el sexo de la persona, y la longitud del femur.</p>"},{"location":"casos-de-estudio/altura-osea/04-regresion-lineal-python/","title":"Modelo de Regresi\u00f3n Lineal en Python","text":"In\u00a0[115]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport sqlalchemy as sa\nimport os\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\n</pre> import pandas as pd import numpy as np import sqlalchemy as sa import os from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression from sklearn.model_selection import cross_val_predict from sklearn.pipeline import Pipeline from sklearn.feature_selection import SelectFromModel from sklearn.neighbors import LocalOutlierFactor from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score import matplotlib.pyplot as plt In\u00a0[116]: Copied! <pre>db_dir = \"datasets\\originales\\Knogler.accdb\"\nSQLALCHEMY_DATABASE_URI = os.path.abspath(db_dir).replace(\"\\\\\", \"\\\\\\\\\")\n\nconnection_string = (\n    r\"DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};\"\n    rf\"DBQ={SQLALCHEMY_DATABASE_URI};\"\n    r\"ExtendedAnsiSQL=1;\"\n)\nconnection_url = sa.engine.URL.create(\n    \"access+pyodbc\",\n    query={\"odbc_connect\": connection_string}\n)\n\nengine = sa.create_engine(connection_url)\ndf = pd.read_sql(\"SELECT * FROM Dataindsamling\", engine)\n\nprint(df.head())\n</pre> db_dir = \"datasets\\originales\\Knogler.accdb\" SQLALCHEMY_DATABASE_URI = os.path.abspath(db_dir).replace(\"\\\\\", \"\\\\\\\\\")  connection_string = (     r\"DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};\"     rf\"DBQ={SQLALCHEMY_DATABASE_URI};\"     r\"ExtendedAnsiSQL=1;\" ) connection_url = sa.engine.URL.create(     \"access+pyodbc\",     query={\"odbc_connect\": connection_string} )  engine = sa.create_engine(connection_url) df = pd.read_sql(\"SELECT * FROM Dataindsamling\", engine)  print(df.head()) <pre>   ID Location Site_Number  Age_Minumum  Age_Maximum   Sex Grave Number  \\\n0   1     Ribe     ASR1015         20.0         24.0  Male          G40   \n1   4     Ribe     ASR1015         35.0         45.0  Male         G312   \n2   5     Ribe     ASR1015         50.0         60.0  Male         G229   \n3   6     Ribe     ASR1015         30.0         40.0  Male         G257   \n4   7     Ribe     ASR1015         45.0         55.0  Male          G74   \n\n   Canine number  Canine largest age  Canine 2nd largest age  ...  \\\n0            2.0                 3.0                     5.0  ...   \n1            3.0                 4.0                     4.5  ...   \n2            0.0                 0.0                     0.0  ...   \n3            NaN                 NaN                     NaN  ...   \n4            NaN                 NaN                     NaN  ...   \n\n   Height in grave                            Abnormalities Vertebras  \\\n0            173.5                                               None   \n1            170.0                                               None   \n2            171.5  To hvirvler fusioneret - har ingen betydning f...   \n3            165.0                                               None   \n4            165.0                                               None   \n\n   Femur left  Femur right  Abnormalities Femur  \\\n0        49.6         50.0  Br\u00e6kket Postmortalt   \n1        48.4         48.5                 None   \n2        50.8         51.3                 None   \n3        45.4          NaN                 None   \n4        47.5         46.6                 None   \n\n                                               Notes       Date Signature  \\\n0                                               None 2008-05-05      Mwod   \n1                                      l\u00e6bedannelser 2008-05-08      MWOD   \n2                                               None 2008-05-08      MWOD   \n3  Male?, h\u00f8jre femur kan ikke m\u00e5les da nedbrudt,... 2008-05-08      MWOD   \n4  kraniet mangler - kig efter om det er p\u00e5 udsti...        NaT      None   \n\n  Hyperplasia Teeth Scorable  \n0        True           True  \n1        True           True  \n2       False           True  \n3       False          False  \n4       False          False  \n\n[5 rows x 23 columns]\n</pre> In\u00a0[117]: Copied! <pre>print(df.info())\n</pre> print(df.info()) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 117 entries, 0 to 116\nData columns (total 23 columns):\n #   Column                   Non-Null Count  Dtype         \n---  ------                   --------------  -----         \n 0   ID                       117 non-null    int64         \n 1   Location                 117 non-null    object        \n 2   Site_Number              117 non-null    object        \n 3   Age_Minumum              117 non-null    float64       \n 4   Age_Maximum              110 non-null    float64       \n 5   Sex                      115 non-null    object        \n 6   Grave Number             117 non-null    object        \n 7   Canine number            89 non-null     float64       \n 8   Canine largest age       83 non-null     float64       \n 9   Canine 2nd largest age   76 non-null     float64       \n 10  Incisor number           77 non-null     float64       \n 11  Incisor largest age      75 non-null     float64       \n 12  Incisor 2nd largest age  72 non-null     float64       \n 13  Height in grave          117 non-null    float64       \n 14  Abnormalities Vertebras  13 non-null     object        \n 15  Femur left               92 non-null     float64       \n 16  Femur right              90 non-null     float64       \n 17  Abnormalities Femur      11 non-null     object        \n 18  Notes                    65 non-null     object        \n 19  Date                     114 non-null    datetime64[ns]\n 20  Signature                114 non-null    object        \n 21  Hyperplasia              117 non-null    bool          \n 22  Teeth Scorable           117 non-null    bool          \ndtypes: bool(2), datetime64[ns](1), float64(11), int64(1), object(8)\nmemory usage: 19.5+ KB\nNone\n</pre> In\u00a0[118]: Copied! <pre>df = df.drop(['Date', 'Grave Number', 'Location', 'Notes', 'Signature', 'Site_Number', 'ID'], axis=1)\nprint(df.info())\n</pre> df = df.drop(['Date', 'Grave Number', 'Location', 'Notes', 'Signature', 'Site_Number', 'ID'], axis=1) print(df.info()) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 117 entries, 0 to 116\nData columns (total 16 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   Age_Minumum              117 non-null    float64\n 1   Age_Maximum              110 non-null    float64\n 2   Sex                      115 non-null    object \n 3   Canine number            89 non-null     float64\n 4   Canine largest age       83 non-null     float64\n 5   Canine 2nd largest age   76 non-null     float64\n 6   Incisor number           77 non-null     float64\n 7   Incisor largest age      75 non-null     float64\n 8   Incisor 2nd largest age  72 non-null     float64\n 9   Height in grave          117 non-null    float64\n 10  Abnormalities Vertebras  13 non-null     object \n 11  Femur left               92 non-null     float64\n 12  Femur right              90 non-null     float64\n 13  Abnormalities Femur      11 non-null     object \n 14  Hyperplasia              117 non-null    bool   \n 15  Teeth Scorable           117 non-null    bool   \ndtypes: bool(2), float64(11), object(3)\nmemory usage: 13.2+ KB\nNone\n</pre> <p>Tambien removemos las columnas con demasiados valores nulos. En especial aquellas con mas de 30 valores nulos.</p> In\u00a0[119]: Copied! <pre>examples_count = df.shape[0]\ndf = df.dropna(axis=1, thresh=int(examples_count - 30))\nprint(df.info())\n</pre> examples_count = df.shape[0] df = df.dropna(axis=1, thresh=int(examples_count - 30)) print(df.info()) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 117 entries, 0 to 116\nData columns (total 9 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   Age_Minumum      117 non-null    float64\n 1   Age_Maximum      110 non-null    float64\n 2   Sex              115 non-null    object \n 3   Canine number    89 non-null     float64\n 4   Height in grave  117 non-null    float64\n 5   Femur left       92 non-null     float64\n 6   Femur right      90 non-null     float64\n 7   Hyperplasia      117 non-null    bool   \n 8   Teeth Scorable   117 non-null    bool   \ndtypes: bool(2), float64(6), object(1)\nmemory usage: 6.8+ KB\nNone\n</pre> <p>Pasemos ahora a la imputaci\u00f3n de valores nulos. Para ello, utilizaremos la moda de los valores del atributo. Esto solo lo haremos para <code>Canine number</code>. Los otros atributos con valores faltantes se usaran para filtrar ejemplos incompletos.</p> In\u00a0[120]: Copied! <pre>df['Canine number'] = df['Canine number'].fillna(df['Canine number'].mode())\n</pre> df['Canine number'] = df['Canine number'].fillna(df['Canine number'].mode()) <p>El resto de los ejemplos con valores nulos seran eliminados.</p> In\u00a0[121]: Copied! <pre>df = df.dropna(axis=0)\nprint(df.info())\n</pre> df = df.dropna(axis=0) print(df.info()) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 59 entries, 0 to 114\nData columns (total 9 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   Age_Minumum      59 non-null     float64\n 1   Age_Maximum      59 non-null     float64\n 2   Sex              59 non-null     object \n 3   Canine number    59 non-null     float64\n 4   Height in grave  59 non-null     float64\n 5   Femur left       59 non-null     float64\n 6   Femur right      59 non-null     float64\n 7   Hyperplasia      59 non-null     bool   \n 8   Teeth Scorable   59 non-null     bool   \ndtypes: bool(2), float64(6), object(1)\nmemory usage: 3.8+ KB\nNone\n</pre> <p>En este punto ya tenemos un conjunto de datos completo de 59 ejemplos. Para poder aplicar el algoritmo de regresi\u00f3n lineal, debemos convertir los valores de los atributos a valores num\u00e9ricos.</p> In\u00a0[122]: Copied! <pre>df['Sex'] = df['Sex'].replace(['Male', 'Female'], [0, 1]).astype(int)\ndf['Hyperplasia'] = df['Hyperplasia'].astype(int)\ndf['Teeth Scorable'] = df['Teeth Scorable'].astype(int)\nprint(df.info())\n</pre> df['Sex'] = df['Sex'].replace(['Male', 'Female'], [0, 1]).astype(int) df['Hyperplasia'] = df['Hyperplasia'].astype(int) df['Teeth Scorable'] = df['Teeth Scorable'].astype(int) print(df.info()) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 59 entries, 0 to 114\nData columns (total 9 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   Age_Minumum      59 non-null     float64\n 1   Age_Maximum      59 non-null     float64\n 2   Sex              59 non-null     int32  \n 3   Canine number    59 non-null     float64\n 4   Height in grave  59 non-null     float64\n 5   Femur left       59 non-null     float64\n 6   Femur right      59 non-null     float64\n 7   Hyperplasia      59 non-null     int32  \n 8   Teeth Scorable   59 non-null     int32  \ndtypes: float64(6), int32(3)\nmemory usage: 3.9 KB\nNone\n</pre> <p>Proximo paso es remover los atributos correlacionados. Esto lo realizamos utilizando una matriz de correlaci\u00f3n y eliminando aquellos atributos con un valor de correlaci\u00f3n mayor a 0.35.</p> In\u00a0[123]: Copied! <pre>correlation_matrix = df.drop(['Height in grave'], axis=1).corr().abs()\n\nupper = correlation_matrix.where(\n    np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n)\n\nto_drop = [column for column in upper.columns if any(upper[column] &gt; 0.35)]\n\ndf = df.drop(df[to_drop], axis=1)\nprint(df.info())\n</pre> correlation_matrix = df.drop(['Height in grave'], axis=1).corr().abs()  upper = correlation_matrix.where(     np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool) )  to_drop = [column for column in upper.columns if any(upper[column] &gt; 0.35)]  df = df.drop(df[to_drop], axis=1) print(df.info()) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 59 entries, 0 to 114\nData columns (total 5 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   Age_Minumum      59 non-null     float64\n 1   Sex              59 non-null     int32  \n 2   Canine number    59 non-null     float64\n 3   Height in grave  59 non-null     float64\n 4   Femur left       59 non-null     float64\ndtypes: float64(4), int32(1)\nmemory usage: 2.5 KB\nNone\n</pre> <p>Siguiente, vamos a remover aquellos atributos que tengan datos anomalos.</p> In\u00a0[124]: Copied! <pre>lof = LocalOutlierFactor(n_neighbors=20)\nlof.fit(df)\ndf['outlier'] = -lof.negative_outlier_factor_\n\ndf = df[df['outlier'] &lt;= 2]\ndf = df.drop(['outlier'], axis=1)\nprint(df.info())\n</pre> lof = LocalOutlierFactor(n_neighbors=20) lof.fit(df) df['outlier'] = -lof.negative_outlier_factor_  df = df[df['outlier'] &lt;= 2] df = df.drop(['outlier'], axis=1) print(df.info()) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 54 entries, 0 to 114\nData columns (total 5 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   Age_Minumum      54 non-null     float64\n 1   Sex              54 non-null     int32  \n 2   Canine number    54 non-null     float64\n 3   Height in grave  54 non-null     float64\n 4   Femur left       54 non-null     float64\ndtypes: float64(4), int32(1)\nmemory usage: 2.3 KB\nNone\n</pre> <p>Como ultimo paso del preprocesamiento de datos, vamos a preparar una optimizacion de atributos basada en pesos de esos. Lo realizamos utilizando <code>SelectFromModel</code>, y un modelo de regresi\u00f3n lineal. Nos aseguramos tambien de preparar un paso de normalizaci\u00f3n donde sea necesario.</p> In\u00a0[125]: Copied! <pre>pipeline_for_selection = Pipeline([\n    ('scaler', StandardScaler()),\n    ('regression', LinearRegression())\n])\n\ndef coef_getter(estimator):\n    return estimator.named_steps['regression'].coef_\n\nselector = SelectFromModel(pipeline_for_selection, threshold='mean', importance_getter=coef_getter)\n</pre> pipeline_for_selection = Pipeline([     ('scaler', StandardScaler()),     ('regression', LinearRegression()) ])  def coef_getter(estimator):     return estimator.named_steps['regression'].coef_  selector = SelectFromModel(pipeline_for_selection, threshold='mean', importance_getter=coef_getter) <p>Con esto, el preprocesamiento esta finalizado. Pasemos a la etapa de entrenamiento.</p> In\u00a0[126]: Copied! <pre>inputs = df.drop(['Height in grave'], axis=1)\noutputs = df['Height in grave']\n\npipeline_for_training = Pipeline([\n    ('feature_selection', selector),\n    ('scaler', StandardScaler()),\n    ('regression', LinearRegression())\n])\n\npredictions = cross_val_predict(pipeline_for_training, inputs, outputs, cv=10)\n</pre> inputs = df.drop(['Height in grave'], axis=1) outputs = df['Height in grave']  pipeline_for_training = Pipeline([     ('feature_selection', selector),     ('scaler', StandardScaler()),     ('regression', LinearRegression()) ])  predictions = cross_val_predict(pipeline_for_training, inputs, outputs, cv=10) In\u00a0[127]: Copied! <pre>mae = mean_absolute_error(outputs, predictions)\nmse = mean_squared_error(outputs, predictions)\nrmse = np.sqrt(mse)\nr2 = r2_score(outputs, predictions)\n\nprint(f\"Mean Absolute Error (MAE): {mae}\")\nprint(f\"Mean Squared Error (MSE): {mse}\")\nprint(f\"Root Mean Squared Error (RMSE): {rmse}\")\nprint(f\"R-squared: {r2}\")\n</pre> mae = mean_absolute_error(outputs, predictions) mse = mean_squared_error(outputs, predictions) rmse = np.sqrt(mse) r2 = r2_score(outputs, predictions)  print(f\"Mean Absolute Error (MAE): {mae}\") print(f\"Mean Squared Error (MSE): {mse}\") print(f\"Root Mean Squared Error (RMSE): {rmse}\") print(f\"R-squared: {r2}\") <pre>Mean Absolute Error (MAE): 2.9317051721609793\nMean Squared Error (MSE): 13.889753225894177\nRoot Mean Squared Error (RMSE): 3.7268959236735033\nR-squared: 0.8707233999011896\n</pre> <p>Pasemos a graficar el valor real contra el valor predicho. Para mejor visualizar las diferencias.</p> In\u00a0[128]: Copied! <pre>plt.figure(figsize=(10, 6))\n\nplt.scatter(outputs, predictions, alpha=0.5)\n\nm, b = np.polyfit(outputs, predictions, 1)\nplt.plot(outputs, m*outputs + b, color='red', linewidth=2)\n\nplt.plot(outputs, outputs, color='green', linewidth=1, linestyle='--')\n\nplt.title('Actual vs. Predicted')\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.legend(['Predictions', 'Regression Line', 'Perfect Prediction Line'])\nplt.grid(True)\nplt.show()\n</pre> plt.figure(figsize=(10, 6))  plt.scatter(outputs, predictions, alpha=0.5)  m, b = np.polyfit(outputs, predictions, 1) plt.plot(outputs, m*outputs + b, color='red', linewidth=2)  plt.plot(outputs, outputs, color='green', linewidth=1, linestyle='--')  plt.title('Actual vs. Predicted') plt.xlabel('Actual Values') plt.ylabel('Predicted Values') plt.legend(['Predictions', 'Regression Line', 'Perfect Prediction Line']) plt.grid(True) plt.show()"},{"location":"casos-de-estudio/altura-osea/04-regresion-lineal-python/#modelo-de-regresion-lineal-en-python","title":"Modelo de Regresi\u00f3n Lineal en Python\u00b6","text":"<p>Ya creamos un modelo de predicci\u00f3n de la altura de una persona dados datos de sus restos \u00f3seos, utilizando el algoritmo de regresi\u00f3n lineal en RapidMiner. Ahora vamos a hacer lo mismo en Python.</p>"},{"location":"casos-de-estudio/altura-osea/04-regresion-lineal-python/#carga-de-datos","title":"Carga de datos\u00b6","text":"<p>Utilizaremos el mismo procedimiento de carga de datos que usamos en el an\u00e1lisis exploratorio de datos. Para ello, importamos las librer\u00edas necesarias y cargamos el fichero de datos.</p>"},{"location":"casos-de-estudio/altura-osea/04-regresion-lineal-python/#preprocesamiento","title":"Preprocesamiento\u00b6","text":"<p>Intentaremos seguir el mismo procedimiento que el que realizamos en el preprocesamiento de los datos para RapidMiner. La primera tarea es el filtrado de datos que no nos sirven para el caso. Removemos las columnas que no nos interesan.</p>"},{"location":"casos-de-estudio/altura-osea/04-regresion-lineal-python/#entrenamiento","title":"Entrenamiento\u00b6","text":"<p>Para entrenar el modelo, vamos a utilizar un algoritmo de regresi\u00f3n lineal. Para ello, importamos la librer\u00eda de regresi\u00f3n lineal de scikit-learn. Realizamos el proceso dentro de un pipline con el preprocesamiento de selecci\u00f3n de atributos y normalizaci\u00f3n. Esto lon hacemos dentro de una validaci\u00f3n cruzada de 10 folds.</p>"},{"location":"casos-de-estudio/altura-osea/04-regresion-lineal-python/#evaluacion","title":"Evaluaci\u00f3n\u00b6","text":"<p>Para evaluar el modelo, vamos a utilizar el RMSE y el R2. Para ello, importamos las librer\u00edas necesarias y calculamos los valores.</p>"},{"location":"casos-de-estudio/altura-osea/04-regresion-lineal-python/#conclusiones","title":"Conclusiones\u00b6","text":"<p>El modelo de regresi\u00f3n lineal en Python nos da un RMSE de 3.7 y un R2 de 0.87. Para una predicci\u00f3n de altura humana en cm, esto es un error aceptable. El modelo es bastante bueno, y se puede utilizar para predecir la altura de una persona a partir de sus restos \u00f3seos.</p>"},{"location":"casos-de-estudio/enfermedad-coronaria/01-introduccion/","title":"Introducci\u00f3n","text":""},{"location":"casos-de-estudio/enfermedad-coronaria/01-introduccion/#introduccion","title":"Introducci\u00f3n\u00b6","text":"<p>Las enfermedades card\u00edacas presentan un gran problema de salud p\u00fablica en todo el mundo. Se estima que en el mundo 1 de 13 personas viven con alg\u00fan tipo de enfermedad card\u00edaca, y que las enfermedades card\u00edacas son la principal causa de muerte en el mundo. En 2021, 1 de 3 muertes en el mundo se debieron a enfermedades card\u00edacas, llegando a 20.5 millones de muertes. Un promedio de 1.5 muertes por segundo.</p> <p>Muchos de los casos de enfermedad card\u00edaca presentan s\u00edntomas que pueden ser detectados y tratados a tiempo, y que pueden prevenir la muerte. Por lo tanto, es importante identificar cual son estos factores que contribuyen a la enfermedad card\u00edaca, y predecir a partir de ellos con cierta certeza si una persona tiene o no una enfermedad card\u00edaca.</p> <p>En este art\u00edculo, se presenta un caso de estudio de una base de datos de pacientes, algunos con enfermedad card\u00edaca y otros sin ella. Estos datos fueron obtenidos de la UCI Machine Learning Repository. El objetivo es construir un modelo de aprendizaje autom\u00e1tico que pueda predecir si un paciente tiene o no una enfermedad card\u00edaca, a partir de los datos de los pacientes.</p>"},{"location":"casos-de-estudio/enfermedad-coronaria/01-introduccion/#contenidos","title":"Contenidos\u00b6","text":"<ol> <li>Datos</li> <li>Modelado en RapidMiner</li> <li>Modelado en Python</li> </ol>"},{"location":"casos-de-estudio/enfermedad-coronaria/02-datos/","title":"Datos","text":"In\u00a0[56]: Copied! <pre>import pandas as pd\nimport numpy as np\n\nfiles = ['cleveland.data', 'hungarian.data', 'switzerland.data', 'long-beach-va.data']\ndatasets: dict[pd.DataFrame] = {}\n\ncolumns = [\n    ('id', 'Int64'),\n    ('ccf', 'Int64'),\n    ('age', 'Int64'),\n    ('sex', 'category'),\n    ('painloc', 'category'),\n    ('painexer', 'category'),\n    ('relrest', 'category'),\n    ('pncaden', 'category'),\n    ('cp', 'category'),\n    ('trestbps', 'float'),\n    ('htn', 'object'),\n    ('chol', 'float'),\n    ('smoke', 'category'),\n    ('cigs', 'Int64'),\n    ('years', 'Int64'),\n    ('fbs', 'category'),\n    ('dm', 'category'),\n    ('famhist', 'category'),\n    ('restecg', 'category'),\n    ('ekgmo', 'Int64'),\n    ('ekgday', 'Int64'),\n    ('ekgyr', 'Int64'),\n    ('dig', 'category'),\n    ('prop', 'category'),\n    ('nitr', 'category'),\n    ('pro', 'category'),\n    ('diuretic', 'category'),\n    ('proto', 'category'),\n    ('thaldur', 'float'),\n    ('thaltime', 'float'),\n    ('met', 'float'),\n    ('thalach', 'float'),\n    ('thalrest', 'float'),\n    ('tpeakbps', 'float'),\n    ('tpeakbpd', 'float'),\n    ('dummy', 'object'),\n    ('trestbpd', 'float'),\n    ('exang', 'category'),\n    ('xhypo', 'category'),\n    ('oldpeak', 'float'),\n    ('slope', 'category'),\n    ('rldv5', 'float'),\n    ('rldv5e', 'float'),\n    ('ca', 'Int64'),\n    ('restckm', 'object'),\n    ('exerckm', 'object'),\n    ('restef', 'float'),\n    ('restwm', 'category'),\n    ('exeref', 'float'),\n    ('exerwm', 'float'),\n    ('thal', 'category'),\n    ('thalsev', 'object'),\n    ('thalpul', 'object'),\n    ('earlobe', 'object'),\n    ('cmo', 'Int64'),\n    ('cday', 'Int64'),\n    ('cyr', 'Int64'),\n    ('num', 'category'),\n    ('lmt', 'object'),\n    ('ladprox', 'object'),\n    ('laddist', 'object'),\n    ('diag', 'object'),\n    ('cxmain', 'object'),\n    ('ramus', 'object'),\n    ('om1', 'object'),\n    ('om2', 'object'),\n    ('rcaprox', 'object'),\n    ('rcadist', 'object'),\n    ('lvx1', 'object'),\n    ('lvx2', 'object'),\n    ('lvx3', 'object'),\n    ('lvx4', 'object'),\n    ('lvf', 'object'),\n    ('cathef', 'object'),\n    ('junk', 'object'),\n    ('name', 'string')\n]\n\nfor file in files:\n    with open(f'./datasets/originales/{file}', 'r', encoding='utf-8') as f:\n        data = ''\n        while True:\n            try:\n                char = f.read(1)\n                if not char:\n                    break\n                data += char\n            except:\n                break\n\n        data = data.replace('-9.', 'NaN')\n        data = data.replace('-9', 'NaN')\n        data = data.replace('\\n', ' ')\n        data = data.replace('name', 'name\\n')\n        data = data.split('\\n')\n        for i in range(len(data)):\n            data[i] = data[i].split()\n        data = [datum for datum in data if len(datum) == 76]\n        dataset = pd.DataFrame(data)\n\n        for i in range(len(dataset)):\n            for j in range(len(dataset.columns)):\n                if dataset[j][i] == 'NaN':\n                    dataset[j][i] = np.nan\n\n        for i in range(len(dataset.columns)):\n            dataset[i] = dataset[i].astype(columns[i][1])\n\n        dataset.columns = [column[0] for column in columns]\n\n        dataset.fillna(np.nan)\n\n        datasets[file] = dataset\n\nprint([ds.info() for ds in datasets.values()])\n</pre> import pandas as pd import numpy as np  files = ['cleveland.data', 'hungarian.data', 'switzerland.data', 'long-beach-va.data'] datasets: dict[pd.DataFrame] = {}  columns = [     ('id', 'Int64'),     ('ccf', 'Int64'),     ('age', 'Int64'),     ('sex', 'category'),     ('painloc', 'category'),     ('painexer', 'category'),     ('relrest', 'category'),     ('pncaden', 'category'),     ('cp', 'category'),     ('trestbps', 'float'),     ('htn', 'object'),     ('chol', 'float'),     ('smoke', 'category'),     ('cigs', 'Int64'),     ('years', 'Int64'),     ('fbs', 'category'),     ('dm', 'category'),     ('famhist', 'category'),     ('restecg', 'category'),     ('ekgmo', 'Int64'),     ('ekgday', 'Int64'),     ('ekgyr', 'Int64'),     ('dig', 'category'),     ('prop', 'category'),     ('nitr', 'category'),     ('pro', 'category'),     ('diuretic', 'category'),     ('proto', 'category'),     ('thaldur', 'float'),     ('thaltime', 'float'),     ('met', 'float'),     ('thalach', 'float'),     ('thalrest', 'float'),     ('tpeakbps', 'float'),     ('tpeakbpd', 'float'),     ('dummy', 'object'),     ('trestbpd', 'float'),     ('exang', 'category'),     ('xhypo', 'category'),     ('oldpeak', 'float'),     ('slope', 'category'),     ('rldv5', 'float'),     ('rldv5e', 'float'),     ('ca', 'Int64'),     ('restckm', 'object'),     ('exerckm', 'object'),     ('restef', 'float'),     ('restwm', 'category'),     ('exeref', 'float'),     ('exerwm', 'float'),     ('thal', 'category'),     ('thalsev', 'object'),     ('thalpul', 'object'),     ('earlobe', 'object'),     ('cmo', 'Int64'),     ('cday', 'Int64'),     ('cyr', 'Int64'),     ('num', 'category'),     ('lmt', 'object'),     ('ladprox', 'object'),     ('laddist', 'object'),     ('diag', 'object'),     ('cxmain', 'object'),     ('ramus', 'object'),     ('om1', 'object'),     ('om2', 'object'),     ('rcaprox', 'object'),     ('rcadist', 'object'),     ('lvx1', 'object'),     ('lvx2', 'object'),     ('lvx3', 'object'),     ('lvx4', 'object'),     ('lvf', 'object'),     ('cathef', 'object'),     ('junk', 'object'),     ('name', 'string') ]  for file in files:     with open(f'./datasets/originales/{file}', 'r', encoding='utf-8') as f:         data = ''         while True:             try:                 char = f.read(1)                 if not char:                     break                 data += char             except:                 break          data = data.replace('-9.', 'NaN')         data = data.replace('-9', 'NaN')         data = data.replace('\\n', ' ')         data = data.replace('name', 'name\\n')         data = data.split('\\n')         for i in range(len(data)):             data[i] = data[i].split()         data = [datum for datum in data if len(datum) == 76]         dataset = pd.DataFrame(data)          for i in range(len(dataset)):             for j in range(len(dataset.columns)):                 if dataset[j][i] == 'NaN':                     dataset[j][i] = np.nan          for i in range(len(dataset.columns)):             dataset[i] = dataset[i].astype(columns[i][1])          dataset.columns = [column[0] for column in columns]          dataset.fillna(np.nan)          datasets[file] = dataset  print([ds.info() for ds in datasets.values()]) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 282 entries, 0 to 281\nData columns (total 76 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   id        282 non-null    Int64   \n 1   ccf       282 non-null    Int64   \n 2   age       282 non-null    Int64   \n 3   sex       282 non-null    category\n 4   painloc   0 non-null      category\n 5   painexer  0 non-null      category\n 6   relrest   0 non-null      category\n 7   pncaden   0 non-null      category\n 8   cp        282 non-null    category\n 9   trestbps  282 non-null    float64 \n 10  htn       282 non-null    object  \n 11  chol      282 non-null    float64 \n 12  smoke     0 non-null      category\n 13  cigs      277 non-null    Int64   \n 14  years     277 non-null    Int64   \n 15  fbs       282 non-null    category\n 16  dm        23 non-null     category\n 17  famhist   282 non-null    category\n 18  restecg   282 non-null    category\n 19  ekgmo     282 non-null    Int64   \n 20  ekgday    282 non-null    Int64   \n 21  ekgyr     282 non-null    Int64   \n 22  dig       280 non-null    category\n 23  prop      280 non-null    category\n 24  nitr      280 non-null    category\n 25  pro       280 non-null    category\n 26  diuretic  280 non-null    category\n 27  proto     282 non-null    category\n 28  thaldur   282 non-null    float64 \n 29  thaltime  213 non-null    float64 \n 30  met       282 non-null    float64 \n 31  thalach   282 non-null    float64 \n 32  thalrest  282 non-null    float64 \n 33  tpeakbps  282 non-null    float64 \n 34  tpeakbpd  282 non-null    float64 \n 35  dummy     282 non-null    object  \n 36  trestbpd  282 non-null    float64 \n 37  exang     282 non-null    category\n 38  xhypo     282 non-null    category\n 39  oldpeak   282 non-null    float64 \n 40  slope     282 non-null    category\n 41  rldv5     0 non-null      float64 \n 42  rldv5e    282 non-null    float64 \n 43  ca        280 non-null    Int64   \n 44  restckm   0 non-null      object  \n 45  exerckm   0 non-null      object  \n 46  restef    0 non-null      float64 \n 47  restwm    0 non-null      category\n 48  exeref    0 non-null      float64 \n 49  exerwm    0 non-null      float64 \n 50  thal      280 non-null    category\n 51  thalsev   0 non-null      object  \n 52  thalpul   0 non-null      object  \n 53  earlobe   0 non-null      object  \n 54  cmo       282 non-null    Int64   \n 55  cday      282 non-null    Int64   \n 56  cyr       282 non-null    Int64   \n 57  num       282 non-null    category\n 58  lmt       282 non-null    object  \n 59  ladprox   282 non-null    object  \n 60  laddist   282 non-null    object  \n 61  diag      0 non-null      object  \n 62  cxmain    282 non-null    object  \n 63  ramus     0 non-null      object  \n 64  om1       282 non-null    object  \n 65  om2       0 non-null      object  \n 66  rcaprox   282 non-null    object  \n 67  rcadist   282 non-null    object  \n 68  lvx1      282 non-null    object  \n 69  lvx2      282 non-null    object  \n 70  lvx3      282 non-null    object  \n 71  lvx4      282 non-null    object  \n 72  lvf       282 non-null    object  \n 73  cathef    0 non-null      object  \n 74  junk      0 non-null      object  \n 75  name      282 non-null    string  \ndtypes: Int64(12), category(23), float64(16), object(24), string(1)\nmemory usage: 129.4+ KB\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 294 entries, 0 to 293\nData columns (total 76 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   id        294 non-null    Int64   \n 1   ccf       294 non-null    Int64   \n 2   age       294 non-null    Int64   \n 3   sex       294 non-null    category\n 4   painloc   294 non-null    category\n 5   painexer  294 non-null    category\n 6   relrest   294 non-null    category\n 7   pncaden   0 non-null      category\n 8   cp        294 non-null    category\n 9   trestbps  293 non-null    float64 \n 10  htn       293 non-null    object  \n 11  chol      271 non-null    float64 \n 12  smoke     12 non-null     category\n 13  cigs      1 non-null      Int64   \n 14  years     0 non-null      Int64   \n 15  fbs       286 non-null    category\n 16  dm        21 non-null     category\n 17  famhist   1 non-null      category\n 18  restecg   293 non-null    category\n 19  ekgmo     294 non-null    Int64   \n 20  ekgday    294 non-null    Int64   \n 21  ekgyr     294 non-null    Int64   \n 22  dig       293 non-null    category\n 23  prop      292 non-null    category\n 24  nitr      293 non-null    category\n 25  pro       293 non-null    category\n 26  diuretic  293 non-null    category\n 27  proto     285 non-null    category\n 28  thaldur   292 non-null    float64 \n 29  thaltime  104 non-null    float64 \n 30  met       292 non-null    float64 \n 31  thalach   293 non-null    float64 \n 32  thalrest  293 non-null    float64 \n 33  tpeakbps  293 non-null    float64 \n 34  tpeakbpd  293 non-null    float64 \n 35  dummy     293 non-null    object  \n 36  trestbpd  293 non-null    float64 \n 37  exang     293 non-null    category\n 38  xhypo     292 non-null    category\n 39  oldpeak   294 non-null    float64 \n 40  slope     104 non-null    category\n 41  rldv5     293 non-null    float64 \n 42  rldv5e    294 non-null    float64 \n 43  ca        4 non-null      Int64   \n 44  restckm   0 non-null      object  \n 45  exerckm   0 non-null      object  \n 46  restef    0 non-null      float64 \n 47  restwm    3 non-null      category\n 48  exeref    0 non-null      float64 \n 49  exerwm    2 non-null      float64 \n 50  thal      28 non-null     category\n 51  thalsev   27 non-null     object  \n 52  thalpul   17 non-null     object  \n 53  earlobe   0 non-null      object  \n 54  cmo       294 non-null    Int64   \n 55  cday      294 non-null    Int64   \n 56  cyr       294 non-null    Int64   \n 57  num       294 non-null    category\n 58  lmt       19 non-null     object  \n 59  ladprox   58 non-null     object  \n 60  laddist   48 non-null     object  \n 61  diag      18 non-null     object  \n 62  cxmain    59 non-null     object  \n 63  ramus     9 non-null      object  \n 64  om1       23 non-null     object  \n 65  om2       5 non-null      object  \n 66  rcaprox   50 non-null     object  \n 67  rcadist   25 non-null     object  \n 68  lvx1      294 non-null    object  \n 69  lvx2      294 non-null    object  \n 70  lvx3      294 non-null    object  \n 71  lvx4      294 non-null    object  \n 72  lvf       294 non-null    object  \n 73  cathef    28 non-null     object  \n 74  junk      0 non-null      object  \n 75  name      294 non-null    string  \ndtypes: Int64(12), category(23), float64(16), object(24), string(1)\nmemory usage: 135.1+ KB\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 123 entries, 0 to 122\nData columns (total 76 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   id        123 non-null    Int64   \n 1   ccf       123 non-null    Int64   \n 2   age       123 non-null    Int64   \n 3   sex       123 non-null    category\n 4   painloc   123 non-null    category\n 5   painexer  123 non-null    category\n 6   relrest   123 non-null    category\n 7   pncaden   0 non-null      category\n 8   cp        123 non-null    category\n 9   trestbps  121 non-null    float64 \n 10  htn       93 non-null     object  \n 11  chol      123 non-null    float64 \n 12  smoke     23 non-null     category\n 13  cigs      11 non-null     Int64   \n 14  years     2 non-null      Int64   \n 15  fbs       48 non-null     category\n 16  dm        8 non-null      category\n 17  famhist   2 non-null      category\n 18  restecg   122 non-null    category\n 19  ekgmo     122 non-null    Int64   \n 20  ekgday    122 non-null    Int64   \n 21  ekgyr     122 non-null    Int64   \n 22  dig       118 non-null    category\n 23  prop      121 non-null    category\n 24  nitr      120 non-null    category\n 25  pro       122 non-null    category\n 26  diuretic  117 non-null    category\n 27  proto     73 non-null     category\n 28  thaldur   122 non-null    float64 \n 29  thaltime  89 non-null     float64 \n 30  met       73 non-null     float64 \n 31  thalach   122 non-null    float64 \n 32  thalrest  122 non-null    float64 \n 33  tpeakbps  120 non-null    float64 \n 34  tpeakbpd  120 non-null    float64 \n 35  dummy     121 non-null    object  \n 36  trestbpd  121 non-null    float64 \n 37  exang     122 non-null    category\n 38  xhypo     120 non-null    category\n 39  oldpeak   117 non-null    float64 \n 40  slope     106 non-null    category\n 41  rldv5     46 non-null     float64 \n 42  rldv5e    46 non-null     float64 \n 43  ca        5 non-null      Int64   \n 44  restckm   0 non-null      object  \n 45  exerckm   0 non-null      object  \n 46  restef    0 non-null      float64 \n 47  restwm    0 non-null      category\n 48  exeref    0 non-null      float64 \n 49  exerwm    0 non-null      float64 \n 50  thal      73 non-null     category\n 51  thalsev   72 non-null     object  \n 52  thalpul   26 non-null     object  \n 53  earlobe   0 non-null      object  \n 54  cmo       114 non-null    Int64   \n 55  cday      114 non-null    Int64   \n 56  cyr       114 non-null    Int64   \n 57  num       123 non-null    category\n 58  lmt       123 non-null    object  \n 59  ladprox   123 non-null    object  \n 60  laddist   123 non-null    object  \n 61  diag      123 non-null    object  \n 62  cxmain    123 non-null    object  \n 63  ramus     123 non-null    object  \n 64  om1       123 non-null    object  \n 65  om2       123 non-null    object  \n 66  rcaprox   123 non-null    object  \n 67  rcadist   123 non-null    object  \n 68  lvx1      106 non-null    object  \n 69  lvx2      106 non-null    object  \n 70  lvx3      106 non-null    object  \n 71  lvx4      106 non-null    object  \n 72  lvf       110 non-null    object  \n 73  cathef    106 non-null    object  \n 74  junk      0 non-null      object  \n 75  name      123 non-null    string  \ndtypes: Int64(12), category(23), float64(16), object(24), string(1)\nmemory usage: 58.3+ KB\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 200 entries, 0 to 199\nData columns (total 76 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   id        200 non-null    Int64   \n 1   ccf       200 non-null    Int64   \n 2   age       200 non-null    Int64   \n 3   sex       200 non-null    category\n 4   painloc   200 non-null    category\n 5   painexer  200 non-null    category\n 6   relrest   196 non-null    category\n 7   pncaden   0 non-null      category\n 8   cp        200 non-null    category\n 9   trestbps  144 non-null    float64 \n 10  htn       197 non-null    object  \n 11  chol      193 non-null    float64 \n 12  smoke     195 non-null    category\n 13  cigs      190 non-null    Int64   \n 14  years     188 non-null    Int64   \n 15  fbs       193 non-null    category\n 16  dm        43 non-null     category\n 17  famhist   192 non-null    category\n 18  restecg   200 non-null    category\n 19  ekgmo     148 non-null    Int64   \n 20  ekgday    147 non-null    Int64   \n 21  ekgyr     148 non-null    Int64   \n 22  dig       140 non-null    category\n 23  prop      140 non-null    category\n 24  nitr      141 non-null    category\n 25  pro       141 non-null    category\n 26  diuretic  127 non-null    category\n 27  proto     147 non-null    category\n 28  thaldur   147 non-null    float64 \n 29  thaltime  40 non-null     float64 \n 30  met       147 non-null    float64 \n 31  thalach   147 non-null    float64 \n 32  thalrest  146 non-null    float64 \n 33  tpeakbps  141 non-null    float64 \n 34  tpeakbpd  141 non-null    float64 \n 35  dummy     144 non-null    object  \n 36  trestbpd  144 non-null    float64 \n 37  exang     147 non-null    category\n 38  xhypo     147 non-null    category\n 39  oldpeak   144 non-null    float64 \n 40  slope     99 non-null     category\n 41  rldv5     135 non-null    float64 \n 42  rldv5e    135 non-null    float64 \n 43  ca        2 non-null      Int64   \n 44  restckm   0 non-null      object  \n 45  exerckm   1 non-null      object  \n 46  restef    28 non-null     float64 \n 47  restwm    27 non-null     category\n 48  exeref    2 non-null      float64 \n 49  exerwm    3 non-null      float64 \n 50  thal      41 non-null     category\n 51  thalsev   31 non-null     object  \n 52  thalpul   1 non-null      object  \n 53  earlobe   1 non-null      object  \n 54  cmo       198 non-null    Int64   \n 55  cday      200 non-null    Int64   \n 56  cyr       200 non-null    Int64   \n 57  num       200 non-null    category\n 58  lmt       200 non-null    object  \n 59  ladprox   200 non-null    object  \n 60  laddist   200 non-null    object  \n 61  diag      200 non-null    object  \n 62  cxmain    200 non-null    object  \n 63  ramus     200 non-null    object  \n 64  om1       200 non-null    object  \n 65  om2       199 non-null    object  \n 66  rcaprox   199 non-null    object  \n 67  rcadist   199 non-null    object  \n 68  lvx1      198 non-null    object  \n 69  lvx2      198 non-null    object  \n 70  lvx3      198 non-null    object  \n 71  lvx4      198 non-null    object  \n 72  lvf       197 non-null    object  \n 73  cathef    177 non-null    object  \n 74  junk      119 non-null    object  \n 75  name      200 non-null    string  \ndtypes: Int64(12), category(23), float64(16), object(24), string(1)\nmemory usage: 93.1+ KB\n[None, None, None, None]\n</pre> In\u00a0[53]: Copied! <pre>pd.set_option('display.max_rows', None)\n\nfor name, dataset in datasets.items():\n    statistic = pd.DataFrame([], index=['mean', 'std', 'min', 'max', 'median', 'mising'])\n    for column in statistic.columns:\n        statistic[column] = statistic[column].astype('float')\n\n    for column in dataset.columns:\n        if dataset[column].dtype in ('object', 'category', 'string'):\n            statistic[column] = [np.nan] * 5 + [dataset[column].isna().sum()]\n            continue\n\n        if dataset[column].isna().all():\n            statistic[column] = [np.nan] * 5 + [len(dataset[column])]\n            continue\n\n        statistic[column] = [\n            dataset[column].mean(),\n            dataset[column].std(),\n            dataset[column].min(),\n            dataset[column].max(),\n            dataset[column].median(),\n            np.isnan(dataset[column]).sum()\n        ]\n    statistic = statistic.T\n    print(name)\n    print(statistic)\n    print('\\n')\n\npd.set_option('display.max_rows', 10)\n</pre> pd.set_option('display.max_rows', None)  for name, dataset in datasets.items():     statistic = pd.DataFrame([], index=['mean', 'std', 'min', 'max', 'median', 'mising'])     for column in statistic.columns:         statistic[column] = statistic[column].astype('float')      for column in dataset.columns:         if dataset[column].dtype in ('object', 'category', 'string'):             statistic[column] = [np.nan] * 5 + [dataset[column].isna().sum()]             continue          if dataset[column].isna().all():             statistic[column] = [np.nan] * 5 + [len(dataset[column])]             continue          statistic[column] = [             dataset[column].mean(),             dataset[column].std(),             dataset[column].min(),             dataset[column].max(),             dataset[column].median(),             np.isnan(dataset[column]).sum()         ]     statistic = statistic.T     print(name)     print(statistic)     print('\\n')  pd.set_option('display.max_rows', 10) <pre>cleveland.data\n                mean        std    min    max  median  mising\nid        151.517730  87.131234    1.0  298.0   151.5     0.0\nccf         0.000000   0.000000    0.0    0.0     0.0     0.0\nage        54.411348   9.053083   29.0   77.0    55.0     0.0\nsex              NaN        NaN    NaN    NaN     NaN     0.0\npainloc          NaN        NaN    NaN    NaN     NaN   282.0\npainexer         NaN        NaN    NaN    NaN     NaN   282.0\nrelrest          NaN        NaN    NaN    NaN     NaN   282.0\npncaden          NaN        NaN    NaN    NaN     NaN   282.0\ncp               NaN        NaN    NaN    NaN     NaN     0.0\ntrestbps  131.563830  17.757496   94.0  200.0   130.0     0.0\nhtn              NaN        NaN    NaN    NaN     NaN     0.0\nchol      249.092199  51.217546  126.0  564.0   244.0     0.0\nsmoke            NaN        NaN    NaN    NaN     NaN   282.0\ncigs       16.920578  19.451934    0.0   99.0    10.0     0.0\nyears      15.259928  15.367867    0.0   54.0    15.0     0.0\nfbs              NaN        NaN    NaN    NaN     NaN     0.0\ndm               NaN        NaN    NaN    NaN     NaN   259.0\nfamhist          NaN        NaN    NaN    NaN     NaN     0.0\nrestecg          NaN        NaN    NaN    NaN     NaN     0.0\nekgmo       6.404255   3.790752    1.0   12.0     7.0     0.0\nekgday     15.223404   8.686089    1.0   31.0    15.0     0.0\nekgyr      82.301418   0.960759   81.0   84.0    82.0     0.0\ndig              NaN        NaN    NaN    NaN     NaN     2.0\nprop             NaN        NaN    NaN    NaN     NaN     2.0\nnitr             NaN        NaN    NaN    NaN     NaN     2.0\npro              NaN        NaN    NaN    NaN     NaN     2.0\ndiuretic         NaN        NaN    NaN    NaN     NaN     2.0\nproto            NaN        NaN    NaN    NaN     NaN     0.0\nthaldur     8.418440   2.590569    1.8   15.0     8.5     0.0\nthaltime    4.882629   3.465331    0.0   15.0     5.5    69.0\nmet         9.753546   2.961870    3.0   18.0     9.0     0.0\nthalach   149.765957  22.923869   71.0  202.0   153.5     0.0\nthalrest   75.117021  13.795809   40.0  119.0    74.0     0.0\ntpeakbps  168.056738  23.575961   84.0  232.0   168.0     0.0\ntpeakbpd   78.744681  13.331206   26.0  120.0    80.0     0.0\ndummy            NaN        NaN    NaN    NaN     NaN     0.0\ntrestbpd   84.946809   9.477864   50.0  110.0    85.0     0.0\nexang            NaN        NaN    NaN    NaN     NaN     0.0\nxhypo            NaN        NaN    NaN    NaN     NaN     0.0\noldpeak     1.026950   1.138825    0.0    6.2     0.8     0.0\nslope            NaN        NaN    NaN    NaN     NaN     0.0\nrldv5            NaN        NaN    NaN    NaN     NaN   282.0\nrldv5e    123.585106  46.783839   24.0  270.0   118.0     0.0\nca          0.664286   0.936023    0.0    3.0     0.0     0.0\nrestckm          NaN        NaN    NaN    NaN     NaN   282.0\nexerckm          NaN        NaN    NaN    NaN     NaN   282.0\nrestef           NaN        NaN    NaN    NaN     NaN   282.0\nrestwm           NaN        NaN    NaN    NaN     NaN   282.0\nexeref           NaN        NaN    NaN    NaN     NaN   282.0\nexerwm           NaN        NaN    NaN    NaN     NaN   282.0\nthal             NaN        NaN    NaN    NaN     NaN     2.0\nthalsev          NaN        NaN    NaN    NaN     NaN   282.0\nthalpul          NaN        NaN    NaN    NaN     NaN   282.0\nearlobe          NaN        NaN    NaN    NaN     NaN   282.0\ncmo         6.400709   3.744376    1.0   12.0     7.0     0.0\ncday       15.347518   8.830541    1.0   31.0    15.0     0.0\ncyr        82.304965   0.957780   81.0   84.0    82.0     0.0\nnum              NaN        NaN    NaN    NaN     NaN     0.0\nlmt              NaN        NaN    NaN    NaN     NaN     0.0\nladprox          NaN        NaN    NaN    NaN     NaN     0.0\nladdist          NaN        NaN    NaN    NaN     NaN     0.0\ndiag             NaN        NaN    NaN    NaN     NaN   282.0\ncxmain           NaN        NaN    NaN    NaN     NaN     0.0\nramus            NaN        NaN    NaN    NaN     NaN   282.0\nom1              NaN        NaN    NaN    NaN     NaN     0.0\nom2              NaN        NaN    NaN    NaN     NaN   282.0\nrcaprox          NaN        NaN    NaN    NaN     NaN     0.0\nrcadist          NaN        NaN    NaN    NaN     NaN     0.0\nlvx1             NaN        NaN    NaN    NaN     NaN     0.0\nlvx2             NaN        NaN    NaN    NaN     NaN     0.0\nlvx3             NaN        NaN    NaN    NaN     NaN     0.0\nlvx4             NaN        NaN    NaN    NaN     NaN     0.0\nlvf              NaN        NaN    NaN    NaN     NaN     0.0\ncathef           NaN        NaN    NaN    NaN     NaN   282.0\njunk             NaN        NaN    NaN    NaN     NaN   282.0\nname             NaN        NaN    NaN    NaN     NaN     0.0\n\n\nhungarian.data\n                 mean         std     min     max  median mising\nid        1195.853741  397.340367  1001.0  5002.0  1158.5    0.0\nccf               0.0         0.0     0.0     0.0     0.0    0.0\nage         47.826531    7.811812    28.0    66.0    49.0    0.0\nsex               NaN         NaN     NaN     NaN     NaN    0.0\npainloc           NaN         NaN     NaN     NaN     NaN    0.0\npainexer          NaN         NaN     NaN     NaN     NaN    0.0\nrelrest           NaN         NaN     NaN     NaN     NaN    0.0\npncaden           NaN         NaN     NaN     NaN     NaN  294.0\ncp                NaN         NaN     NaN     NaN     NaN    0.0\ntrestbps   132.583618   17.626568    92.0   200.0   130.0    1.0\nhtn               NaN         NaN     NaN     NaN     NaN    1.0\nchol       250.848708   67.657711    85.0   603.0   243.0   23.0\nsmoke             NaN         NaN     NaN     NaN     NaN  282.0\ncigs             60.0        &lt;NA&gt;      60      60    60.0      0\nyears             NaN         NaN     NaN     NaN     NaN  294.0\nfbs               NaN         NaN     NaN     NaN     NaN    8.0\ndm                NaN         NaN     NaN     NaN     NaN  273.0\nfamhist           NaN         NaN     NaN     NaN     NaN  293.0\nrestecg           NaN         NaN     NaN     NaN     NaN    1.0\nekgmo        6.061224    3.203506     1.0    12.0     5.0    0.0\nekgday      15.761905    8.717667     1.0    31.0    16.0    0.0\nekgyr        85.20068    1.122467    83.0    87.0    85.0    0.0\ndig               NaN         NaN     NaN     NaN     NaN    1.0\nprop              NaN         NaN     NaN     NaN     NaN    2.0\nnitr              NaN         NaN     NaN     NaN     NaN    1.0\npro               NaN         NaN     NaN     NaN     NaN    1.0\ndiuretic          NaN         NaN     NaN     NaN     NaN    1.0\nproto             NaN         NaN     NaN     NaN     NaN    9.0\nthaldur     10.626712     4.64322     1.0    24.0    10.0    2.0\nthaltime     7.576923    4.144694     0.5    20.0    7.25  190.0\nmet          5.700342    1.864453     2.0    11.0     6.0    2.0\nthalach    139.129693   23.589749    82.0   190.0   140.0    1.0\nthalrest     81.03413   15.534943    46.0   134.0    80.0    1.0\ntpeakbps   181.279863   25.615776   120.0   240.0   180.0    1.0\ntpeakbpd    95.713311   12.369859    60.0   134.0    95.0    1.0\ndummy             NaN         NaN     NaN     NaN     NaN    1.0\ntrestbpd    84.395904    9.267722    50.0   110.0    80.0    1.0\nexang             NaN         NaN     NaN     NaN     NaN    1.0\nxhypo             NaN         NaN     NaN     NaN     NaN    2.0\noldpeak      0.586054    0.908648     0.0     5.0     0.0    0.0\nslope             NaN         NaN     NaN     NaN     NaN  190.0\nrldv5       13.682594    5.581734     3.0    31.0    13.0    1.0\nrldv5e      13.295918    5.345808     2.0    30.0    13.0    0.0\nca               2.25         4.5     0.0     9.0     0.0    0.0\nrestckm           NaN         NaN     NaN     NaN     NaN  294.0\nexerckm           NaN         NaN     NaN     NaN     NaN  294.0\nrestef            NaN         NaN     NaN     NaN     NaN  294.0\nrestwm            NaN         NaN     NaN     NaN     NaN  291.0\nexeref            NaN         NaN     NaN     NaN     NaN  294.0\nexerwm            0.0         0.0     0.0     0.0     0.0  292.0\nthal              NaN         NaN     NaN     NaN     NaN  266.0\nthalsev           NaN         NaN     NaN     NaN     NaN  267.0\nthalpul           NaN         NaN     NaN     NaN     NaN  277.0\nearlobe           NaN         NaN     NaN     NaN     NaN  294.0\ncmo          6.241497    3.212525     1.0    12.0     6.0    0.0\ncday        17.244898    8.667614     1.0    31.0    17.0    0.0\ncyr         85.204082    1.117278    83.0    87.0    85.0    0.0\nnum               NaN         NaN     NaN     NaN     NaN    0.0\nlmt               NaN         NaN     NaN     NaN     NaN  275.0\nladprox           NaN         NaN     NaN     NaN     NaN  236.0\nladdist           NaN         NaN     NaN     NaN     NaN  246.0\ndiag              NaN         NaN     NaN     NaN     NaN  276.0\ncxmain            NaN         NaN     NaN     NaN     NaN  235.0\nramus             NaN         NaN     NaN     NaN     NaN  285.0\nom1               NaN         NaN     NaN     NaN     NaN  271.0\nom2               NaN         NaN     NaN     NaN     NaN  289.0\nrcaprox           NaN         NaN     NaN     NaN     NaN  244.0\nrcadist           NaN         NaN     NaN     NaN     NaN  269.0\nlvx1              NaN         NaN     NaN     NaN     NaN    0.0\nlvx2              NaN         NaN     NaN     NaN     NaN    0.0\nlvx3              NaN         NaN     NaN     NaN     NaN    0.0\nlvx4              NaN         NaN     NaN     NaN     NaN    0.0\nlvf               NaN         NaN     NaN     NaN     NaN    0.0\ncathef            NaN         NaN     NaN     NaN     NaN  266.0\njunk              NaN         NaN     NaN     NaN     NaN  294.0\nname              NaN         NaN     NaN     NaN     NaN    0.0\n\n\nswitzerland.data\n                 mean         std     min     max  median  mising\nid        3625.886179  499.271374  3001.0  4074.0  4012.0     0.0\nccf          0.000000    0.000000     0.0     0.0     0.0     0.0\nage         55.317073    9.032108    32.0    74.0    56.0     0.0\nsex               NaN         NaN     NaN     NaN     NaN     0.0\npainloc           NaN         NaN     NaN     NaN     NaN     0.0\npainexer          NaN         NaN     NaN     NaN     NaN     0.0\nrelrest           NaN         NaN     NaN     NaN     NaN     0.0\npncaden           NaN         NaN     NaN     NaN     NaN   123.0\ncp                NaN         NaN     NaN     NaN     NaN     0.0\ntrestbps   130.206612   22.559151    80.0   200.0   125.0     2.0\nhtn               NaN         NaN     NaN     NaN     NaN    30.0\nchol         0.000000    0.000000     0.0     0.0     0.0     0.0\nsmoke             NaN         NaN     NaN     NaN     NaN   100.0\ncigs        25.454545    9.341987    20.0    40.0    20.0     0.0\nyears       45.000000    7.071068    40.0    50.0    45.0     0.0\nfbs               NaN         NaN     NaN     NaN     NaN    75.0\ndm                NaN         NaN     NaN     NaN     NaN   115.0\nfamhist           NaN         NaN     NaN     NaN     NaN   121.0\nrestecg           NaN         NaN     NaN     NaN     NaN     1.0\nekgmo        4.500000    3.191544     1.0    12.0     3.5     0.0\nekgday      15.721311    9.019045     1.0    30.0    17.0     0.0\nekgyr       84.631148    0.484484    84.0    85.0    85.0     0.0\ndig               NaN         NaN     NaN     NaN     NaN     5.0\nprop              NaN         NaN     NaN     NaN     NaN     2.0\nnitr              NaN         NaN     NaN     NaN     NaN     3.0\npro               NaN         NaN     NaN     NaN     NaN     1.0\ndiuretic          NaN         NaN     NaN     NaN     NaN     6.0\nproto             NaN         NaN     NaN     NaN     NaN    50.0\nthaldur      7.427049    2.250952     3.0    12.0     7.0     1.0\nthaltime     5.707865    4.804241     0.0    17.8     6.0    34.0\nmet        106.506849   35.849656    50.0   200.0   100.0    50.0\nthalach    121.557377   25.977438    60.0   182.0   121.0     1.0\nthalrest    69.918033   12.949738    39.0   110.0    70.0     1.0\ntpeakbps   166.083333   25.156513   110.0   230.0   162.5     3.0\ntpeakbpd    85.791667   11.559052    55.0   115.0    85.0     3.0\ndummy             NaN         NaN     NaN     NaN     NaN     2.0\ntrestbpd    81.942149   11.390272    60.0   120.0    80.0     2.0\nexang             NaN         NaN     NaN     NaN     NaN     1.0\nxhypo             NaN         NaN     NaN     NaN     NaN     3.0\noldpeak      0.653846    1.056061    -2.6     3.7     0.3     6.0\nslope             NaN         NaN     NaN     NaN     NaN    17.0\nrldv5       14.239130    5.237407     7.0    26.0    13.0    77.0\nrldv5e      14.739130    5.555517     6.0    27.0    15.0    77.0\nca           1.600000    0.547723     1.0     2.0     2.0     0.0\nrestckm           NaN         NaN     NaN     NaN     NaN   123.0\nexerckm           NaN         NaN     NaN     NaN     NaN   123.0\nrestef            NaN         NaN     NaN     NaN     NaN   123.0\nrestwm            NaN         NaN     NaN     NaN     NaN   123.0\nexeref            NaN         NaN     NaN     NaN     NaN   123.0\nexerwm            NaN         NaN     NaN     NaN     NaN   123.0\nthal              NaN         NaN     NaN     NaN     NaN    50.0\nthalsev           NaN         NaN     NaN     NaN     NaN    51.0\nthalpul           NaN         NaN     NaN     NaN     NaN    97.0\nearlobe           NaN         NaN     NaN     NaN     NaN   123.0\ncmo          4.789474    3.270879     1.0    12.0     4.0     0.0\ncday        15.991228    9.184920     1.0    31.0    17.5     0.0\ncyr         84.596491    0.590774    82.0    87.0    85.0     0.0\nnum               NaN         NaN     NaN     NaN     NaN     0.0\nlmt               NaN         NaN     NaN     NaN     NaN     0.0\nladprox           NaN         NaN     NaN     NaN     NaN     0.0\nladdist           NaN         NaN     NaN     NaN     NaN     0.0\ndiag              NaN         NaN     NaN     NaN     NaN     0.0\ncxmain            NaN         NaN     NaN     NaN     NaN     0.0\nramus             NaN         NaN     NaN     NaN     NaN     0.0\nom1               NaN         NaN     NaN     NaN     NaN     0.0\nom2               NaN         NaN     NaN     NaN     NaN     0.0\nrcaprox           NaN         NaN     NaN     NaN     NaN     0.0\nrcadist           NaN         NaN     NaN     NaN     NaN     0.0\nlvx1              NaN         NaN     NaN     NaN     NaN    17.0\nlvx2              NaN         NaN     NaN     NaN     NaN    17.0\nlvx3              NaN         NaN     NaN     NaN     NaN    17.0\nlvx4              NaN         NaN     NaN     NaN     NaN    17.0\nlvf               NaN         NaN     NaN     NaN     NaN    13.0\ncathef            NaN         NaN     NaN     NaN     NaN    17.0\njunk              NaN         NaN     NaN     NaN     NaN   123.0\nname              NaN         NaN     NaN     NaN     NaN     0.0\n\n\nlong-beach-va.data\n                mean         std     min    max  median  mising\nid        101.310000   58.717824    1.00  202.0  100.50     0.0\nccf         0.000000    0.000000    0.00    0.0    0.00     0.0\nage        59.350000    7.811697   35.00   77.0   60.00     0.0\nsex              NaN         NaN     NaN    NaN     NaN     0.0\npainloc          NaN         NaN     NaN    NaN     NaN     0.0\npainexer         NaN         NaN     NaN    NaN     NaN     0.0\nrelrest          NaN         NaN     NaN    NaN     NaN     4.0\npncaden          NaN         NaN     NaN    NaN     NaN   200.0\ncp               NaN         NaN     NaN    NaN     NaN     0.0\ntrestbps  133.763889   21.537733    0.00  190.0  130.00    56.0\nhtn              NaN         NaN     NaN    NaN     NaN     3.0\nchol      178.746114  114.035232    0.00  458.0  216.00     7.0\nsmoke            NaN         NaN     NaN    NaN     NaN     5.0\ncigs       21.742105   16.256600    0.00   80.0   20.00     0.0\nyears      23.728723   16.362727    0.00   60.0   25.00     0.0\nfbs              NaN         NaN     NaN    NaN     NaN     7.0\ndm               NaN         NaN     NaN    NaN     NaN   157.0\nfamhist          NaN         NaN     NaN    NaN     NaN     8.0\nrestecg          NaN         NaN     NaN    NaN     NaN     0.0\nekgmo       6.195946    3.367833    1.00   12.0    6.00     0.0\nekgday     15.285714    8.848032    1.00   31.0   16.00     0.0\nekgyr      84.655405    1.353820   81.00   87.0   85.00     0.0\ndig              NaN         NaN     NaN    NaN     NaN    60.0\nprop             NaN         NaN     NaN    NaN     NaN    60.0\nnitr             NaN         NaN     NaN    NaN     NaN    59.0\npro              NaN         NaN     NaN    NaN     NaN    59.0\ndiuretic         NaN         NaN     NaN    NaN     NaN    73.0\nproto            NaN         NaN     NaN    NaN     NaN    53.0\nthaldur     6.216327    2.394937    1.50   12.0    6.00    53.0\nthaltime    5.047500    2.423601    0.00   11.3    5.00   160.0\nmet         6.106122    2.296669    2.00   13.0    6.00    53.0\nthalach   122.795918   21.990328   69.00  180.0  120.00    53.0\nthalrest   69.726027   11.952504   37.00  139.0   68.00    54.0\ntpeakbps  163.510638   24.787848  100.00  220.0  160.00    59.0\ntpeakbpd   88.170213   14.393926   11.00  120.0   90.00    59.0\ndummy            NaN         NaN     NaN    NaN     NaN    56.0\ntrestbpd   80.291667   11.737141    0.00  100.0   80.00    56.0\nexang            NaN         NaN     NaN    NaN     NaN    53.0\nxhypo            NaN         NaN     NaN    NaN     NaN    53.0\noldpeak     1.320833    1.106236   -0.50    4.0    1.50    56.0\nslope            NaN         NaN     NaN    NaN     NaN   101.0\nrldv5      16.007407    5.827747    2.00   36.0   16.00    65.0\nrldv5e     15.792593    6.125669    4.00   36.0   16.00    65.0\nca          0.000000    0.000000    0.00    0.0    0.00     0.0\nrestckm          NaN         NaN     NaN    NaN     NaN   200.0\nexerckm          NaN         NaN     NaN    NaN     NaN   199.0\nrestef      0.531071    0.146195    0.22    0.8    0.57   172.0\nrestwm           NaN         NaN     NaN    NaN     NaN   173.0\nexeref      0.550000    0.070711    0.50    0.6    0.55   198.0\nexerwm      0.333333    0.577350    0.00    1.0    0.00   197.0\nthal             NaN         NaN     NaN    NaN     NaN   159.0\nthalsev          NaN         NaN     NaN    NaN     NaN   169.0\nthalpul          NaN         NaN     NaN    NaN     NaN   199.0\nearlobe          NaN         NaN     NaN    NaN     NaN   199.0\ncmo         6.318182    3.414626    1.00   12.0    6.00     0.0\ncday       15.045000    8.849551    1.00   31.0   15.00     0.0\ncyr        83.565000    8.772718    1.00   87.0   85.00     0.0\nnum              NaN         NaN     NaN    NaN     NaN     0.0\nlmt              NaN         NaN     NaN    NaN     NaN     0.0\nladprox          NaN         NaN     NaN    NaN     NaN     0.0\nladdist          NaN         NaN     NaN    NaN     NaN     0.0\ndiag             NaN         NaN     NaN    NaN     NaN     0.0\ncxmain           NaN         NaN     NaN    NaN     NaN     0.0\nramus            NaN         NaN     NaN    NaN     NaN     0.0\nom1              NaN         NaN     NaN    NaN     NaN     0.0\nom2              NaN         NaN     NaN    NaN     NaN     1.0\nrcaprox          NaN         NaN     NaN    NaN     NaN     1.0\nrcadist          NaN         NaN     NaN    NaN     NaN     1.0\nlvx1             NaN         NaN     NaN    NaN     NaN     2.0\nlvx2             NaN         NaN     NaN    NaN     NaN     2.0\nlvx3             NaN         NaN     NaN    NaN     NaN     2.0\nlvx4             NaN         NaN     NaN    NaN     NaN     2.0\nlvf              NaN         NaN     NaN    NaN     NaN     3.0\ncathef           NaN         NaN     NaN    NaN     NaN    23.0\njunk             NaN         NaN     NaN    NaN     NaN    81.0\nname             NaN         NaN     NaN    NaN     NaN     0.0\n\n\n</pre> In\u00a0[54]: Copied! <pre>import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nfor name, dataset in datasets.items():\n    dataset.insert(0, 'dataset', name)\n\nfull_dataset = pd.concat(datasets.values())\n\nfull_dataset.to_csv(f'./datasets/procesados/full.csv', index=False, encoding='utf-8', header=True)\n</pre> import warnings warnings.simplefilter(action='ignore', category=FutureWarning)  for name, dataset in datasets.items():     dataset.insert(0, 'dataset', name)  full_dataset = pd.concat(datasets.values())  full_dataset.to_csv(f'./datasets/procesados/full.csv', index=False, encoding='utf-8', header=True)"},{"location":"casos-de-estudio/enfermedad-coronaria/02-datos/#datos","title":"Datos\u00b6","text":"<p>Tenemos a nuestra disposici\u00f3n 4 conjuntos de datos para trabajar sobre el problema. Estos son:</p> <ul> <li>cleveland.data:<ul> <li>Origen: Cleveland Clinic Foundation.</li> <li>Ejemplos: 303</li> </ul> </li> <li>hungarian.data:<ul> <li>Origen: Hungarian Institute of Cardiology, Budapest</li> <li>Ejemplos: 294</li> </ul> </li> <li>switzerland.data:<ul> <li>Origen: University Hospital, Zurich, Switzerland</li> <li>Ejemplos: 123</li> </ul> </li> <li>long-beach-va.data:<ul> <li>Origen: V.A. Medical Center, Long Beach, CA</li> <li>Ejemplos: 200</li> </ul> </li> </ul> <p>Estos datos fueron recolectados por:</p> <ul> <li>Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.</li> <li>University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.</li> <li>University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.</li> <li>V.A. Medical Center, Long Beach and Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D.</li> </ul> <p>A solicitud de los autores, se requiere que cualquier publicaci\u00f3n que utilice estos datos, incluya las anteriores atribuciones.</p>"},{"location":"casos-de-estudio/enfermedad-coronaria/02-datos/#presentacion-de-los-datos","title":"Presentaci\u00f3n de los Datos\u00b6","text":"<p>Los datos est\u00e1n en formato de archivo de texto plano, con extensi\u00f3n <code>.data</code>. Cada ejemplo tiene sus atributos separados por espacios, y se incluyen saltos de linea dentro de cada ejemplo, por lo que se precisa de un poco de procesamiento para poder leerlos.</p>"},{"location":"casos-de-estudio/enfermedad-coronaria/02-datos/#descripcion-de-los-datos","title":"Descripci\u00f3n de los Datos\u00b6","text":"<p>Dentro de cada conjunto de datos, cada l\u00ednea representa un paciente. Cada l\u00ednea contiene 76 atributos, separados por comas. El atributo objetivo es el numero 58, indicando con un numero entero de 0 a 4, la presencia de enfermedad card\u00edaca en el paciente, siendo 0 la ausencia. Existen varios ejemplos con valores faltantes en algunos atributos, esto es denotado por el numero -9.0. Existen varios atributos que no son usados directamente, o que contienen datos que fueron remplazados por valores falsos debido a contener informaci\u00f3n sensible. Igualmente, estos atributos son cosas como el nombre del paciente, o su n\u00famero de seguridad social, aspectos que no son relevantes para el problema.</p>"},{"location":"casos-de-estudio/enfermedad-coronaria/02-datos/#estadistica","title":"Estad\u00edstica\u00b6","text":"<p>Exploremos los datos un poco, para ver sus caracter\u00edsticas estad\u00edsticas. Para esto, usaremos la librer\u00eda <code>pandas</code> de Python, que nos permite leer los datos de una manera muy sencilla, y nos permite hacer operaciones estad\u00edsticas sobre ellos.</p>"},{"location":"casos-de-estudio/enfermedad-coronaria/02-datos/#exportacion-de-los-datos","title":"Exportaci\u00f3n de los Datos\u00b6","text":"<p>Dentro de este articulo, hemos le\u00eddo los datos y hemos realizado un procesamiento preliminar donde hemos eliminado datos inv\u00e1lidos, y hemos convertido los datos a un formato que es m\u00e1s f\u00e1cil de leer. Para futuro proceso, tanto en Python como en RapidMiner, es m\u00e1s sencillo trabajar con los datos en formato CSV, por lo que exportaremos los datos a este formato.</p>"},{"location":"casos-de-estudio/enfermedad-coronaria/02-datos/#seleccion-de-algoritmo","title":"Seleccion de Algoritmo\u00b6","text":"<p>Analizando los datos nos encontramos con que tenemos un problema de clasificaci\u00f3n, donde tenemos que predecir la presencia de enfermedad card\u00edaca en un paciente. La variable de salida es categ\u00f3rica, y tiene 2 posibles valores: 0 y 1. Dadas estas caracter\u00edsticas, podemos considerar de los algoritmos vistos lo siguiente:</p> <ul> <li>Regresi\u00f3n lineal: No es un algoritmo adecuado para este problema, ya que la variable de salida es categ\u00f3rica, y no continua.</li> <li>Regresi\u00f3n log\u00edstica: Se trata de un problema de clasificaci\u00f3n de 5 clases, pero como solo nos importa la ausencia o no de enfermedad card\u00edaca, podemos utilizarlo. Requiere convertir la variable de salida a una variable binaria.</li> <li>An\u00e1lisis discriminante lineal: Utilizado mayormente para problemas de clasificaci\u00f3n multiclase, lo cual se acerca mas al problema sin necesidad de convertir la variable de salida. Pero como solo nos importa la ausencia o no de enfermedad card\u00edaca, el problema se reduce a un problema de clasificaci\u00f3n binaria. Sigue siendo posible utilizarlo, dado que 2 clases es un caso especial de N clases.</li> <li>kNN: Este algoritmo es adecuado para el problema, ya que es un problema de clasificaci\u00f3n, y la variable de salida es categ\u00f3rica.</li> <li>Bayesiano ingenuo: Debido a la presencia de atributos continuos, este algoritmo solo podria ser utilizado si se discretizan los datos.</li> </ul> <p>Con estas consideraciones, estudiaremos la creaci\u00f3n de un modelo utilizando el algoritmo de regresi\u00f3n log\u00edstica, tanto en Python como en RapidMiner.</p>"},{"location":"casos-de-estudio/enfermedad-coronaria/03-regresion-logistica-rapidminer/","title":"Modelo de Regresi\u00f3n Logistica en RapidMiner","text":""},{"location":"casos-de-estudio/enfermedad-coronaria/03-regresion-logistica-rapidminer/#modelo-de-regresion-logistica-en-rapidminer","title":"Modelo de Regresi\u00f3n Logistica en RapidMiner\u00b6","text":"<p>Visto en el articulo anterior, analizamos los datos disponibles en los conjuntos de datos de enfermedad card\u00edaca y, a partir de nuestras observaciones, decidimos que la regresi\u00f3n log\u00edstica ser\u00eda un buen algoritmo para usar en este caso dado que la variable objetivo es binaria (0 o 1). En este art\u00edculo, veremos c\u00f3mo implementar la regresi\u00f3n log\u00edstica en RapidMiner.</p>"},{"location":"casos-de-estudio/enfermedad-coronaria/03-regresion-logistica-rapidminer/#cargar-datos","title":"Cargar datos\u00b6","text":"<p>Anteriormente, utilizamos pandas para combinar los conjuntos de datos en uno solo, y mejor formatearlo para su importacion en RapidMiner. La carga es requiere importar el archivo full.csv creado en el articulo anterior. En el proceso, verificamos que los datos tengan los tipos correctos, y corregimos basados en los tipos de datos vistos en el articulo anterior. Luego de importado, introducimos el conjunto de datos al proceso de RapidMiner.</p>"},{"location":"casos-de-estudio/enfermedad-coronaria/03-regresion-logistica-rapidminer/#preprocesamiento","title":"Preprocesamiento\u00b6","text":"<p>Primera tarea de preprocesamiento que realizaremos, es la de convertir la variable de salida en un atributo binario. Para ello, utilizaremos el operador <code>Replace</code>, que nos permite reemplazar valores en un atributo. En este caso, reemplazaremos todo valor de la columna <code>num</code> que sea mayor a 0, por el valor 1. Tambien nos aseguraremos de asignar a este atributo el rol de <code>label</code> utilizando <code>Set Role</code>, y convertirlo en un atributo binario utilizando <code>Nominal to Binominal</code>.</p> <p>Luego, inspeccionemos los casos de atributos con valores faltantes. Para ello, revisamos la estad\u00edstica del conjunto resultante. Aqu\u00ed podemos ver que existen varios atributos con valores faltantes, la cantidad de estos variando entre muy pocos y casi todo el conjunto de los casos. Tomamos nota de estos atributos, decidiendo que los atributos con m\u00e1s de 400 valores faltantes ser\u00e1n eliminados. Estos atributos son:</p> <ul> <li>ca</li> <li>cathef</li> <li>cigs</li> <li>diag</li> <li>dm</li> <li>earlobe</li> <li>exerckm</li> <li>exeref</li> <li>exerwm</li> <li>famhist</li> <li>junk</li> <li>om2</li> <li>pncaden</li> <li>ramus</li> <li>restckm</li> <li>restef</li> <li>restwm</li> <li>rldv5</li> <li>smoke</li> <li>thal</li> <li>thalpul</li> <li>thalsev</li> <li>thaltime</li> <li>years</li> </ul> <p>Utilizamos el operador <code>Select Attributes</code> para eliminar estos atributos del conjunto de datos.</p> <p>Siguiendo buscando atributos que no aporten a la solucion del problema, nos encontramos con la documentaci\u00f3n de los conjuntos de datos, donde se especifica la definici\u00f3n de estos. En esta documentaci\u00f3n, se especifica que varios atributos no son utilizados, no estan bien definidos, o no aportan al problema. Estos atributos son:</p> <ul> <li>ccf</li> <li>dummy</li> <li>lvf</li> <li>lvx1</li> <li>lvx2</li> <li>lvx3</li> <li>lvx4</li> <li>name</li> </ul> <p>Nuevamente utilizamos el operador <code>Select Attributes</code> para eliminar estos atributos del conjunto de datos. Por el momento, nos quedamos con los atributos restantes, y continuamos con el preprocesamiento de los datos.</p> <p>Pasamos a reemplazar los valores faltantes de los atributos restantes. Para ello, utilizamos el operador <code>Replace Missing Values</code> para reemplazar los valores faltantes de los atributos restantes. En este caso, utilizaremos el valor promedio de cada atributo. Esto solo lo haremos para los atributos num\u00e9ricos reales, ya que en los atributos categ\u00f3ricos, el valor promedio no tiene sentido. Para los atributos categ\u00f3ricos, utilizaremos el valor m\u00e1s frecuente para reemplazar los valores faltantes. En RapidMiner, esto requiere seleccionar el valor de reemplazo como <code>average</code>, que en atributos nominales se convierte en la moda del atributo. Utilizaremos dos operadores <code>Replace Missing Values</code>, uno para los atributos num\u00e9ricos reales, y otro para los atributos categ\u00f3ricos. Ambos utilizando el valor <code>average</code> como valor de reemplazo.</p> <p>El siguiente paso es remover los atributos correlacionados. RapidMiner nos permite hacer esto utilizando el operador <code>Remove Correlated Attributes</code>. En este caso, utilizaremos un umbral de 0.35 para remover atributos de correlaci\u00f3n significativa.</p> <p>Volvemos ahora a revisar los atributos a utilizar. Para optimizar esta selecci\u00f3n, utilizaremos el operador <code>Select Attributes</code> para seleccionar los 20 atributos m\u00e1s relevantes para el modelo. Esto lo haremos dentro de un <code>Cross Validation</code> de 10 folds, para prevenir contaminaci\u00f3n accidental de los datos en la selecci\u00f3n de atributos. Dentro de este operador, utilizaremos el operador <code>Logistic Regression</code> para evaluar el la seleccion de atributos dentro de un <code>Split Validation</code> de 70% para entrenamiento y 30% para prueba.</p> <p>Como ultimo paso en el preprocesamiento de los datos, utilizaremos el operador <code>Optimize Selection</code> utilizando direcci\u00f3n <code>forwards</code> para seleccionar los atributos m\u00e1s relevantes para el modelo, y filtraremos cualquier ejemplo restante que tenga valores nulos utilizando <code>Filter Examples</code>. Esto lo haremos dentro del propio operador de <code>Cross Validation</code> utilizado para entrenar el modelo como veremos en la siguiente secci\u00f3n. Esto es para prevenir contaminaci\u00f3n accidental de los datos en la selecci\u00f3n de atributos.</p>"},{"location":"casos-de-estudio/enfermedad-coronaria/03-regresion-logistica-rapidminer/#entrenamiento","title":"Entrenamiento\u00b6","text":"<p>Para la etapa de entrenamiento, utilizaremos el operador <code>Logistic Regression</code> para entrenar el modelo. En este caso, utilizaremos un <code>Cross Validation</code> de 10 folds para evaluar el modelo, y una normalizaci\u00f3n de los datos utilizando el metodo <code>z-normalization</code> y el operador <code>Normalize</code> previo a entrenar el modelo para los datos de entrenamiento, y previa a probar el modelo para los datos de prueba.</p> <p>El modelo de preprocesamiento creado por el operador <code>Normalize</code> es utilizado luego en la etapa de prueba para normalizar los datos de prueba. La aplicaci\u00f3n tanto del modelo de preprocesamiento como del modelo predictivo en la etapa de prueba se realiza utilizando 2 operadores <code>Apply Model</code>.</p>"},{"location":"casos-de-estudio/enfermedad-coronaria/03-regresion-logistica-rapidminer/#proceso-completo","title":"Proceso Completo\u00b6","text":"<p>El proceso completo se puede ver en las siguientes imagenes:</p> <p> </p>"},{"location":"casos-de-estudio/enfermedad-coronaria/03-regresion-logistica-rapidminer/#evaluacion","title":"Evaluaci\u00f3n\u00b6","text":"<p>Una vez ejecutado el proceso, podemos ver los resultados de la evaluaci\u00f3n del modelo en la pesta\u00f1a <code>Results</code>. Aqu\u00ed podemos ver la precisi\u00f3n del modelo y su matriz de confusi\u00f3n en la salida de <code>Performance Vector</code>:</p> <p></p> <p>El resultado es una precisi\u00f3n del 91.63%. Considero que este es un buen resultado, dado la mala calidad de los datos, con muchos valores faltantes y atributos no relevantes.</p>"},{"location":"casos-de-estudio/enfermedad-coronaria/04-regresion-logistica-python/","title":"Modelo de Regresi\u00f3n Logistica en Python","text":"In\u00a0[26]: Copied! <pre>import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\n</pre> import pandas as pd import numpy as np from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.model_selection import cross_val_predict from sklearn.pipeline import Pipeline from sklearn.feature_selection import SelectFromModel from sklearn.metrics import classification_report from sklearn.metrics import confusion_matrix from sklearn.metrics import roc_auc_score In\u00a0[27]: Copied! <pre>dataset: pd.DataFrame = pd.read_csv('./datasets/procesados/full.csv')\n</pre> dataset: pd.DataFrame = pd.read_csv('./datasets/procesados/full.csv') <p>Aseguremonos antes de continuar, de que cada columna tenga el tipo de datos correcto.</p> In\u00a0[28]: Copied! <pre>columns = [\n    ('id', 'Int64'),\n    ('ccf', 'Int64'),\n    ('age', 'Int64'),\n    ('sex', 'category'),\n    ('painloc', 'category'),\n    ('painexer', 'category'),\n    ('relrest', 'category'),\n    ('pncaden', 'category'),\n    ('cp', 'category'),\n    ('trestbps', 'float'),\n    ('htn', 'object'),\n    ('chol', 'float'),\n    ('smoke', 'category'),\n    ('cigs', 'Int64'),\n    ('years', 'Int64'),\n    ('fbs', 'category'),\n    ('dm', 'category'),\n    ('famhist', 'category'),\n    ('restecg', 'category'),\n    ('ekgmo', 'Int64'),\n    ('ekgday', 'Int64'),\n    ('ekgyr', 'Int64'),\n    ('dig', 'category'),\n    ('prop', 'category'),\n    ('nitr', 'category'),\n    ('pro', 'category'),\n    ('diuretic', 'category'),\n    ('proto', 'category'),\n    ('thaldur', 'float'),\n    ('thaltime', 'float'),\n    ('met', 'float'),\n    ('thalach', 'float'),\n    ('thalrest', 'float'),\n    ('tpeakbps', 'float'),\n    ('tpeakbpd', 'float'),\n    ('dummy', 'object'),\n    ('trestbpd', 'float'),\n    ('exang', 'category'),\n    ('xhypo', 'category'),\n    ('oldpeak', 'float'),\n    ('slope', 'category'),\n    ('rldv5', 'float'),\n    ('rldv5e', 'float'),\n    ('ca', 'Int64'),\n    ('restckm', 'object'),\n    ('exerckm', 'object'),\n    ('restef', 'float'),\n    ('restwm', 'category'),\n    ('exeref', 'float'),\n    ('exerwm', 'float'),\n    ('thal', 'category'),\n    ('thalsev', 'object'),\n    ('thalpul', 'object'),\n    ('earlobe', 'object'),\n    ('cmo', 'Int64'),\n    ('cday', 'Int64'),\n    ('cyr', 'Int64'),\n    ('num', 'category'),\n    ('lmt', 'object'),\n    ('ladprox', 'object'),\n    ('laddist', 'object'),\n    ('diag', 'object'),\n    ('cxmain', 'object'),\n    ('ramus', 'object'),\n    ('om1', 'object'),\n    ('om2', 'object'),\n    ('rcaprox', 'object'),\n    ('rcadist', 'object'),\n    ('lvx1', 'object'),\n    ('lvx2', 'object'),\n    ('lvx3', 'object'),\n    ('lvx4', 'object'),\n    ('lvf', 'object'),\n    ('cathef', 'object'),\n    ('junk', 'object'),\n    ('name', 'string'),\n    ('dataset', 'string')\n]\n\ndataset = dataset.astype(dict(columns))\n\nprint(dataset.info())\n</pre> columns = [     ('id', 'Int64'),     ('ccf', 'Int64'),     ('age', 'Int64'),     ('sex', 'category'),     ('painloc', 'category'),     ('painexer', 'category'),     ('relrest', 'category'),     ('pncaden', 'category'),     ('cp', 'category'),     ('trestbps', 'float'),     ('htn', 'object'),     ('chol', 'float'),     ('smoke', 'category'),     ('cigs', 'Int64'),     ('years', 'Int64'),     ('fbs', 'category'),     ('dm', 'category'),     ('famhist', 'category'),     ('restecg', 'category'),     ('ekgmo', 'Int64'),     ('ekgday', 'Int64'),     ('ekgyr', 'Int64'),     ('dig', 'category'),     ('prop', 'category'),     ('nitr', 'category'),     ('pro', 'category'),     ('diuretic', 'category'),     ('proto', 'category'),     ('thaldur', 'float'),     ('thaltime', 'float'),     ('met', 'float'),     ('thalach', 'float'),     ('thalrest', 'float'),     ('tpeakbps', 'float'),     ('tpeakbpd', 'float'),     ('dummy', 'object'),     ('trestbpd', 'float'),     ('exang', 'category'),     ('xhypo', 'category'),     ('oldpeak', 'float'),     ('slope', 'category'),     ('rldv5', 'float'),     ('rldv5e', 'float'),     ('ca', 'Int64'),     ('restckm', 'object'),     ('exerckm', 'object'),     ('restef', 'float'),     ('restwm', 'category'),     ('exeref', 'float'),     ('exerwm', 'float'),     ('thal', 'category'),     ('thalsev', 'object'),     ('thalpul', 'object'),     ('earlobe', 'object'),     ('cmo', 'Int64'),     ('cday', 'Int64'),     ('cyr', 'Int64'),     ('num', 'category'),     ('lmt', 'object'),     ('ladprox', 'object'),     ('laddist', 'object'),     ('diag', 'object'),     ('cxmain', 'object'),     ('ramus', 'object'),     ('om1', 'object'),     ('om2', 'object'),     ('rcaprox', 'object'),     ('rcadist', 'object'),     ('lvx1', 'object'),     ('lvx2', 'object'),     ('lvx3', 'object'),     ('lvx4', 'object'),     ('lvf', 'object'),     ('cathef', 'object'),     ('junk', 'object'),     ('name', 'string'),     ('dataset', 'string') ]  dataset = dataset.astype(dict(columns))  print(dataset.info()) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 899 entries, 0 to 898\nData columns (total 77 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   dataset   899 non-null    string  \n 1   id        899 non-null    Int64   \n 2   ccf       899 non-null    Int64   \n 3   age       899 non-null    Int64   \n 4   sex       899 non-null    category\n 5   painloc   617 non-null    category\n 6   painexer  617 non-null    category\n 7   relrest   613 non-null    category\n 8   pncaden   0 non-null      category\n 9   cp        899 non-null    category\n 10  trestbps  840 non-null    float64 \n 11  htn       865 non-null    object  \n 12  chol      869 non-null    float64 \n 13  smoke     230 non-null    category\n 14  cigs      479 non-null    Int64   \n 15  years     467 non-null    Int64   \n 16  fbs       809 non-null    category\n 17  dm        95 non-null     category\n 18  famhist   477 non-null    category\n 19  restecg   897 non-null    category\n 20  ekgmo     846 non-null    Int64   \n 21  ekgday    845 non-null    Int64   \n 22  ekgyr     846 non-null    Int64   \n 23  dig       831 non-null    category\n 24  prop      833 non-null    category\n 25  nitr      834 non-null    category\n 26  pro       836 non-null    category\n 27  diuretic  817 non-null    category\n 28  proto     787 non-null    category\n 29  thaldur   843 non-null    float64 \n 30  thaltime  446 non-null    float64 \n 31  met       794 non-null    float64 \n 32  thalach   844 non-null    float64 \n 33  thalrest  843 non-null    float64 \n 34  tpeakbps  836 non-null    float64 \n 35  tpeakbpd  836 non-null    float64 \n 36  dummy     840 non-null    object  \n 37  trestbpd  840 non-null    float64 \n 38  exang     844 non-null    category\n 39  xhypo     841 non-null    category\n 40  oldpeak   837 non-null    float64 \n 41  slope     591 non-null    category\n 42  rldv5     474 non-null    float64 \n 43  rldv5e    757 non-null    float64 \n 44  ca        291 non-null    Int64   \n 45  restckm   0 non-null      object  \n 46  exerckm   1 non-null      object  \n 47  restef    28 non-null     float64 \n 48  restwm    30 non-null     category\n 49  exeref    2 non-null      float64 \n 50  exerwm    5 non-null      float64 \n 51  thal      422 non-null    category\n 52  thalsev   130 non-null    object  \n 53  thalpul   44 non-null     object  \n 54  earlobe   1 non-null      object  \n 55  cmo       888 non-null    Int64   \n 56  cday      890 non-null    Int64   \n 57  cyr       890 non-null    Int64   \n 58  num       899 non-null    category\n 59  lmt       624 non-null    object  \n 60  ladprox   663 non-null    object  \n 61  laddist   653 non-null    object  \n 62  diag      341 non-null    object  \n 63  cxmain    664 non-null    object  \n 64  ramus     332 non-null    object  \n 65  om1       628 non-null    object  \n 66  om2       327 non-null    object  \n 67  rcaprox   654 non-null    object  \n 68  rcadist   629 non-null    object  \n 69  lvx1      880 non-null    object  \n 70  lvx2      880 non-null    object  \n 71  lvx3      880 non-null    object  \n 72  lvx4      880 non-null    object  \n 73  lvf       883 non-null    object  \n 74  cathef    311 non-null    object  \n 75  junk      119 non-null    object  \n 76  name      899 non-null    string  \ndtypes: Int64(12), category(23), float64(16), object(24), string(2)\nmemory usage: 414.0+ KB\nNone\n</pre> In\u00a0[29]: Copied! <pre>for value in dataset['num'].unique():\n    if value &gt; 0:\n        dataset['num'] = dataset['num'].replace(value, 1)\n\nprint(dataset['num'].unique())\n</pre> for value in dataset['num'].unique():     if value &gt; 0:         dataset['num'] = dataset['num'].replace(value, 1)  print(dataset['num'].unique()) <pre>[0, 1]\nCategories (2, int64): [0, 1]\n</pre> <p>Luego, tomemos todas las columnas que contienen una cantidad de valores faltantes mayor a 400 y eliminemos esas columnas del dataset. Este numero fue seleccionado basados en que existen 899 registros en el dataset.</p> In\u00a0[30]: Copied! <pre>examples_count = dataset.shape[0]\ndataset = dataset.dropna(axis=1, thresh=examples_count - 400)\nprint(dataset.info())\n</pre> examples_count = dataset.shape[0] dataset = dataset.dropna(axis=1, thresh=examples_count - 400) print(dataset.info()) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 899 entries, 0 to 898\nData columns (total 53 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   dataset   899 non-null    string  \n 1   id        899 non-null    Int64   \n 2   ccf       899 non-null    Int64   \n 3   age       899 non-null    Int64   \n 4   sex       899 non-null    category\n 5   painloc   617 non-null    category\n 6   painexer  617 non-null    category\n 7   relrest   613 non-null    category\n 8   cp        899 non-null    category\n 9   trestbps  840 non-null    float64 \n 10  htn       865 non-null    object  \n 11  chol      869 non-null    float64 \n 12  fbs       809 non-null    category\n 13  restecg   897 non-null    category\n 14  ekgmo     846 non-null    Int64   \n 15  ekgday    845 non-null    Int64   \n 16  ekgyr     846 non-null    Int64   \n 17  dig       831 non-null    category\n 18  prop      833 non-null    category\n 19  nitr      834 non-null    category\n 20  pro       836 non-null    category\n 21  diuretic  817 non-null    category\n 22  proto     787 non-null    category\n 23  thaldur   843 non-null    float64 \n 24  met       794 non-null    float64 \n 25  thalach   844 non-null    float64 \n 26  thalrest  843 non-null    float64 \n 27  tpeakbps  836 non-null    float64 \n 28  tpeakbpd  836 non-null    float64 \n 29  dummy     840 non-null    object  \n 30  trestbpd  840 non-null    float64 \n 31  exang     844 non-null    category\n 32  xhypo     841 non-null    category\n 33  oldpeak   837 non-null    float64 \n 34  slope     591 non-null    category\n 35  rldv5e    757 non-null    float64 \n 36  cmo       888 non-null    Int64   \n 37  cday      890 non-null    Int64   \n 38  cyr       890 non-null    Int64   \n 39  num       899 non-null    category\n 40  lmt       624 non-null    object  \n 41  ladprox   663 non-null    object  \n 42  laddist   653 non-null    object  \n 43  cxmain    664 non-null    object  \n 44  om1       628 non-null    object  \n 45  rcaprox   654 non-null    object  \n 46  rcadist   629 non-null    object  \n 47  lvx1      880 non-null    object  \n 48  lvx2      880 non-null    object  \n 49  lvx3      880 non-null    object  \n 50  lvx4      880 non-null    object  \n 51  lvf       883 non-null    object  \n 52  name      899 non-null    string  \ndtypes: Int64(9), category(17), float64(11), object(14), string(2)\nmemory usage: 278.6+ KB\nNone\n</pre> <p>Ahora procedemos a eliminar las columnas que son marcadas por el autor del dataset como no relevantes para el modelo. Estas columnas son <code>ccf</code>, <code>dummy</code>, <code>lvx1</code>, <code>lvx2</code>, <code>lvx3</code>, <code>lvx4</code>, <code>lvf</code>, <code>junk</code>, y <code>name</code>.</p> In\u00a0[31]: Copied! <pre>columns = [\n    'ccf',\n    'dummy',\n    'lvx1',\n    'lvx2',\n    'lvx3',\n    'lvx4',\n    'lvf',\n    'name'\n]\n\ndataset = dataset.drop(columns, axis=1)\n# dataset = dataset.select_dtypes(exclude=['object'])\nprint(dataset.info())\n</pre> columns = [     'ccf',     'dummy',     'lvx1',     'lvx2',     'lvx3',     'lvx4',     'lvf',     'name' ]  dataset = dataset.drop(columns, axis=1) # dataset = dataset.select_dtypes(exclude=['object']) print(dataset.info()) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 899 entries, 0 to 898\nData columns (total 45 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   dataset   899 non-null    string  \n 1   id        899 non-null    Int64   \n 2   age       899 non-null    Int64   \n 3   sex       899 non-null    category\n 4   painloc   617 non-null    category\n 5   painexer  617 non-null    category\n 6   relrest   613 non-null    category\n 7   cp        899 non-null    category\n 8   trestbps  840 non-null    float64 \n 9   htn       865 non-null    object  \n 10  chol      869 non-null    float64 \n 11  fbs       809 non-null    category\n 12  restecg   897 non-null    category\n 13  ekgmo     846 non-null    Int64   \n 14  ekgday    845 non-null    Int64   \n 15  ekgyr     846 non-null    Int64   \n 16  dig       831 non-null    category\n 17  prop      833 non-null    category\n 18  nitr      834 non-null    category\n 19  pro       836 non-null    category\n 20  diuretic  817 non-null    category\n 21  proto     787 non-null    category\n 22  thaldur   843 non-null    float64 \n 23  met       794 non-null    float64 \n 24  thalach   844 non-null    float64 \n 25  thalrest  843 non-null    float64 \n 26  tpeakbps  836 non-null    float64 \n 27  tpeakbpd  836 non-null    float64 \n 28  trestbpd  840 non-null    float64 \n 29  exang     844 non-null    category\n 30  xhypo     841 non-null    category\n 31  oldpeak   837 non-null    float64 \n 32  slope     591 non-null    category\n 33  rldv5e    757 non-null    float64 \n 34  cmo       888 non-null    Int64   \n 35  cday      890 non-null    Int64   \n 36  cyr       890 non-null    Int64   \n 37  num       899 non-null    category\n 38  lmt       624 non-null    object  \n 39  ladprox   663 non-null    object  \n 40  laddist   653 non-null    object  \n 41  cxmain    664 non-null    object  \n 42  om1       628 non-null    object  \n 43  rcaprox   654 non-null    object  \n 44  rcadist   629 non-null    object  \ndtypes: Int64(8), category(17), float64(11), object(8), string(1)\nmemory usage: 221.5+ KB\nNone\n</pre> <p>En este punto nos encontramos con 51 atributos. Dejemos de lado la seleccion de atributos por el momento y continuemos con otras tareas de preprocesamiento.</p> <p>Ahora, tomemos las columnas que contienen valores faltantes y reemplacemos esos valores por el promedio de la columna. En el caso de numeros reales, tomemos el promedio de los valores. En el caso de valores discretos, tomemos el valor mas frecuente.</p> In\u00a0[32]: Copied! <pre># Reeplazemos valores faltantes de columnas 'float', por la media.\nfor column in dataset.columns:\n    if dataset[column].dtype == 'float64':\n        dataset[column] = dataset[column].fillna(dataset[column].mean())\n\n# Reemplazamos valores faltantes de columnas 'category', por la moda.\nfor column in dataset.columns:\n    if dataset[column].dtype == 'category':\n        dataset[column] = dataset[column].fillna(dataset[column].mode()[0])\n\n# Reemplazamos valores faltantes de columnas 'Int64', por la moda.\nfor column in dataset.columns:\n    if dataset[column].dtype == 'Int64':\n        dataset[column] = dataset[column].fillna(dataset[column].mode()[0])\n</pre> # Reeplazemos valores faltantes de columnas 'float', por la media. for column in dataset.columns:     if dataset[column].dtype == 'float64':         dataset[column] = dataset[column].fillna(dataset[column].mean())  # Reemplazamos valores faltantes de columnas 'category', por la moda. for column in dataset.columns:     if dataset[column].dtype == 'category':         dataset[column] = dataset[column].fillna(dataset[column].mode()[0])  # Reemplazamos valores faltantes de columnas 'Int64', por la moda. for column in dataset.columns:     if dataset[column].dtype == 'Int64':         dataset[column] = dataset[column].fillna(dataset[column].mode()[0]) <p>Ya con los valores faltantes que podemos reemplazar, reemplazados, eliminemos las filas que contienen valores faltantes en las columnas restantes.</p> In\u00a0[33]: Copied! <pre>dataset = dataset.dropna(axis=0)\nprint(dataset.info())\n</pre> dataset = dataset.dropna(axis=0) print(dataset.info()) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 572 entries, 0 to 898\nData columns (total 45 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   dataset   572 non-null    string  \n 1   id        572 non-null    Int64   \n 2   age       572 non-null    Int64   \n 3   sex       572 non-null    category\n 4   painloc   572 non-null    category\n 5   painexer  572 non-null    category\n 6   relrest   572 non-null    category\n 7   cp        572 non-null    category\n 8   trestbps  572 non-null    float64 \n 9   htn       572 non-null    object  \n 10  chol      572 non-null    float64 \n 11  fbs       572 non-null    category\n 12  restecg   572 non-null    category\n 13  ekgmo     572 non-null    Int64   \n 14  ekgday    572 non-null    Int64   \n 15  ekgyr     572 non-null    Int64   \n 16  dig       572 non-null    category\n 17  prop      572 non-null    category\n 18  nitr      572 non-null    category\n 19  pro       572 non-null    category\n 20  diuretic  572 non-null    category\n 21  proto     572 non-null    category\n 22  thaldur   572 non-null    float64 \n 23  met       572 non-null    float64 \n 24  thalach   572 non-null    float64 \n 25  thalrest  572 non-null    float64 \n 26  tpeakbps  572 non-null    float64 \n 27  tpeakbpd  572 non-null    float64 \n 28  trestbpd  572 non-null    float64 \n 29  exang     572 non-null    category\n 30  xhypo     572 non-null    category\n 31  oldpeak   572 non-null    float64 \n 32  slope     572 non-null    category\n 33  rldv5e    572 non-null    float64 \n 34  cmo       572 non-null    Int64   \n 35  cday      572 non-null    Int64   \n 36  cyr       572 non-null    Int64   \n 37  num       572 non-null    category\n 38  lmt       572 non-null    object  \n 39  ladprox   572 non-null    object  \n 40  laddist   572 non-null    object  \n 41  cxmain    572 non-null    object  \n 42  om1       572 non-null    object  \n 43  rcaprox   572 non-null    object  \n 44  rcadist   572 non-null    object  \ndtypes: Int64(8), category(17), float64(11), object(8), string(1)\nmemory usage: 146.3+ KB\nNone\n</pre> <p>Volvamos a la selecci\u00f3n de atributos momentariamente. Vamos a investigar la correlacion entre los atributos, y eliminar aquellos que tengan una correlaci\u00f3n mayor al 35%.</p> In\u00a0[34]: Copied! <pre>correlation_matrix = dataset.drop(['dataset', 'id', 'num'], axis=1).corr().abs()\n\nupper = correlation_matrix.where(\n    np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n)\n\nto_drop = [column for column in upper.columns if any(upper[column] &gt; 0.35)]\n\ndataset = dataset.drop(dataset[to_drop], axis=1)\nprint(dataset.info())\n</pre> correlation_matrix = dataset.drop(['dataset', 'id', 'num'], axis=1).corr().abs()  upper = correlation_matrix.where(     np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool) )  to_drop = [column for column in upper.columns if any(upper[column] &gt; 0.35)]  dataset = dataset.drop(dataset[to_drop], axis=1) print(dataset.info()) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 572 entries, 0 to 898\nData columns (total 29 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   dataset   572 non-null    string  \n 1   id        572 non-null    Int64   \n 2   age       572 non-null    Int64   \n 3   sex       572 non-null    category\n 4   painloc   572 non-null    category\n 5   painexer  572 non-null    category\n 6   trestbps  572 non-null    float64 \n 7   htn       572 non-null    object  \n 8   chol      572 non-null    float64 \n 9   fbs       572 non-null    category\n 10  restecg   572 non-null    category\n 11  ekgmo     572 non-null    Int64   \n 12  ekgday    572 non-null    Int64   \n 13  dig       572 non-null    category\n 14  prop      572 non-null    category\n 15  nitr      572 non-null    category\n 16  pro       572 non-null    category\n 17  diuretic  572 non-null    category\n 18  thaldur   572 non-null    float64 \n 19  tpeakbpd  572 non-null    float64 \n 20  oldpeak   572 non-null    float64 \n 21  num       572 non-null    category\n 22  lmt       572 non-null    object  \n 23  ladprox   572 non-null    object  \n 24  laddist   572 non-null    object  \n 25  cxmain    572 non-null    object  \n 26  om1       572 non-null    object  \n 27  rcaprox   572 non-null    object  \n 28  rcadist   572 non-null    object  \ndtypes: Int64(4), category(11), float64(5), object(8), string(1)\nmemory usage: 94.6+ KB\nNone\n</pre> <p>En este punto tenemos un conjunto de datos completo, procedemos a realizar una optimizaci\u00f3n de atributos basada en pesos para los atributos. Esto lo hacemos con <code>SelectFromModel</code>, utilizando un modelo de regresi\u00f3n logistica para calcular los pesos de los atributos. Esta operacion la preparamos para ser ejecutada dentro de un Pipeline en la etapa de entrenamiento del modelo.</p> In\u00a0[35]: Copied! <pre>pipeline_for_selection = Pipeline([\n    ('scaler', StandardScaler()),\n    ('classification', LogisticRegression(max_iter=1000000))\n])\n\n\ndef coef_getter(estimator):\n    return estimator.named_steps['classification'].coef_\n\nselector = SelectFromModel(pipeline_for_selection, threshold='mean', importance_getter=coef_getter)\n</pre> pipeline_for_selection = Pipeline([     ('scaler', StandardScaler()),     ('classification', LogisticRegression(max_iter=1000000)) ])   def coef_getter(estimator):     return estimator.named_steps['classification'].coef_  selector = SelectFromModel(pipeline_for_selection, threshold='mean', importance_getter=coef_getter) <p>Con esta ultima operaci\u00f3n de preprocesamiento, nos quedamos con 18 atributos, mas 2 de metadata, y uno de clase. En total, 21 columnas.</p> In\u00a0[36]: Copied! <pre>inputs = dataset.drop(['num'], axis=1).drop(['dataset', 'id'], axis=1)\noutputs = dataset['num']\n\npipeline = Pipeline([\n    ('selected_features', selector),\n    ('scaler', StandardScaler()),\n    ('classification', LogisticRegression(max_iter=1000000))\n])\n\noutput_predictions = cross_val_predict(pipeline, inputs, outputs, cv=10)\n</pre> inputs = dataset.drop(['num'], axis=1).drop(['dataset', 'id'], axis=1) outputs = dataset['num']  pipeline = Pipeline([     ('selected_features', selector),     ('scaler', StandardScaler()),     ('classification', LogisticRegression(max_iter=1000000)) ])  output_predictions = cross_val_predict(pipeline, inputs, outputs, cv=10) In\u00a0[37]: Copied! <pre>print('ROC AUC Score: \\n', roc_auc_score(outputs, output_predictions), end='\\n\\n')\nprint('Confusion Matrix: \\n', confusion_matrix(outputs, output_predictions), end='\\n\\n')\nprint('Classification Report: \\n', classification_report(outputs, output_predictions), end='\\n\\n')\n</pre> print('ROC AUC Score: \\n', roc_auc_score(outputs, output_predictions), end='\\n\\n') print('Confusion Matrix: \\n', confusion_matrix(outputs, output_predictions), end='\\n\\n') print('Classification Report: \\n', classification_report(outputs, output_predictions), end='\\n\\n') <pre>ROC AUC Score: \n 0.9920901655485683\n\nConfusion Matrix: \n [[210   1]\n [  4 357]]\n\nClassification Report: \n               precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99       211\n           1       1.00      0.99      0.99       361\n\n    accuracy                           0.99       572\n   macro avg       0.99      0.99      0.99       572\nweighted avg       0.99      0.99      0.99       572\n\n\n</pre> <p>Con una precisi\u00f3n del 99%, podemos decir que el modelo de regresi\u00f3n logistica es un buen modelo para este dataset.</p>"},{"location":"casos-de-estudio/enfermedad-coronaria/04-regresion-logistica-python/#modelo-de-regresion-logistica-en-python","title":"Modelo de Regresi\u00f3n Logistica en Python\u00b6","text":"<p>En el articulo anterior, creamos un modelo de predicci\u00f3n de enfermedades cardiacas usando un modelo de regresi\u00f3n logistica en RapidMiner. En este articulo, vamos a crear el mismo modelo usando Python.</p>"},{"location":"casos-de-estudio/enfermedad-coronaria/04-regresion-logistica-python/#carga-de-datos","title":"Carga de datos\u00b6","text":"<p>Utilizaremos el mismo archivo de datos que en el articulo anterior. Este archivo fue creado luego de procesar las cuatro bases de datos originales, conviertiendolas a un formato csv mas f\u00e1cil de importar a las distintas herramientas de aprendizaje automatizado</p>"},{"location":"casos-de-estudio/enfermedad-coronaria/04-regresion-logistica-python/#preprocesamiento","title":"Preprocesamiento\u00b6","text":"<p>En el articulo anterior, realizamos un preprocesamiento de los datos usando RapidMiner. En este caso, realizaremos el preprocesamiento usando Python. Para ello, utilizaremos las librerias Pandas y Numpy.</p> <p>Inicialmente, tomaremos la variable objetivo <code>num</code> y convertiremos todo valor distinto de cero a 1. Esto es necesario para que el modelo de regresi\u00f3n logistica pueda funcionar correctamente ya que es un modelo de clasificaci\u00f3n binaria.</p>"},{"location":"casos-de-estudio/enfermedad-coronaria/04-regresion-logistica-python/#entrenamiento","title":"Entrenamiento\u00b6","text":"<p>Ahora que tenemos el dataset preprocesado, procedemos a entrenar el modelo de regresi\u00f3n logistica. Para ello, continuaremos utilizando la libreria <code>sklearn</code>. Este paso es similar al utilizado dentro de la optimizaci\u00f3n de atributos, pero en este caso, utilizaremos validaci\u00f3n cruzada en lugar de una divisi\u00f3n 70-30.</p>"},{"location":"casos-de-estudio/enfermedad-coronaria/04-regresion-logistica-python/#evaluacion","title":"Evaluaci\u00f3n\u00b6","text":"<p>Finalmente, evaluemos el modelo. Para ello, utilizaremos la libreria <code>sklearn</code> nuevamente. En este caso, utilizaremos varias m\u00e9tricas de evaluaci\u00f3n, incluyendo la matriz de confusi\u00f3n, la precisi\u00f3n, el recall, y el F1 score.</p>"},{"location":"introduccion/01-definiciones/","title":"Que es \"Machine Learining\"","text":""},{"location":"introduccion/01-definiciones/#que-es-machine-learining","title":"Que es \"Machine Learining\"\u00b6","text":"<p>Machine Learning o \"aprendizaje automatico\" es una rama de la inteligencia artificial que se encarga de crear algoritmos que aprenden de los datos, sin necesidad de ser programados explicitamente para ello. Es el proceso de crear, a partir de muestras de datos, un programa que aprende de esos datos y que generaliza lo aprendido para poder predecir el comportamiento de datos nuevos.</p> <p>Se diferencia tanto de Data Mining como de la Inteligencia Artificial en que el objetivo de Machine Learning es la creacion de programas que aprenden de los datos, mientras que el objetivo de Data Mining es la extraccion de conocimiento de los datos y el objetivo de la Inteligencia Artificial es la creacion de programas que piensen como los humanos.</p>"},{"location":"introduccion/01-definiciones/#tipos-de-machine-learning","title":"Tipos de Machine Learning\u00b6","text":"<p>Machine Learning se puede dividir en dos grandes grupos: <code>Aprendizaje Supervisado</code> y <code>Aprendizaje No Supervisado</code>. El Aprendizaje Supervisado se encarga de crear programas que aprenden de datos categorizados, mientras que el Aprendizaje No Supervisado se encarga de crear programas que aprenden de datos no categorizados.</p>"},{"location":"introduccion/02-herramientas/","title":"Plataformas y Herramientas","text":""},{"location":"introduccion/02-herramientas/#plataformas-y-herramientas","title":"Plataformas y Herramientas\u00b6","text":""},{"location":"introduccion/02-herramientas/#rapidminer","title":"RapidMiner\u00b6","text":"<p>RapidMiner es una herramienta que provee procedimientos de Data Mining y Machine Learning. Permite cargar datos, aplicar transformaciones a los mismos, pre procesamiento de datos, visualizacion, analisis predictivo y modelado estadistico, evaluacion, y despliegue de modelos.</p> <p>Es una herramienta no-code que permite realizar todas estas tareas sin necesidad de escribir codigo. Tanto desarrolladores como usuarios de negocio pueden utilizar RapidMiner para crear modelos de Machine Learning.</p>"},{"location":"introduccion/02-herramientas/#scikit-learn","title":"SciKit Learn\u00b6","text":"<p>SciKit Learn es una libreria de Machine Learning de codigo abierto. Soporta tanto Aprendizaje Supervisado como Aprendizaje No Supervisado. Tambien provee herramientas para pre procesamiento de datos, seleccion de modelos, evaluacion de modelos, entre otras utilidades. Esta libreria esta disponible para Python y fue dise\u00f1ada para trabajar con otras librerias de Python como NumPy y SciPy.</p>"},{"location":"introduccion/02-herramientas/#tensorflow","title":"TensorFlow\u00b6","text":"<p>TensorFlow es una plataforma de extremo a extremo para Machine Learning de codigo abierto. Tiene un ecosistema completo de herramientas, librerias y recursos de la comunidad que permite a los investigadores de Machine Learning crear y desplegar aplicaciones de Machine Learning.</p> <p>Originalmente desarrollado por investigadores e ingenieros que trabajan en el equipo de Google Brain para avanzar su investigacion en Machine Learning, y redes neurales, pero la herramienta es versatil y puede ser utilizada en una amplia variedad de otros dominios.</p>"},{"location":"introduccion/02-herramientas/#azure-machine-learning","title":"Azure Machine Learning\u00b6","text":"<p>Azure Machine Learning es un servicio de Machine Learning en la nube que permite a los desarrolladores y cientificos de datos crear, y desplegar modelos de Machine Learning. Permite crear modelos utilizando un entorno de desarrollo basado en notebooks.</p> <p>Es compatible con modelos de Machine Learning de codigo abierto como TensorFlow, PyTorch y scikit-learn.</p>"},{"location":"introduccion/02-herramientas/#weka","title":"Weka\u00b6","text":"<p>Weka es una herramienta de Machine Learning de codigo abierto escrita en Java. Contiene herramientas para preparacion de datos, clasificacion, regresion, clustering, y visualizacion. Es facil de usar gracias a su interfaz grafica, y gracias a estar completamente implementada en Java, es portable y puede ser ejecutada en cualquier plataforma.</p>"},{"location":"introduccion/02-herramientas/#knime","title":"KNIME\u00b6","text":"<p>KNIME ofrece una plataforma completa de extremo a extremo para Machine Learning y Data Science. Permite crear modelos analiticos, y desplegarlos en produccion, facilitando la integracion de modelos de Machine Learning en aplicaciones de negocio.</p> <p>Utiliza una interface intuitiva no-code/low-code que permite a los usuarios de negocio y a los desarrolladores crear modelos de Machine Learning sin necesidad de escribir codigo.</p>"},{"location":"introduccion/03-crisp-dm/","title":"CRISP-DM","text":""},{"location":"introduccion/03-crisp-dm/#crisp-dm","title":"CRISP-DM\u00b6","text":"<p>CRoss Industry Standard Process for Data Mining (CRISP-DM) es una metodologia de trabajo para proyectos de data science. Fue publicada en 1999 con la intencion de estandarizar procesos de mineria de datos en diferentes industrias, y se ha convertido en el estandar de facto para proyectos de data science, analytics y machine learning.</p> <p>La metodologia CRISP-DM se divide en 6 fases:</p> <ol> <li>Business Understanding</li> </ol> <p>La primera fase se centra en comprender los objetivos y requerimientos del proyecto. Se determina desde un punto de vista del negocio, que es lo que el cliente busca resolver con el proyecto. Este objetivo se intenta traducir a un problema de data science, y se define un plan de trabajo para resolverlo.</p> <ol> <li>Data Understanding</li> </ol> <p>Se realiza la recoleccion de datos, y un analisis exploratorio de los mismos, buscando entender la calidad de los datos, y si estos son suficientes para resolver el problema planteado en la fase anterior. Es preciso examinar los datos, visualizarlos, y realizar un analisis estadistico de los mismos. En esta fase se identifica y documenta las necesidades para la preparacion de los datos.</p> <ol> <li>Data Preparation</li> </ol> <p>En esta fase se preparan los datos para el modelado. Se busca seleccionar los datos mas relevantes para el problema, y transformarlos en un formato adecuado para el modelado. Se realizan tareas de limpieza de datos, seleccion de variables, transformacion de variables, y generacion de variables nuevas. Esta fase es la que mas tiempo consume en un proyecto de data science, y se estima que ocupa el 80% del tiempo total del proyecto.</p> <ol> <li>Modeling</li> </ol> <p>Con los datos preparados, se procede a la construccion de modelos. Se investigan diferentes algoritmos de machine learning, y se selecciona los mas adecuados para el problema. Se prepara el esquema de validacion de los modelos, incluyendo la division de los datos en conjuntos de entrenamiento y test, y se procede a entrenar los modelos. Puede ser necesario realizar preparacion de datos adicional especifica para cada modelo.</p> <ol> <li>Evaluation</li> </ol> <p>Ya entrenados los modelos, se evaluan los resultados obtenidos. Se analizan los resultados de los modelos, y se determina si estos cumplen con los objetivos del proyecto. Se selecciona el modelo que mejor resuelve el problema. Es necesario validar todos los pasos del proceso de creacion del modelo, para asegurar que no se hayan cometido errores. Tambien se debe considerar si hay algunos factores que no se hayan tenido en cuenta, y que puedan afectar los resultados. Finalmente se determina si los resultados son suficientemente buenos para resolver el problema, y se decide si se debe volver a alguna de las fases anteriores.</p> <ol> <li>Despliegue</li> </ol> <p>Una vez aprovado el modelo para despliegue, se procede a su implementacion. Dependiendo de los requerimientos del proyecto, este paso puede ser solo la creacion de un informe con los datos obtenidos, o la implementacion de un sistema de machine learning en produccion. En este ultimo caso, se debe crear un plan de mantenimiento del sistema, y se debe definir como se va a monitorear el funcionamiento del mismo.</p>"},{"location":"introduccion/04-comparacion-herramientas/","title":"Comparacion de Algoritmos en Herramientas","text":"<p>| Algoritmo                    | RapidMiner  | SciKit Learn    | Azure Machine Learning | | :--------------------------- | :---------: | :-------------: | :--------------------: | | k-means                      | \u2713     | \u2713         | \u2713                | | k-modes                      |             |                 |                        | | hierarchical                 | \u2713     | \u2713         |                        | | DBSCAN                       | \u2713     | \u2713         |                        | | Gaussiean Mixture Model      |             | \u2713         |                        | | Linear SVM                   | \u2713     | \u2713         | \u2713                | | Naive Bayes                  | \u2713     | \u2713         | | Decision Tree                | \u2713     | \u2713         | \u2713                | | Logistic Regression          | \u2713     | \u2713         | \u2713                | | Kernel SVM                   | \u2713     | \u2713         |                        | | Random Forest                | \u2713     | \u2713         |                        | | Neural Network               | \u2713     | \u2713         | \u2713                | | Gradient Boosting Tree       | \u2713     | \u2713         |                        | | Principal Component Analysis | \u2713     | \u2713         |                        | | Singular Value Decomposition | \u2713     | \u2713         | \u2713                | | Latent Dirichlet Analysis    |             | \u2713         | \u2713                | | Linear Regression            | \u2713     | \u2713         | \u2713                |</p>"},{"location":"introduccion/04-comparacion-herramientas/#comparacion-de-algoritmos-en-herramientas","title":"Comparacion de Algoritmos en Herramientas\u00b6","text":"<p>Fecha de comparacion: 2023-09-22</p>"},{"location":"introduccion/05-dataset/","title":"Vista de un Dataset","text":""},{"location":"introduccion/05-dataset/#vista-de-un-dataset","title":"Vista de un Dataset\u00b6","text":"<p>Este sera un ejercicio en el que realizaremos un analisis de un dataset. Tomaremos un dataset altamente conocido, el dataset de Mushroom, el cual contiene informacion sobre hongos y si son venenosos o no. Este se puede encontrar en el siguiente enlace: Link!</p>"},{"location":"introduccion/05-dataset/#problema","title":"Problema\u00b6","text":"<p>El problema que se quiere resolver es el de clasificar si un hongo es venenoso o no, basado en sus caracteristicas. Se intenta identificar si un hongo es comestible, venenoso, o si no se tiene suficiente informacion para clasificarlo. En el ultimo caso, se agrupa con los venenosos, terminando con una clasificacion binaria.</p>"},{"location":"introduccion/05-dataset/#atributos","title":"Atributos\u00b6","text":"<p>El dataset contiene 23 atributos, siendo 1 de ellos el atributo de clase. Los atributos son los siguientes:</p> Atributo Valores Descripcion <code>class</code> edible=<code>e</code>, poisonous=<code>p</code> Clase a la que pertenece el hongo <code>cap-shape</code> bell=<code>b</code>,conical=<code>c</code>,convex=<code>x</code>,flat=<code>f</code>, knobbed=<code>k</code>,sunken=<code>s</code> Forma del sombrero <code>cap-surface</code> fibrous=<code>f</code>,grooves=<code>g</code>,scaly=<code>y</code>,smooth=<code>s</code> Textura de la superficie del sombrero <code>cap-color</code> brown=<code>n</code>,buff=<code>b</code>,cinnamon=<code>c</code>,gray=<code>g</code>,green=<code>r</code>, pink=<code>p</code>,purple=<code>u</code>,red=<code>e</code>,white=<code>w</code>,yellow=<code>y</code> Color del sombrero <code>bruises?</code> bruises=<code>t</code>,no=<code>f</code> Tiene marcas de golpes? <code>odor</code> almond=<code>a</code>,anise=<code>l</code>,creosote=<code>c</code>,fishy=<code>y</code>,foul=<code>f</code>, musty=<code>m</code>,none=<code>n</code>,pungent=<code>p</code>,spicy=<code>s</code> Olor <code>gill-attachment</code> attached=<code>a</code>,descending=<code>d</code>,free=<code>f</code>,notched=<code>n</code> Union de las laminas con el sombrero <code>gill-spacing</code> close=<code>c</code>,crowded=<code>w</code>,distant=<code>d</code> Espacio entre las laminas <code>gill-size</code> broad=<code>b</code>,narrow=<code>n</code> Tama\u00f1o de las laminas <code>gill-color</code> black=<code>k</code>,brown=<code>n</code>,buff=<code>b</code>,chocolate=<code>h</code>,gray=<code>g</code>, green=<code>r</code>,orange=<code>o</code>,pink=<code>p</code>,purple=<code>u</code>,red=<code>e</code>, white=<code>w</code>,yellow=<code>y</code> Color de las laminas <code>stalk-shape</code> enlarging=<code>e</code>,tapering=<code>t</code> Forma del tallo <code>stalk-root</code> bulbous=<code>b</code>,club=<code>c</code>,cup=<code>u</code>,equal=<code>e</code>, rhizomorphs=<code>z</code>,rooted=<code>r</code>,missing=<code>?</code> Raiz del tallo <code>stalk-surface-above-ring</code> fibrous=<code>f</code>,scaly=<code>y</code>,silky=<code>k</code>,smooth=<code>s</code> Textura del tallo por encima del anillo <code>stalk-surface-below-ring</code> fibrous=<code>f</code>,scaly=<code>y</code>,silky=<code>k</code>,smooth=<code>s</code> Textura del tallo por debajo del anillo <code>stalk-color-above-ring</code> brown=<code>n</code>,buff=<code>b</code>,cinnamon=<code>c</code>,gray=<code>g</code>,orange=<code>o</code>, pink=<code>p</code>,red=<code>e</code>,white=<code>w</code>,yellow=<code>y</code> Color del tallo por encima del anillo <code>stalk-color-below-ring</code> brown=<code>n</code>,buff=<code>b</code>,cinnamon=<code>c</code>,gray=<code>g</code>,orange=<code>o</code>, pink=<code>p</code>,red=<code>e</code>,white=<code>w</code>,yellow=<code>y</code> Color del tallo por debajo del anillo <code>veil-type</code> partial=<code>p</code>,universal=<code>u</code> Tipo de velo <code>veil-color</code> brown=<code>n</code>,orange=<code>o</code>,white=<code>w</code>,yellow=<code>y</code> Color del velo <code>ring-number</code> none=<code>n</code>,one=<code>o</code>,two=<code>t</code> Numero de anillos <code>ring-type</code> cobwebby=<code>c</code>,evanescent=<code>e</code>,flaring=<code>f</code>,large=<code>l</code>, none=<code>n</code>,pendant=<code>p</code>,sheathing=<code>s</code>,zone=<code>z</code> Tipo de anillo <code>spore-print-color</code> black=<code>k</code>,brown=<code>n</code>,buff=<code>b</code>,chocolate=<code>h</code>,green=<code>r</code>, orange=<code>o</code>,purple=<code>u</code>,white=<code>w</code>,yellow=<code>y</code> Color de la espora <code>population</code> abundant=<code>a</code>,clustered=<code>c</code>,numerous=<code>n</code>, scattered=<code>s</code>,several=<code>v</code>,solitary=<code>y</code> Agrupacion <code>habitat</code> grasses=<code>g</code>,leaves=<code>l</code>,meadows=<code>m</code>,paths=<code>p</code>, urban=<code>u</code>,waste=<code>w</code>,woods=<code>d</code> Habitat"},{"location":"introduccion/05-dataset/#algoritmos-aplicables","title":"Algoritmos Aplicables\u00b6","text":"<p>Realizando una busqueda en internet para encontrar algoritmos que se puedan aplicar a este problema, se encontraron los siguientes:</p> <ul> <li>Gaussian Naive Bayes Classifier</li> <li>Logistic Regression Classifier</li> <li>Decision Tree Classifier</li> <li>Random Forest Classifier</li> <li>xgBoost Classifier</li> <li>Linear Discriminant Classifier</li> <li>Gaussian Process Classifier</li> <li>Ada-boost Classifier</li> </ul>"},{"location":"introduccion/06-dataset-estadistica/","title":"Estadistica de un Dataset","text":"In\u00a0[2]: Copied! <pre>from ucimlrepo import fetch_ucirepo\n\niris = fetch_ucirepo(id=53)\n</pre> from ucimlrepo import fetch_ucirepo  iris = fetch_ucirepo(id=53) In\u00a0[3]: Copied! <pre>X = iris.data.features\n\nfor column in X.columns:\n    print()\n    print(f\"Atributo: {column}\")\n    print(f\"Media: {X[column].mean()}\")\n    print(f\"Desviaci\u00f3n est\u00e1ndar: {X[column].std()}\")\n    print(f\"Varianza: {X[column].var()}\")\n</pre> X = iris.data.features  for column in X.columns:     print()     print(f\"Atributo: {column}\")     print(f\"Media: {X[column].mean()}\")     print(f\"Desviaci\u00f3n est\u00e1ndar: {X[column].std()}\")     print(f\"Varianza: {X[column].var()}\")  <pre>\nAtributo: sepal length\nMedia: 5.843333333333334\nDesviaci\u00f3n est\u00e1ndar: 0.828066127977863\nVarianza: 0.6856935123042507\n\nAtributo: sepal width\nMedia: 3.0540000000000003\nDesviaci\u00f3n est\u00e1ndar: 0.4335943113621737\nVarianza: 0.1880040268456376\n\nAtributo: petal length\nMedia: 3.758666666666666\nDesviaci\u00f3n est\u00e1ndar: 1.7644204199522626\nVarianza: 3.113179418344519\n\nAtributo: petal width\nMedia: 1.1986666666666668\nDesviaci\u00f3n est\u00e1ndar: 0.7631607417008411\nVarianza: 0.582414317673378\n</pre>"},{"location":"introduccion/06-dataset-estadistica/#estadistica-de-un-dataset","title":"Estadistica de un Dataset\u00b6","text":"<p>Tomaremos un dataset y lo utilizaremos para visualizar distintos datos estadisticos sobre sus atributos. Para este ejercicio utilizaremos el dataset Iris, que contiene informacion sobre distintas especies de flores. El dataset se puede encontrar en el siguiente link: Link!</p>"},{"location":"introduccion/06-dataset-estadistica/#problema","title":"Problema\u00b6","text":"<p>Tenemos un dataset con ejemplos representando distintas plantas. El mismo contiene 3 clases de 50 ejemplos cada una. Las clases se refieren a distintos tipos de plantas de Iris. Cada ejemplo tiene 4 atributos numericos:</p> <ol> <li>Longitud del sepalo en cm</li> <li>Ancho del sepalo en cm</li> <li>Longitud del petalo en cm</li> <li>Ancho del petalo en cm</li> </ol> <p>El 5 atributo es la clase a la que pertenece la planta. Esta puede ser: Iris Setosa, Iris Versicolour, Iris Virginica.</p>"},{"location":"introduccion/06-dataset-estadistica/#estadistica","title":"Estadi\u0301stica\u00b6","text":"<p>Analicemos los datos para obtener informacio\u0301n sobre los mismos. Para ello, calcularemos los siguientes valores: Media, Desviacio\u0301n esta\u0301ndar, y Varianza.</p>"},{"location":"tratamiento-de-datos/00-introduccion/","title":"Introducci\u00f3n","text":""},{"location":"tratamiento-de-datos/00-introduccion/#introduccion","title":"Introducci\u00f3n\u00b6","text":"<p>Un algoritmo de aprendizaje automatizado no es una bala de plata capaz de recibir cualquier conjunto de datos y producir un modelo \u00fatil. Si la calidad de los datos utilizados para su entrenamiento es subpar, el modelo resultante tambi\u00e9n lo ser\u00e1. Por lo tanto, es importante que, al comenzar un proyecto de aprendizaje automatizado, se dedique tiempo a comprender los datos con los que se est\u00e1 trabajando.</p> <p>El proceso requiere conocimiento del \u00e1rea de aplicaci\u00f3n para interpretar los datos correctamente. Esto significa conocer cuales son los datos disponibles, como se recopilaron y qu\u00e9 significan, y si estos son representativos de la poblaci\u00f3n de inter\u00e9s.</p> <p>Es de importancia tambi\u00e9n conocer los tipos de datos con los que se trabaja. Ciertos algoritmos de aprendizaje automatizado funcionan mejor con ciertos tipos de datos. Y otros simplemente no se pueden aplicar a ciertos tipos de datos. Por ejemplo, los algoritmos de aprendizaje automatizado que utilizan distancias euclidianas, como la regresi\u00f3n lineal, no funcionan bien con datos categ\u00f3ricos. En estos casos puede ser requerida una transformaci\u00f3n de los datos.</p> <p>Otro aspecto a tener en cuenta es el an\u00e1lisis estad\u00edstico de los datos. Caracter\u00edsticas estad\u00edsticas como lo son la media, la moda, el desv\u00edo, los minimos, y los maximos, pueden ayudar a comprender los datos. Tambi\u00e9n vuelve a ser importante para la selecci\u00f3n de algoritmos a utilizar, dado que algunos realizan suposiciones sobre la distribuci\u00f3n de los datos. Ejemplos con valores faltantes o valores at\u00edpicos pueden tambi\u00e9n afectar el rendimiento de los algoritmos de aprendizaje automatizado.</p> <p>Por estas razones, es importante realizar un an\u00e1lisis exploratorio de los datos antes de comenzar a trabajar con ellos. Este an\u00e1lisis exploratorio de datos nos permitir\u00e1 realizar decisiones informadas sobre los algoritmos a utilizar, y sobre las transformaciones que se deben aplicar a los datos.</p>"},{"location":"tratamiento-de-datos/01-estadistica/","title":"Estad\u00edstica","text":"In\u00a0[12]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndatos_1 = np.random.binomial(45, 0.3, 1000)\ndatos_2 = np.append(np.random.binomial(20, 0.2, 750), [np.nan] * 250)\ndatos_3 = np.append(np.random.binomial(78, 0.4, 800), [np.nan] * 200)\n\ndata_df = pd.DataFrame({'datos_1': datos_1, 'datos_2': datos_2, 'datos_3': datos_3})\n\nstatistic = pd.DataFrame([], index=['mean', 'std', 'min', 'max', 'median', 'mising'])\nfor column in statistic.columns:\n    statistic[column] = statistic[column].astype('float')\n\nfor column in data_df.columns:\n    if data_df[column].dtype in ('object', 'category', 'string'):\n        statistic[column] = [np.nan] * 5 + [data_df[column].isna().sum()]\n        continue\n\n    if data_df[column].isna().all():\n        statistic[column] = [np.nan] * 5 + [len(data_df[column])]\n        continue\n\n    statistic[column] = [\n        data_df[column].mean(),\n        data_df[column].std(),\n        data_df[column].min(),\n        data_df[column].max(),\n        data_df[column].median(),\n        np.isnan(data_df[column]).sum()\n    ]\nstatistic = statistic.T\nprint('Estad\u00edsticas de los datos')\nprint(statistic)\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt  datos_1 = np.random.binomial(45, 0.3, 1000) datos_2 = np.append(np.random.binomial(20, 0.2, 750), [np.nan] * 250) datos_3 = np.append(np.random.binomial(78, 0.4, 800), [np.nan] * 200)  data_df = pd.DataFrame({'datos_1': datos_1, 'datos_2': datos_2, 'datos_3': datos_3})  statistic = pd.DataFrame([], index=['mean', 'std', 'min', 'max', 'median', 'mising']) for column in statistic.columns:     statistic[column] = statistic[column].astype('float')  for column in data_df.columns:     if data_df[column].dtype in ('object', 'category', 'string'):         statistic[column] = [np.nan] * 5 + [data_df[column].isna().sum()]         continue      if data_df[column].isna().all():         statistic[column] = [np.nan] * 5 + [len(data_df[column])]         continue      statistic[column] = [         data_df[column].mean(),         data_df[column].std(),         data_df[column].min(),         data_df[column].max(),         data_df[column].median(),         np.isnan(data_df[column]).sum()     ] statistic = statistic.T print('Estad\u00edsticas de los datos') print(statistic) <pre>Estad\u00edsticas de los datos\n              mean       std   min   max  median  mising\ndatos_1  13.524000  3.037540   5.0  24.0    14.0     0.0\ndatos_2   4.021333  1.762106   0.0   9.0     4.0   250.0\ndatos_3  31.331250  4.396668  15.0  43.0    31.0   200.0\n</pre> In\u00a0[13]: Copied! <pre>fig, ax = plt.subplots(1, 3, figsize=(15, 5))\nax[0].hist(data_df['datos_1'], bins=10, color='red')\nax[0].set_title('datos_1')\nax[1].hist(data_df['datos_2'], bins=10, color='green')\nax[1].set_title('datos_2')\nax[2].hist(data_df['datos_3'], bins=10, color='blue')\nax[2].set_title('datos_3')\nplt.show()\n</pre> fig, ax = plt.subplots(1, 3, figsize=(15, 5)) ax[0].hist(data_df['datos_1'], bins=10, color='red') ax[0].set_title('datos_1') ax[1].hist(data_df['datos_2'], bins=10, color='green') ax[1].set_title('datos_2') ax[2].hist(data_df['datos_3'], bins=10, color='blue') ax[2].set_title('datos_3') plt.show()"},{"location":"tratamiento-de-datos/01-estadistica/#estadistica","title":"Estad\u00edstica\u00b6","text":"<p>Para el aprendizaje automatizado, la estad\u00edstica es una herramienta fundamental para entender los datos con los que se trabaja. En este cap\u00edtulo se presentan los conceptos b\u00e1sicos de estad\u00edstica que se utilizan en el aprendizaje automatizado.</p> <p>De inter\u00e9s para el aprendizaje automatizado son los conceptos de:</p> <ul> <li>Media: Es el promedio de los datos. Se calcula sumando todos los datos y dividiendo entre el n\u00famero de datos. Es una medida de la tendencia de los datos.</li> <li>Moda: Es el dato que m\u00e1s se repite en los datos. Nos da otra idea de la tendencia de los datos.</li> <li>Desv\u00edo: Es la diferencia entre cada dato y la media. Es una medida de la dispersi\u00f3n de los datos.</li> <li>Varianza: Es el promedio de los desv\u00edos al cuadrado.</li> <li>M\u00ednimo y m\u00e1ximo: El dato m\u00e1s peque\u00f1o y el dato m\u00e1s grande. Establecen el rango de los datos.</li> <li>Datos at\u00edpicos: Son datos que se alejan mucho de la media. Pueden ser errores de medici\u00f3n o datos que no siguen el patr\u00f3n de los dem\u00e1s datos. Introducen ruido en los datos.</li> <li>Distribuci\u00f3n: Es la forma en que se distribuyen los datos. Puede ser normal, uniforme, etc.</li> </ul> <p>Estos datos son importantes a la hora de seleccionar el algoritmo a utilizar y para entender los resultados obtenidos. Por ejemplo, varios algoritmos de aprendizaje automatizado asumen que los datos tienen una distribuci\u00f3n normal, o que la escala de estos datos es la misma, lo cual puede requerir una normalizaci\u00f3n de los datos en caso de que estas suposiciones no se cumplan.</p>"},{"location":"tratamiento-de-datos/01-estadistica/#python","title":"Python\u00b6","text":"<p>En Python, la librer\u00eda <code>numpy</code> provee funciones para calcular la media, la moda, el desv\u00edo, la varianza, el m\u00ednimo y el m\u00e1ximo de un conjunto de datos. Al mismo tiempo, la librer\u00eda <code>matplotlib</code> provee funciones para graficar los datos y ver su distribuci\u00f3n.</p>"},{"location":"tratamiento-de-datos/01-estadistica/#rapidminer","title":"RapidMiner\u00b6","text":"<p>En RapidMiner, la estadistica es provista directamente por el panel de resultados.</p> <p> </p>"},{"location":"tratamiento-de-datos/01-rapidminer-tutoriales/","title":"Tutoriales de RapidMiner","text":""},{"location":"tratamiento-de-datos/01-rapidminer-tutoriales/#tutoriales-de-rapidminer","title":"Tutoriales de RapidMiner\u00b6","text":""},{"location":"tratamiento-de-datos/01-rapidminer-tutoriales/#handling-missing-values","title":"Handling Missing Values\u00b6","text":"<p>El tutorial comienza separando dos grupos generales de tratamiento de datos: mezclado y limpieza. Se aclara que este tutorial y el pr\u00f3ximo tratan de la limpieza de los datos. El tutorial utiliza el dataset Titanic. Dentro del dataset se destacan varios atributos con valores faltantes en los ejemplos: Age, Passenger Fare, Cabin, Port of Embarkation, y Life Boat. Se remueven inicialmente los atributos Cabin, y Life Boat: el primero debido a datos faltantes y restantes sin utilidad, y el segundo debido a alta correlaci\u00f3n con el label que se utilizara. Luego generamos los valores faltantes para el atributo Age basados en el promedio del atributo para los valores existentes, y utilizando el operador Replace Missing Values. Para el resto de los atributos con datos faltantes, se filtran todos los ejemplos donde el atributo no tenga dato utilizando el operador Filter Examples, y seleccionando la condici\u00f3n de filtrado a no_missing_attributes. Como resultado, obtenemos un dataset sin valores faltantes, reducido a 1306 ejemplos.</p> <p>El tutorial demuestra como se puede limpiar un dataset utilizando varios operadores para remover columnas, remover ejemplos, y reemplazar valores vac\u00edos con otros valores. Al mismo tiempo, advierte de que cada operador se debe considerar antes de utilizar para no modificar demasiado el valor representativo del dataset.</p>"},{"location":"tratamiento-de-datos/01-rapidminer-tutoriales/#normalization-and-outlier-detection","title":"Normalization and Outlier Detection\u00b6","text":"<p>Avanzando en la limpieza de los datos se pasa a declarar la importancia de detectar casos inusuales y, en algunos casos, removerlos del dataset. Esto es debido que en varios casos los datos inusuales son resultado de medidas incorrectas, pero en algunos son precisamente los datos de inter\u00e9s que deseamos encontrar. En este tutorial se busca tratarlos de datos inv\u00e1lidos y eliminarlos.</p> <p>Se comienza removiendo las columnas de Cabin, Life Boat, Name, y Ticket Number. Esto es debido a su baja contribuci\u00f3n al detectado de valores inusuales. Para el resto se utilizar\u00e1 un algoritmo basado en distancia euclideana entre puntos de datos, marcando los puntos mas lejanos como inusuales. Luego se utiliza el operador Normalize para transformar los valores de cada atributo de forma que sus datos tengan una media de 0 y una distribuci\u00f3n est\u00e1ndar de 1. Esto es para poder comparar los atributos uno con cada otro. Se contin\u00faa aplicado el operador Detect Outliers (Distance). Este operador busca los ejemplos con mayor distancia del resto y los marca como inusuales utilizando una nueva columna con valores true o false. Por \u00faltimo utilizaremos el operador Filter Examples para remover todos los ejemplos con el atributo Outlier en true, removiendo efectivamente los valores inusuales.</p>"},{"location":"tratamiento-de-datos/02-valores-faltantes/","title":"Valores Faltantes","text":"In\u00a0[27]: Copied! <pre>import pandas as pd\nimport numpy as np\n</pre> import pandas as pd import numpy as np In\u00a0[28]: Copied! <pre>dataset: pd.DataFrame = pd.read_csv('./datasets/full.csv')\ndataset.info()\n</pre> dataset: pd.DataFrame = pd.read_csv('./datasets/full.csv') dataset.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 899 entries, 0 to 898\nData columns (total 77 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   dataset   899 non-null    object \n 1   id        899 non-null    int64  \n 2   ccf       899 non-null    int64  \n 3   age       899 non-null    int64  \n 4   sex       899 non-null    int64  \n 5   painloc   617 non-null    float64\n 6   painexer  617 non-null    float64\n 7   relrest   613 non-null    float64\n 8   pncaden   0 non-null      float64\n 9   cp        899 non-null    int64  \n 10  trestbps  840 non-null    float64\n 11  htn       865 non-null    float64\n 12  chol      869 non-null    float64\n 13  smoke     230 non-null    float64\n 14  cigs      479 non-null    float64\n 15  years     467 non-null    float64\n 16  fbs       809 non-null    float64\n 17  dm        95 non-null     float64\n 18  famhist   477 non-null    float64\n 19  restecg   897 non-null    float64\n 20  ekgmo     846 non-null    float64\n 21  ekgday    845 non-null    float64\n 22  ekgyr     846 non-null    float64\n 23  dig       831 non-null    float64\n 24  prop      833 non-null    float64\n 25  nitr      834 non-null    float64\n 26  pro       836 non-null    float64\n 27  diuretic  817 non-null    float64\n 28  proto     787 non-null    float64\n 29  thaldur   843 non-null    float64\n 30  thaltime  446 non-null    float64\n 31  met       794 non-null    float64\n 32  thalach   844 non-null    float64\n 33  thalrest  843 non-null    float64\n 34  tpeakbps  836 non-null    float64\n 35  tpeakbpd  836 non-null    float64\n 36  dummy     840 non-null    float64\n 37  trestbpd  840 non-null    float64\n 38  exang     844 non-null    float64\n 39  xhypo     841 non-null    float64\n 40  oldpeak   837 non-null    float64\n 41  slope     591 non-null    float64\n 42  rldv5     474 non-null    float64\n 43  rldv5e    757 non-null    float64\n 44  ca        291 non-null    float64\n 45  restckm   0 non-null      float64\n 46  exerckm   1 non-null      float64\n 47  restef    28 non-null     float64\n 48  restwm    30 non-null     float64\n 49  exeref    2 non-null      float64\n 50  exerwm    5 non-null      float64\n 51  thal      422 non-null    float64\n 52  thalsev   130 non-null    float64\n 53  thalpul   44 non-null     float64\n 54  earlobe   1 non-null      float64\n 55  cmo       888 non-null    float64\n 56  cday      890 non-null    float64\n 57  cyr       890 non-null    float64\n 58  num       899 non-null    int64  \n 59  lmt       624 non-null    float64\n 60  ladprox   663 non-null    float64\n 61  laddist   653 non-null    float64\n 62  diag      341 non-null    float64\n 63  cxmain    664 non-null    float64\n 64  ramus     332 non-null    float64\n 65  om1       628 non-null    float64\n 66  om2       327 non-null    float64\n 67  rcaprox   654 non-null    float64\n 68  rcadist   629 non-null    float64\n 69  lvx1      880 non-null    float64\n 70  lvx2      880 non-null    float64\n 71  lvx3      880 non-null    float64\n 72  lvx4      880 non-null    float64\n 73  lvf       883 non-null    float64\n 74  cathef    311 non-null    float64\n 75  junk      119 non-null    float64\n 76  name      899 non-null    object \ndtypes: float64(69), int64(6), object(2)\nmemory usage: 540.9+ KB\n</pre> <p>Asignemos tipos de datos correctos a las columnas:</p> In\u00a0[29]: Copied! <pre>for column in dataset.columns:\n    try:\n        dataset[column] = dataset[column].astype('string')\n        dataset[column] = dataset[column].astype(float)\n        dataset[column] = dataset[column].astype(int)\n    except:\n        pass\ndataset.info()\n</pre> for column in dataset.columns:     try:         dataset[column] = dataset[column].astype('string')         dataset[column] = dataset[column].astype(float)         dataset[column] = dataset[column].astype(int)     except:         pass dataset.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 899 entries, 0 to 898\nData columns (total 77 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   dataset   899 non-null    string \n 1   id        899 non-null    int32  \n 2   ccf       899 non-null    int32  \n 3   age       899 non-null    int32  \n 4   sex       899 non-null    int32  \n 5   painloc   617 non-null    float64\n 6   painexer  617 non-null    float64\n 7   relrest   613 non-null    float64\n 8   pncaden   0 non-null      float64\n 9   cp        899 non-null    int32  \n 10  trestbps  840 non-null    float64\n 11  htn       865 non-null    float64\n 12  chol      869 non-null    float64\n 13  smoke     230 non-null    float64\n 14  cigs      479 non-null    float64\n 15  years     467 non-null    float64\n 16  fbs       809 non-null    float64\n 17  dm        95 non-null     float64\n 18  famhist   477 non-null    float64\n 19  restecg   897 non-null    float64\n 20  ekgmo     846 non-null    float64\n 21  ekgday    845 non-null    float64\n 22  ekgyr     846 non-null    float64\n 23  dig       831 non-null    float64\n 24  prop      833 non-null    float64\n 25  nitr      834 non-null    float64\n 26  pro       836 non-null    float64\n 27  diuretic  817 non-null    float64\n 28  proto     787 non-null    float64\n 29  thaldur   843 non-null    float64\n 30  thaltime  446 non-null    float64\n 31  met       794 non-null    float64\n 32  thalach   844 non-null    float64\n 33  thalrest  843 non-null    float64\n 34  tpeakbps  836 non-null    float64\n 35  tpeakbpd  836 non-null    float64\n 36  dummy     840 non-null    float64\n 37  trestbpd  840 non-null    float64\n 38  exang     844 non-null    float64\n 39  xhypo     841 non-null    float64\n 40  oldpeak   837 non-null    float64\n 41  slope     591 non-null    float64\n 42  rldv5     474 non-null    float64\n 43  rldv5e    757 non-null    float64\n 44  ca        291 non-null    float64\n 45  restckm   0 non-null      float64\n 46  exerckm   1 non-null      float64\n 47  restef    28 non-null     float64\n 48  restwm    30 non-null     float64\n 49  exeref    2 non-null      float64\n 50  exerwm    5 non-null      float64\n 51  thal      422 non-null    float64\n 52  thalsev   130 non-null    float64\n 53  thalpul   44 non-null     float64\n 54  earlobe   1 non-null      float64\n 55  cmo       888 non-null    float64\n 56  cday      890 non-null    float64\n 57  cyr       890 non-null    float64\n 58  num       899 non-null    int32  \n 59  lmt       624 non-null    float64\n 60  ladprox   663 non-null    float64\n 61  laddist   653 non-null    float64\n 62  diag      341 non-null    float64\n 63  cxmain    664 non-null    float64\n 64  ramus     332 non-null    float64\n 65  om1       628 non-null    float64\n 66  om2       327 non-null    float64\n 67  rcaprox   654 non-null    float64\n 68  rcadist   629 non-null    float64\n 69  lvx1      880 non-null    float64\n 70  lvx2      880 non-null    float64\n 71  lvx3      880 non-null    float64\n 72  lvx4      880 non-null    float64\n 73  lvf       883 non-null    float64\n 74  cathef    311 non-null    float64\n 75  junk      119 non-null    float64\n 76  name      899 non-null    string \ndtypes: float64(69), int32(6), string(2)\nmemory usage: 519.9 KB\n</pre> <p>Eliminemos las columnas que tienen mas del 30% de valores faltantes.</p> In\u00a0[30]: Copied! <pre>dataset = dataset.dropna(axis=1, thresh=0.7*len(dataset))\ndataset.info()\n</pre> dataset = dataset.dropna(axis=1, thresh=0.7*len(dataset)) dataset.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 899 entries, 0 to 898\nData columns (total 46 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   dataset   899 non-null    string \n 1   id        899 non-null    int32  \n 2   ccf       899 non-null    int32  \n 3   age       899 non-null    int32  \n 4   sex       899 non-null    int32  \n 5   cp        899 non-null    int32  \n 6   trestbps  840 non-null    float64\n 7   htn       865 non-null    float64\n 8   chol      869 non-null    float64\n 9   fbs       809 non-null    float64\n 10  restecg   897 non-null    float64\n 11  ekgmo     846 non-null    float64\n 12  ekgday    845 non-null    float64\n 13  ekgyr     846 non-null    float64\n 14  dig       831 non-null    float64\n 15  prop      833 non-null    float64\n 16  nitr      834 non-null    float64\n 17  pro       836 non-null    float64\n 18  diuretic  817 non-null    float64\n 19  proto     787 non-null    float64\n 20  thaldur   843 non-null    float64\n 21  met       794 non-null    float64\n 22  thalach   844 non-null    float64\n 23  thalrest  843 non-null    float64\n 24  tpeakbps  836 non-null    float64\n 25  tpeakbpd  836 non-null    float64\n 26  dummy     840 non-null    float64\n 27  trestbpd  840 non-null    float64\n 28  exang     844 non-null    float64\n 29  xhypo     841 non-null    float64\n 30  oldpeak   837 non-null    float64\n 31  rldv5e    757 non-null    float64\n 32  cmo       888 non-null    float64\n 33  cday      890 non-null    float64\n 34  cyr       890 non-null    float64\n 35  num       899 non-null    int32  \n 36  ladprox   663 non-null    float64\n 37  laddist   653 non-null    float64\n 38  cxmain    664 non-null    float64\n 39  rcaprox   654 non-null    float64\n 40  lvx1      880 non-null    float64\n 41  lvx2      880 non-null    float64\n 42  lvx3      880 non-null    float64\n 43  lvx4      880 non-null    float64\n 44  lvf       883 non-null    float64\n 45  name      899 non-null    string \ndtypes: float64(38), int32(6), string(2)\nmemory usage: 302.1 KB\n</pre> <p>Inputemos valores faltantes con la media, para aquellos valores que sean num\u00e9ricos reales.</p> In\u00a0[31]: Copied! <pre>for column in dataset.columns:\n    if dataset[column].dtype == np.float64:\n        dataset[column] = dataset[column].fillna(dataset[column].mean())\ndataset.info()\n</pre> for column in dataset.columns:     if dataset[column].dtype == np.float64:         dataset[column] = dataset[column].fillna(dataset[column].mean()) dataset.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 899 entries, 0 to 898\nData columns (total 46 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   dataset   899 non-null    string \n 1   id        899 non-null    int32  \n 2   ccf       899 non-null    int32  \n 3   age       899 non-null    int32  \n 4   sex       899 non-null    int32  \n 5   cp        899 non-null    int32  \n 6   trestbps  899 non-null    float64\n 7   htn       899 non-null    float64\n 8   chol      899 non-null    float64\n 9   fbs       899 non-null    float64\n 10  restecg   899 non-null    float64\n 11  ekgmo     899 non-null    float64\n 12  ekgday    899 non-null    float64\n 13  ekgyr     899 non-null    float64\n 14  dig       899 non-null    float64\n 15  prop      899 non-null    float64\n 16  nitr      899 non-null    float64\n 17  pro       899 non-null    float64\n 18  diuretic  899 non-null    float64\n 19  proto     899 non-null    float64\n 20  thaldur   899 non-null    float64\n 21  met       899 non-null    float64\n 22  thalach   899 non-null    float64\n 23  thalrest  899 non-null    float64\n 24  tpeakbps  899 non-null    float64\n 25  tpeakbpd  899 non-null    float64\n 26  dummy     899 non-null    float64\n 27  trestbpd  899 non-null    float64\n 28  exang     899 non-null    float64\n 29  xhypo     899 non-null    float64\n 30  oldpeak   899 non-null    float64\n 31  rldv5e    899 non-null    float64\n 32  cmo       899 non-null    float64\n 33  cday      899 non-null    float64\n 34  cyr       899 non-null    float64\n 35  num       899 non-null    int32  \n 36  ladprox   899 non-null    float64\n 37  laddist   899 non-null    float64\n 38  cxmain    899 non-null    float64\n 39  rcaprox   899 non-null    float64\n 40  lvx1      899 non-null    float64\n 41  lvx2      899 non-null    float64\n 42  lvx3      899 non-null    float64\n 43  lvx4      899 non-null    float64\n 44  lvf       899 non-null    float64\n 45  name      899 non-null    string \ndtypes: float64(38), int32(6), string(2)\nmemory usage: 302.1 KB\n</pre> <p>Por ultimo, eliminemos las filas que tengan valores faltantes.</p> In\u00a0[32]: Copied! <pre>dataset = dataset.dropna(axis=0, how='any')\ndataset.info()\n</pre> dataset = dataset.dropna(axis=0, how='any') dataset.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 899 entries, 0 to 898\nData columns (total 46 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   dataset   899 non-null    string \n 1   id        899 non-null    int32  \n 2   ccf       899 non-null    int32  \n 3   age       899 non-null    int32  \n 4   sex       899 non-null    int32  \n 5   cp        899 non-null    int32  \n 6   trestbps  899 non-null    float64\n 7   htn       899 non-null    float64\n 8   chol      899 non-null    float64\n 9   fbs       899 non-null    float64\n 10  restecg   899 non-null    float64\n 11  ekgmo     899 non-null    float64\n 12  ekgday    899 non-null    float64\n 13  ekgyr     899 non-null    float64\n 14  dig       899 non-null    float64\n 15  prop      899 non-null    float64\n 16  nitr      899 non-null    float64\n 17  pro       899 non-null    float64\n 18  diuretic  899 non-null    float64\n 19  proto     899 non-null    float64\n 20  thaldur   899 non-null    float64\n 21  met       899 non-null    float64\n 22  thalach   899 non-null    float64\n 23  thalrest  899 non-null    float64\n 24  tpeakbps  899 non-null    float64\n 25  tpeakbpd  899 non-null    float64\n 26  dummy     899 non-null    float64\n 27  trestbpd  899 non-null    float64\n 28  exang     899 non-null    float64\n 29  xhypo     899 non-null    float64\n 30  oldpeak   899 non-null    float64\n 31  rldv5e    899 non-null    float64\n 32  cmo       899 non-null    float64\n 33  cday      899 non-null    float64\n 34  cyr       899 non-null    float64\n 35  num       899 non-null    int32  \n 36  ladprox   899 non-null    float64\n 37  laddist   899 non-null    float64\n 38  cxmain    899 non-null    float64\n 39  rcaprox   899 non-null    float64\n 40  lvx1      899 non-null    float64\n 41  lvx2      899 non-null    float64\n 42  lvx3      899 non-null    float64\n 43  lvx4      899 non-null    float64\n 44  lvf       899 non-null    float64\n 45  name      899 non-null    string \ndtypes: float64(38), int32(6), string(2)\nmemory usage: 302.1 KB\n</pre> <p>El resultado es un conjunto de datos que no tiene valores faltantes.</p>"},{"location":"tratamiento-de-datos/02-valores-faltantes/#valores-faltantes","title":"Valores Faltantes\u00b6","text":"<p>Es los suficientemente com\u00fan, al trabajar con un problema de aprendizaje automatizado, el encontrarse que el conjunto de datos a utilizar tiene valores faltantes. Esto puede resultar en modelos que no funcionan correctamente, o en la imposibilidad de utilizar ciertos algoritmos. Por lo tanto, es importante saber c\u00f3mo manejar estos valores faltantes.</p> <p>Al encontrarse con valores faltantes, uno puede realizar cuatro posibles acciones:</p> <ul> <li>Dejarlos como est\u00e1n. El peor de los casos, ya que la mayor\u00eda de los algoritmos de aprendizaje automatizado no pueden trabajar con valores faltantes.</li> <li>Imputar valores faltantes. Esto significa reemplazar los valores faltantes con alg\u00fan valor. Por ejemplo, se puede reemplazar con la media de los valores de la columna, o la moda.</li> <li>Eliminar las filas que contienen valores faltantes. Esto puede ser una buena opci\u00f3n si el n\u00famero de filas con valores faltantes es peque\u00f1o.</li> <li>Eliminar las columnas que contienen valores faltantes. Si la columna tiene muchos valores faltantes, puede ser una buena opci\u00f3n eliminarla.</li> </ul> <p>Veamos como realizar varias de estas tareas.</p>"},{"location":"tratamiento-de-datos/02-valores-faltantes/#python","title":"Python\u00b6","text":""},{"location":"tratamiento-de-datos/02-valores-faltantes/#rapidminer","title":"RapidMiner\u00b6","text":"<p>En RapidMiner el proceso es complicado. Desafortunadamente no existen operadores que permitan remover columnas basados en cantidad o proporcion de valores faltantes. Para realizar esto crearemos un proceso que:</p> <ol> <li>Transponga el conjunto de datos. Permitiendonos trabajar con las columnas como filas.</li> <li>Cree dos nuevas columnas: una con la cantidad de datos faltantes por ejemplo, y otra con la cantidad de atributos totales.</li> <li>Elimine las filas que tengan mas del 30% de valores faltantes.</li> <li>Transponga el conjunto de datos nuevamente.</li> </ol> <p></p> <p>La definicion del proceso es la siguiente:</p> <pre>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;process version=\"10.2.000\"&gt;\n  &lt;operator activated=\"true\" class=\"subprocess\" compatibility=\"10.2.000\" expanded=\"true\" height=\"82\" name=\"Remover Columnas Vacias\" width=\"90\" x=\"179\" y=\"34\"&gt;\n    &lt;process expanded=\"true\"&gt;\n      &lt;operator activated=\"true\" class=\"blending:rename\" compatibility=\"10.2.000\" expanded=\"true\" height=\"82\" name=\"Rename\" width=\"90\" x=\"179\" y=\"34\"&gt;\n        &lt;list key=\"rename attributes\"&gt;\n          &lt;parameter key=\"id\" value=\"id_data\"/&gt;\n        &lt;/list&gt;\n        &lt;parameter key=\"from_attribute\" value=\"\"/&gt;\n        &lt;parameter key=\"to_attribute\" value=\"\"/&gt;\n      &lt;/operator&gt;\n      &lt;operator activated=\"true\" class=\"transpose\" compatibility=\"10.2.000\" expanded=\"true\" height=\"82\" name=\"Transpose\" width=\"90\" x=\"45\" y=\"136\"/&gt;\n      &lt;operator activated=\"true\" class=\"multiply\" compatibility=\"10.2.000\" expanded=\"true\" height=\"103\" name=\"Multiply\" width=\"90\" x=\"179\" y=\"136\"/&gt;\n      &lt;operator activated=\"true\" class=\"generate_aggregation\" compatibility=\"10.2.000\" expanded=\"true\" height=\"82\" name=\"Generate Aggregation\" width=\"90\" x=\"313\" y=\"34\"&gt;\n        &lt;parameter key=\"attribute_name\" value=\"no_missing_count\"/&gt;\n        &lt;parameter key=\"attribute_filter_type\" value=\"all\"/&gt;\n        &lt;parameter key=\"attribute\" value=\"\"/&gt;\n        &lt;parameter key=\"attributes\" value=\"\"/&gt;\n        &lt;parameter key=\"use_except_expression\" value=\"false\"/&gt;\n        &lt;parameter key=\"value_type\" value=\"attribute_value\"/&gt;\n        &lt;parameter key=\"use_value_type_exception\" value=\"false\"/&gt;\n        &lt;parameter key=\"except_value_type\" value=\"time\"/&gt;\n        &lt;parameter key=\"block_type\" value=\"attribute_block\"/&gt;\n        &lt;parameter key=\"use_block_type_exception\" value=\"false\"/&gt;\n        &lt;parameter key=\"except_block_type\" value=\"value_matrix_row_start\"/&gt;\n        &lt;parameter key=\"invert_selection\" value=\"false\"/&gt;\n        &lt;parameter key=\"include_special_attributes\" value=\"false\"/&gt;\n        &lt;parameter key=\"aggregation_function\" value=\"count\"/&gt;\n        &lt;parameter key=\"concatenation_separator\" value=\"|\"/&gt;\n        &lt;parameter key=\"keep_all\" value=\"true\"/&gt;\n        &lt;parameter key=\"ignore_missings\" value=\"true\"/&gt;\n        &lt;parameter key=\"ignore_missing_attributes\" value=\"true\"/&gt;\n      &lt;/operator&gt;\n      &lt;operator activated=\"true\" class=\"extract_macro\" compatibility=\"10.2.000\" expanded=\"true\" height=\"68\" name=\"Extract Macro\" width=\"90\" x=\"313\" y=\"136\"&gt;\n        &lt;parameter key=\"macro\" value=\"count\"/&gt;\n        &lt;parameter key=\"macro_type\" value=\"number_of_attributes\"/&gt;\n        &lt;parameter key=\"statistics\" value=\"average\"/&gt;\n        &lt;parameter key=\"attribute_name\" value=\"\"/&gt;\n        &lt;list key=\"additional_macros\"/&gt;\n      &lt;/operator&gt;\n      &lt;operator activated=\"true\" class=\"blending:generate_columns\" compatibility=\"10.2.000\" expanded=\"true\" height=\"82\" name=\"Generate Attributes (2)\" width=\"90\" x=\"447\" y=\"136\"&gt;\n        &lt;list key=\"function_descriptions\"&gt;\n          &lt;parameter key=\"attr_count\" value=\"%{count}\"/&gt;\n        &lt;/list&gt;\n        &lt;parameter key=\"keep_all_columns\" value=\"true\"/&gt;\n      &lt;/operator&gt;\n      &lt;operator activated=\"true\" class=\"concurrency:join\" compatibility=\"10.2.000\" expanded=\"true\" height=\"82\" name=\"Join\" width=\"90\" x=\"447\" y=\"34\"&gt;\n        &lt;parameter key=\"remove_double_attributes\" value=\"true\"/&gt;\n        &lt;parameter key=\"join_type\" value=\"inner\"/&gt;\n        &lt;parameter key=\"use_id_attribute_as_key\" value=\"false\"/&gt;\n        &lt;list key=\"key_attributes\"&gt;\n          &lt;parameter key=\"id\" value=\"id\"/&gt;\n        &lt;/list&gt;\n        &lt;parameter key=\"keep_both_join_attributes\" value=\"false\"/&gt;\n      &lt;/operator&gt;\n      &lt;operator activated=\"true\" class=\"guess_types\" compatibility=\"10.2.000\" expanded=\"true\" height=\"82\" name=\"Guess Types\" width=\"90\" x=\"581\" y=\"34\"&gt;\n        &lt;parameter key=\"attribute_filter_type\" value=\"single\"/&gt;\n        &lt;parameter key=\"attribute\" value=\"attr_count\"/&gt;\n        &lt;parameter key=\"attributes\" value=\"\"/&gt;\n        &lt;parameter key=\"use_except_expression\" value=\"false\"/&gt;\n        &lt;parameter key=\"value_type\" value=\"attribute_value\"/&gt;\n        &lt;parameter key=\"use_value_type_exception\" value=\"false\"/&gt;\n        &lt;parameter key=\"except_value_type\" value=\"time\"/&gt;\n        &lt;parameter key=\"block_type\" value=\"attribute_block\"/&gt;\n        &lt;parameter key=\"use_block_type_exception\" value=\"false\"/&gt;\n        &lt;parameter key=\"except_block_type\" value=\"value_matrix_row_start\"/&gt;\n        &lt;parameter key=\"invert_selection\" value=\"false\"/&gt;\n        &lt;parameter key=\"include_special_attributes\" value=\"false\"/&gt;\n        &lt;parameter key=\"decimal_point_character\" value=\".\"/&gt;\n      &lt;/operator&gt;\n      &lt;operator activated=\"true\" class=\"filter_examples\" compatibility=\"10.2.000\" expanded=\"true\" height=\"103\" name=\"Filter Examples\" width=\"90\" x=\"581\" y=\"136\"&gt;\n        &lt;parameter key=\"parameter_expression\" value=\"(no_missing_count / attr_count)&amp;gt;=0.5\"/&gt;\n        &lt;parameter key=\"condition_class\" value=\"expression\"/&gt;\n        &lt;parameter key=\"invert_filter\" value=\"false\"/&gt;\n        &lt;list key=\"filters_list\"/&gt;\n        &lt;parameter key=\"filters_logic_and\" value=\"true\"/&gt;\n        &lt;parameter key=\"filters_check_metadata\" value=\"true\"/&gt;\n      &lt;/operator&gt;\n      &lt;operator activated=\"true\" class=\"blending:select_attributes\" compatibility=\"10.2.000\" expanded=\"true\" height=\"82\" name=\"Select Attributes\" width=\"90\" x=\"715\" y=\"136\"&gt;\n        &lt;parameter key=\"type\" value=\"exclude attributes\"/&gt;\n        &lt;parameter key=\"attribute_filter_type\" value=\"a subset\"/&gt;\n        &lt;parameter key=\"select_attribute\" value=\"\"/&gt;\n        &lt;parameter key=\"select_subset\" value=\"attr_count\u241eno_missing_count\"/&gt;\n        &lt;parameter key=\"also_apply_to_special_attributes_(id,_label..)\" value=\"true\"/&gt;\n      &lt;/operator&gt;\n      &lt;operator activated=\"true\" class=\"transpose\" compatibility=\"10.2.000\" expanded=\"true\" height=\"82\" name=\"Transpose (2)\" width=\"90\" x=\"715\" y=\"34\"/&gt;\n      &lt;operator activated=\"true\" class=\"guess_types\" compatibility=\"10.2.000\" expanded=\"true\" height=\"82\" name=\"Guess Types (2)\" width=\"90\" x=\"849\" y=\"34\"&gt;\n        &lt;parameter key=\"attribute_filter_type\" value=\"all\"/&gt;\n        &lt;parameter key=\"attribute\" value=\"attr_count\"/&gt;\n        &lt;parameter key=\"attributes\" value=\"\"/&gt;\n        &lt;parameter key=\"use_except_expression\" value=\"false\"/&gt;\n        &lt;parameter key=\"value_type\" value=\"attribute_value\"/&gt;\n        &lt;parameter key=\"use_value_type_exception\" value=\"false\"/&gt;\n        &lt;parameter key=\"except_value_type\" value=\"time\"/&gt;\n        &lt;parameter key=\"block_type\" value=\"attribute_block\"/&gt;\n        &lt;parameter key=\"use_block_type_exception\" value=\"false\"/&gt;\n        &lt;parameter key=\"except_block_type\" value=\"value_matrix_row_start\"/&gt;\n        &lt;parameter key=\"invert_selection\" value=\"false\"/&gt;\n        &lt;parameter key=\"include_special_attributes\" value=\"false\"/&gt;\n        &lt;parameter key=\"decimal_point_character\" value=\".\"/&gt;\n      &lt;/operator&gt;\n      &lt;connect from_port=\"in 1\" to_op=\"Rename\" to_port=\"example set input\"/&gt;\n      &lt;connect from_op=\"Rename\" from_port=\"example set output\" to_op=\"Transpose\" to_port=\"example set input\"/&gt;\n      &lt;connect from_op=\"Transpose\" from_port=\"example set output\" to_op=\"Multiply\" to_port=\"input\"/&gt;\n      &lt;connect from_op=\"Multiply\" from_port=\"output 1\" to_op=\"Generate Aggregation\" to_port=\"example set input\"/&gt;\n      &lt;connect from_op=\"Multiply\" from_port=\"output 2\" to_op=\"Extract Macro\" to_port=\"example set\"/&gt;\n      &lt;connect from_op=\"Generate Aggregation\" from_port=\"example set output\" to_op=\"Join\" to_port=\"left\"/&gt;\n      &lt;connect from_op=\"Extract Macro\" from_port=\"example set\" to_op=\"Generate Attributes (2)\" to_port=\"table input\"/&gt;\n      &lt;connect from_op=\"Generate Attributes (2)\" from_port=\"table output\" to_op=\"Join\" to_port=\"right\"/&gt;\n      &lt;connect from_op=\"Join\" from_port=\"join\" to_op=\"Guess Types\" to_port=\"example set input\"/&gt;\n      &lt;connect from_op=\"Guess Types\" from_port=\"example set output\" to_op=\"Filter Examples\" to_port=\"example set input\"/&gt;\n      &lt;connect from_op=\"Filter Examples\" from_port=\"example set output\" to_op=\"Select Attributes\" to_port=\"example set input\"/&gt;\n      &lt;connect from_op=\"Select Attributes\" from_port=\"example set output\" to_op=\"Transpose (2)\" to_port=\"example set input\"/&gt;\n      &lt;connect from_op=\"Transpose (2)\" from_port=\"example set output\" to_op=\"Guess Types (2)\" to_port=\"example set input\"/&gt;\n      &lt;connect from_op=\"Guess Types (2)\" from_port=\"example set output\" to_port=\"out 1\"/&gt;\n      &lt;portSpacing port=\"source_in 1\" spacing=\"0\"/&gt;\n      &lt;portSpacing port=\"source_in 2\" spacing=\"0\"/&gt;\n      &lt;portSpacing port=\"sink_out 1\" spacing=\"0\"/&gt;\n      &lt;portSpacing port=\"sink_out 2\" spacing=\"0\"/&gt;\n    &lt;/process&gt;\n  &lt;/operator&gt;\n&lt;/process&gt;\n</pre> <p>Siguiente completamos los valores faltantes de los atributos numericos reales con la media, utilizando el operador <code>Replace Missing Values</code>.</p> <p></p> <p>Terminamos removiendo todos los ejemplos restantes con valores faltantes. Esto lo hacemos con el operador <code>Filter Examples</code>.</p> <p></p> <p>El resultado es un conjunto de datos que no tiene valores faltantes.</p>"},{"location":"tratamiento-de-datos/03-seleccion-atributos/","title":"Selecci\u00f3n de Atributos","text":"In\u00a0[34]: Copied! <pre>import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectFromModel\n</pre> import pandas as pd import numpy as np from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.feature_selection import SelectFromModel In\u00a0[35]: Copied! <pre>dataset: pd.DataFrame = pd.read_csv('./datasets/full.csv')\ndataset.info()\n</pre> dataset: pd.DataFrame = pd.read_csv('./datasets/full.csv') dataset.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 899 entries, 0 to 898\nData columns (total 77 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   dataset   899 non-null    object \n 1   id        899 non-null    int64  \n 2   ccf       899 non-null    int64  \n 3   age       899 non-null    int64  \n 4   sex       899 non-null    int64  \n 5   painloc   617 non-null    float64\n 6   painexer  617 non-null    float64\n 7   relrest   613 non-null    float64\n 8   pncaden   0 non-null      float64\n 9   cp        899 non-null    int64  \n 10  trestbps  840 non-null    float64\n 11  htn       865 non-null    float64\n 12  chol      869 non-null    float64\n 13  smoke     230 non-null    float64\n 14  cigs      479 non-null    float64\n 15  years     467 non-null    float64\n 16  fbs       809 non-null    float64\n 17  dm        95 non-null     float64\n 18  famhist   477 non-null    float64\n 19  restecg   897 non-null    float64\n 20  ekgmo     846 non-null    float64\n 21  ekgday    845 non-null    float64\n 22  ekgyr     846 non-null    float64\n 23  dig       831 non-null    float64\n 24  prop      833 non-null    float64\n 25  nitr      834 non-null    float64\n 26  pro       836 non-null    float64\n 27  diuretic  817 non-null    float64\n 28  proto     787 non-null    float64\n 29  thaldur   843 non-null    float64\n 30  thaltime  446 non-null    float64\n 31  met       794 non-null    float64\n 32  thalach   844 non-null    float64\n 33  thalrest  843 non-null    float64\n 34  tpeakbps  836 non-null    float64\n 35  tpeakbpd  836 non-null    float64\n 36  dummy     840 non-null    float64\n 37  trestbpd  840 non-null    float64\n 38  exang     844 non-null    float64\n 39  xhypo     841 non-null    float64\n 40  oldpeak   837 non-null    float64\n 41  slope     591 non-null    float64\n 42  rldv5     474 non-null    float64\n 43  rldv5e    757 non-null    float64\n 44  ca        291 non-null    float64\n 45  restckm   0 non-null      float64\n 46  exerckm   1 non-null      float64\n 47  restef    28 non-null     float64\n 48  restwm    30 non-null     float64\n 49  exeref    2 non-null      float64\n 50  exerwm    5 non-null      float64\n 51  thal      422 non-null    float64\n 52  thalsev   130 non-null    float64\n 53  thalpul   44 non-null     float64\n 54  earlobe   1 non-null      float64\n 55  cmo       888 non-null    float64\n 56  cday      890 non-null    float64\n 57  cyr       890 non-null    float64\n 58  num       899 non-null    int64  \n 59  lmt       624 non-null    float64\n 60  ladprox   663 non-null    float64\n 61  laddist   653 non-null    float64\n 62  diag      341 non-null    float64\n 63  cxmain    664 non-null    float64\n 64  ramus     332 non-null    float64\n 65  om1       628 non-null    float64\n 66  om2       327 non-null    float64\n 67  rcaprox   654 non-null    float64\n 68  rcadist   629 non-null    float64\n 69  lvx1      880 non-null    float64\n 70  lvx2      880 non-null    float64\n 71  lvx3      880 non-null    float64\n 72  lvx4      880 non-null    float64\n 73  lvf       883 non-null    float64\n 74  cathef    311 non-null    float64\n 75  junk      119 non-null    float64\n 76  name      899 non-null    object \ndtypes: float64(69), int64(6), object(2)\nmemory usage: 540.9+ KB\n</pre> In\u00a0[36]: Copied! <pre>correlation_matrix = dataset.drop(['num', 'ccf', 'name', 'dataset'], axis=1).corr().abs()\n\nupper = correlation_matrix.where(\n    np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n)\n\nprint(upper)\n</pre> correlation_matrix = dataset.drop(['num', 'ccf', 'name', 'dataset'], axis=1).corr().abs()  upper = correlation_matrix.where(     np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool) )  print(upper) <pre>          id       age       sex   painloc  painexer   relrest  pncaden  \\\nid       NaN  0.098213  0.067022  0.029687  0.135675  0.004992      NaN   \nage      NaN       NaN  0.063616  0.012959  0.193591  0.198020      NaN   \nsex      NaN       NaN       NaN  0.103689  0.222428  0.258928      NaN   \npainloc  NaN       NaN       NaN       NaN  0.269258  0.332300      NaN   \npainexer NaN       NaN       NaN       NaN       NaN  0.694375      NaN   \n...       ..       ...       ...       ...       ...       ...      ...   \nlvx3     NaN       NaN       NaN       NaN       NaN       NaN      NaN   \nlvx4     NaN       NaN       NaN       NaN       NaN       NaN      NaN   \nlvf      NaN       NaN       NaN       NaN       NaN       NaN      NaN   \ncathef   NaN       NaN       NaN       NaN       NaN       NaN      NaN   \njunk     NaN       NaN       NaN       NaN       NaN       NaN      NaN   \n\n                cp  trestbps       htn  ...       om2   rcaprox   rcadist  \\\nid        0.107745  0.008644  0.184405  ...  0.017602  0.154221  0.143603   \nage       0.160080  0.240246  0.153180  ...  0.047863  0.100287  0.067684   \nsex       0.178850  0.004917  0.034524  ...  0.004327  0.148485  0.117865   \npainloc   0.449421  0.000568  0.008231  ...  0.078934  0.056082  0.069040   \npainexer  0.834903  0.085713  0.016896  ...  0.052864  0.165407  0.143502   \n...            ...       ...       ...  ...       ...       ...       ...   \nlvx3           NaN       NaN       NaN  ...       NaN       NaN       NaN   \nlvx4           NaN       NaN       NaN  ...       NaN       NaN       NaN   \nlvf            NaN       NaN       NaN  ...       NaN       NaN       NaN   \ncathef         NaN       NaN       NaN  ...       NaN       NaN       NaN   \njunk           NaN       NaN       NaN  ...       NaN       NaN       NaN   \n\n              lvx1      lvx2      lvx3      lvx4       lvf    cathef      junk  \nid        0.197736  0.070122  0.037425  0.046351  0.183514  0.891962  0.026468  \nage       0.023271  0.066617  0.055995  0.111391  0.046576  0.269827  0.221078  \nsex       0.038458  0.041324  0.055161  0.148088  0.089861  0.189028  0.043138  \npainloc   0.025565  0.014885  0.028793  0.011222  0.044065  0.091442  0.036274  \npainexer  0.074662  0.053227  0.072625  0.157525  0.069229  0.045956  0.003137  \n...            ...       ...       ...       ...       ...       ...       ...  \nlvx3           NaN       NaN       NaN  0.458452  0.355248  0.008358  0.170833  \nlvx4           NaN       NaN       NaN       NaN  0.265997  0.177599  0.120731  \nlvf            NaN       NaN       NaN       NaN       NaN  0.128736  0.196370  \ncathef         NaN       NaN       NaN       NaN       NaN       NaN  0.292537  \njunk           NaN       NaN       NaN       NaN       NaN       NaN       NaN  \n\n[73 rows x 73 columns]\n</pre> <p>Aqui podemos ver la matriz de correlaci\u00f3n. Buscando remover atributos correlacionados podemos realizar lo siguiente:</p> In\u00a0[37]: Copied! <pre>umbral = 0.35\n\nto_drop = [column for column in upper.columns if any(upper[column] &gt; umbral)]\n\ndf = dataset.drop(dataset[to_drop], axis=1)\nprint(df.info())\n</pre> umbral = 0.35  to_drop = [column for column in upper.columns if any(upper[column] &gt; umbral)]  df = dataset.drop(dataset[to_drop], axis=1) print(df.info()) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 899 entries, 0 to 898\nData columns (total 33 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   dataset   899 non-null    object \n 1   id        899 non-null    int64  \n 2   ccf       899 non-null    int64  \n 3   age       899 non-null    int64  \n 4   sex       899 non-null    int64  \n 5   painloc   617 non-null    float64\n 6   painexer  617 non-null    float64\n 7   pncaden   0 non-null      float64\n 8   trestbps  840 non-null    float64\n 9   htn       865 non-null    float64\n 10  smoke     230 non-null    float64\n 11  fbs       809 non-null    float64\n 12  dm        95 non-null     float64\n 13  famhist   477 non-null    float64\n 14  restecg   897 non-null    float64\n 15  ekgmo     846 non-null    float64\n 16  ekgday    845 non-null    float64\n 17  dig       831 non-null    float64\n 18  prop      833 non-null    float64\n 19  nitr      834 non-null    float64\n 20  diuretic  817 non-null    float64\n 21  xhypo     841 non-null    float64\n 22  rldv5     474 non-null    float64\n 23  ca        291 non-null    float64\n 24  restckm   0 non-null      float64\n 25  exerckm   1 non-null      float64\n 26  earlobe   1 non-null      float64\n 27  num       899 non-null    int64  \n 28  om2       327 non-null    float64\n 29  lvx1      880 non-null    float64\n 30  lvx2      880 non-null    float64\n 31  lvx3      880 non-null    float64\n 32  name      899 non-null    object \ndtypes: float64(26), int64(5), object(2)\nmemory usage: 231.9+ KB\nNone\n</pre> <p>Esto resulto en una reducci\u00f3n del conjunto de datos de 72 atributos a 32., con un umbral de 0.35</p> In\u00a0[38]: Copied! <pre>dataset: pd.DataFrame = pd.read_csv('./datasets/clean.csv')\ndataset.info()\n</pre> dataset: pd.DataFrame = pd.read_csv('./datasets/clean.csv') dataset.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 572 entries, 0 to 571\nData columns (total 29 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   dataset   572 non-null    object \n 1   id        572 non-null    int64  \n 2   age       572 non-null    int64  \n 3   sex       572 non-null    int64  \n 4   painloc   572 non-null    float64\n 5   painexer  572 non-null    float64\n 6   trestbps  572 non-null    float64\n 7   htn       572 non-null    float64\n 8   chol      572 non-null    float64\n 9   fbs       572 non-null    float64\n 10  restecg   572 non-null    float64\n 11  ekgmo     572 non-null    int64  \n 12  ekgday    572 non-null    int64  \n 13  dig       572 non-null    float64\n 14  prop      572 non-null    float64\n 15  nitr      572 non-null    float64\n 16  pro       572 non-null    float64\n 17  diuretic  572 non-null    float64\n 18  thaldur   572 non-null    float64\n 19  tpeakbpd  572 non-null    float64\n 20  oldpeak   572 non-null    float64\n 21  num       572 non-null    int64  \n 22  lmt       572 non-null    float64\n 23  ladprox   572 non-null    float64\n 24  laddist   572 non-null    float64\n 25  cxmain    572 non-null    float64\n 26  om1       572 non-null    float64\n 27  rcaprox   572 non-null    float64\n 28  rcadist   572 non-null    float64\ndtypes: float64(22), int64(6), object(1)\nmemory usage: 129.7+ KB\n</pre> <p>Utilizaremos la libreria <code>sklearn</code> para realizar la optimizaci\u00f3n de atributos. En este caso utilizaremos el algoritmo <code>SelectFrommodel</code> que utiliza un modelo de aprendizaje para calcular los pesos de los atributos. En este caso utilizaremos un modelo de regresi\u00f3n log\u00edstica.</p> In\u00a0[39]: Copied! <pre>pipeline_for_selection = Pipeline([\n    ('scaler', StandardScaler()),\n    ('regression', LogisticRegression(max_iter=1000))\n])\n\ndef coef_getter(estimator):\n    return estimator.named_steps['regression'].coef_\n\nselector = SelectFromModel(estimator=pipeline_for_selection, threshold=0.2, importance_getter=coef_getter)\nselector.fit(dataset.drop(['num', 'dataset', 'id'], axis=1), dataset['num'])\n\nprint(np.where(selector.get_support())[0])\n</pre> pipeline_for_selection = Pipeline([     ('scaler', StandardScaler()),     ('regression', LogisticRegression(max_iter=1000)) ])  def coef_getter(estimator):     return estimator.named_steps['regression'].coef_  selector = SelectFromModel(estimator=pipeline_for_selection, threshold=0.2, importance_getter=coef_getter) selector.fit(dataset.drop(['num', 'dataset', 'id'], axis=1), dataset['num'])  print(np.where(selector.get_support())[0]) <pre>[ 1  6 12 15 16 18 20 21 22 23 24 25]\n</pre> <p>El resultado es una seleccion de 12 atributos, con un umbral de 0.2. Veamos cuales son:</p> In\u00a0[40]: Copied! <pre>print(dataset.drop(['num', 'dataset', 'id'], axis=1).columns[selector.get_support()])\n</pre> print(dataset.drop(['num', 'dataset', 'id'], axis=1).columns[selector.get_support()]) <pre>Index(['sex', 'chol', 'prop', 'diuretic', 'thaldur', 'oldpeak', 'ladprox',\n       'laddist', 'cxmain', 'om1', 'rcaprox', 'rcadist'],\n      dtype='object')\n</pre>"},{"location":"tratamiento-de-datos/03-seleccion-atributos/#seleccion-de-atributos","title":"Selecci\u00f3n de Atributos\u00b6","text":"<p>No todos los atributos de un conjunto de datos son \u00fatiles al momento de entrenar un modelo de aprendizaje automatizado. En especial en algoritmos param\u00e9tricos, como los lineales, algunos atributos pueden ser redundantes o incluso perjudiciales para el desempe\u00f1o del modelo. Por ejemplo, en un modelo de regresi\u00f3n lineal, si dos atributos est\u00e1n altamente correlacionados, el modelo puede tener problemas para encontrar los par\u00e1metros \u00f3ptimos. Por otro lado, si un atributo no tiene correlaci\u00f3n con la variable objetivo, el modelo puede tener un desempe\u00f1o pobre. Debido a esto, una tarea importante a realizar durante la etapa de preprocesamiento de datos, es la selecci\u00f3n de atributos.</p> <p>Esta selecci\u00f3n se puede realizar de forma manual, en base a conocimiento experto, o de forma autom\u00e1tica, utilizando algoritmos de selecci\u00f3n de atributos. Dentro de los m\u00e9todos automaticos, uno puede realizar una optimizaci\u00f3n de la selecci\u00f3n basada en probar combinaciones de atributos, quedandose con aquellos que mejor desempe\u00f1o entreguen. Otro m\u00e9todo es analizar por medio de una matriz de correlaci\u00f3n la relaci\u00f3n entre los atributos, eliminando aquellos que est\u00e9n altamente correlacionados. Finalmente, se pueden utilizar algoritmos de selecci\u00f3n de atributos, que calculan pesos para cada atributo, y se quedan con aquellos que tengan mayor peso o superen un umbral.</p>"},{"location":"tratamiento-de-datos/03-seleccion-atributos/#atributos-correlacionados","title":"Atributos Correlacionados\u00b6","text":""},{"location":"tratamiento-de-datos/03-seleccion-atributos/#python","title":"Python\u00b6","text":"<p>Carguemos un conjunto de datos y veamos como se comportan los atributos entre si.</p>"},{"location":"tratamiento-de-datos/03-seleccion-atributos/#rapidminer","title":"RapidMiner\u00b6","text":"<p>En RapidMiner el proceso es muy sencillo gracias al operador <code>Remove Correlated Attributes</code>.</p> <p></p>"},{"location":"tratamiento-de-datos/03-seleccion-atributos/#optimizacion-de-atributos","title":"Optimizaci\u00f3n de Atributos\u00b6","text":""},{"location":"tratamiento-de-datos/03-seleccion-atributos/#python","title":"Python\u00b6","text":"<p>Volvamos a cargar los datos e intentemos optimizar la selecci\u00f3n de atributos basandonos en pesos.</p>"},{"location":"tratamiento-de-datos/03-seleccion-atributos/#rapidminer","title":"RapidMiner\u00b6","text":"<p>En RapidMiner podemos utilizar el operador <code>Optimize Selection</code> en combinacion con el operador <code>Select By Weights</code>. El primero requiere un modelo de aprendizaje, en este caso utilizaremos un modelo de regresi\u00f3n log\u00edstica. El segundo requiere un umbral, en este caso utilizaremos 0.2.</p> <p> </p> <p>La selecci\u00f3n resultante es de 8 atributos:</p> <p></p>"},{"location":"tratamiento-de-datos/04-datos-anomalos/","title":"Datos An\u00f3malos","text":"In\u00a0[23]: Copied! <pre>import pandas as pd\nimport numpy as np\nfrom sklearn.neighbors import LocalOutlierFactor\n</pre> import pandas as pd import numpy as np from sklearn.neighbors import LocalOutlierFactor In\u00a0[24]: Copied! <pre>dataset: pd.DataFrame = pd.read_csv('./datasets/clean.csv')\ndataset.info()\n</pre> dataset: pd.DataFrame = pd.read_csv('./datasets/clean.csv') dataset.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 572 entries, 0 to 571\nData columns (total 29 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   dataset   572 non-null    object \n 1   id        572 non-null    int64  \n 2   age       572 non-null    int64  \n 3   sex       572 non-null    int64  \n 4   painloc   572 non-null    float64\n 5   painexer  572 non-null    float64\n 6   trestbps  572 non-null    float64\n 7   htn       572 non-null    float64\n 8   chol      572 non-null    float64\n 9   fbs       572 non-null    float64\n 10  restecg   572 non-null    float64\n 11  ekgmo     572 non-null    int64  \n 12  ekgday    572 non-null    int64  \n 13  dig       572 non-null    float64\n 14  prop      572 non-null    float64\n 15  nitr      572 non-null    float64\n 16  pro       572 non-null    float64\n 17  diuretic  572 non-null    float64\n 18  thaldur   572 non-null    float64\n 19  tpeakbpd  572 non-null    float64\n 20  oldpeak   572 non-null    float64\n 21  num       572 non-null    int64  \n 22  lmt       572 non-null    float64\n 23  ladprox   572 non-null    float64\n 24  laddist   572 non-null    float64\n 25  cxmain    572 non-null    float64\n 26  om1       572 non-null    float64\n 27  rcaprox   572 non-null    float64\n 28  rcadist   572 non-null    float64\ndtypes: float64(22), int64(6), object(1)\nmemory usage: 129.7+ KB\n</pre> <p>Este conjunto de datos no tiene valores faltantes, pero desconocemos si tiene valores at\u00edpicos. Para calular el valor de anomal\u00eda de cada dato, utilizaremos la clase <code>LocalOutlierFactor</code> de la librer\u00eda <code>sklearn</code>.</p> In\u00a0[25]: Copied! <pre>lof = LocalOutlierFactor(n_neighbors=20)\nlof.fit(dataset.drop(['id', 'num', 'dataset'], axis=1))\ndataset['outlier'] = -lof.negative_outlier_factor_\n\nprint(dataset.info())\n</pre> lof = LocalOutlierFactor(n_neighbors=20) lof.fit(dataset.drop(['id', 'num', 'dataset'], axis=1)) dataset['outlier'] = -lof.negative_outlier_factor_  print(dataset.info()) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 572 entries, 0 to 571\nData columns (total 30 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   dataset   572 non-null    object \n 1   id        572 non-null    int64  \n 2   age       572 non-null    int64  \n 3   sex       572 non-null    int64  \n 4   painloc   572 non-null    float64\n 5   painexer  572 non-null    float64\n 6   trestbps  572 non-null    float64\n 7   htn       572 non-null    float64\n 8   chol      572 non-null    float64\n 9   fbs       572 non-null    float64\n 10  restecg   572 non-null    float64\n 11  ekgmo     572 non-null    int64  \n 12  ekgday    572 non-null    int64  \n 13  dig       572 non-null    float64\n 14  prop      572 non-null    float64\n 15  nitr      572 non-null    float64\n 16  pro       572 non-null    float64\n 17  diuretic  572 non-null    float64\n 18  thaldur   572 non-null    float64\n 19  tpeakbpd  572 non-null    float64\n 20  oldpeak   572 non-null    float64\n 21  num       572 non-null    int64  \n 22  lmt       572 non-null    float64\n 23  ladprox   572 non-null    float64\n 24  laddist   572 non-null    float64\n 25  cxmain    572 non-null    float64\n 26  om1       572 non-null    float64\n 27  rcaprox   572 non-null    float64\n 28  rcadist   572 non-null    float64\n 29  outlier   572 non-null    float64\ndtypes: float64(23), int64(6), object(1)\nmemory usage: 134.2+ KB\nNone\n</pre> <p>Ahora lo que resta es eliminar las columnas que tengan un valor de anomal\u00eda superior a un umbral. Utilizemos un umbral de 1.3.</p> In\u00a0[26]: Copied! <pre>dataset = dataset[dataset['outlier'] &lt;= 1.3]\ndataset = dataset.drop(['outlier'], axis=1)\nprint(dataset.info())\n</pre> dataset = dataset[dataset['outlier'] &lt;= 1.3] dataset = dataset.drop(['outlier'], axis=1) print(dataset.info()) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 522 entries, 0 to 571\nData columns (total 29 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   dataset   522 non-null    object \n 1   id        522 non-null    int64  \n 2   age       522 non-null    int64  \n 3   sex       522 non-null    int64  \n 4   painloc   522 non-null    float64\n 5   painexer  522 non-null    float64\n 6   trestbps  522 non-null    float64\n 7   htn       522 non-null    float64\n 8   chol      522 non-null    float64\n 9   fbs       522 non-null    float64\n 10  restecg   522 non-null    float64\n 11  ekgmo     522 non-null    int64  \n 12  ekgday    522 non-null    int64  \n 13  dig       522 non-null    float64\n 14  prop      522 non-null    float64\n 15  nitr      522 non-null    float64\n 16  pro       522 non-null    float64\n 17  diuretic  522 non-null    float64\n 18  thaldur   522 non-null    float64\n 19  tpeakbpd  522 non-null    float64\n 20  oldpeak   522 non-null    float64\n 21  num       522 non-null    int64  \n 22  lmt       522 non-null    float64\n 23  ladprox   522 non-null    float64\n 24  laddist   522 non-null    float64\n 25  cxmain    522 non-null    float64\n 26  om1       522 non-null    float64\n 27  rcaprox   522 non-null    float64\n 28  rcadist   522 non-null    float64\ndtypes: float64(22), int64(6), object(1)\nmemory usage: 122.3+ KB\nNone\n</pre> <p>El conjunto de datos se reducio de 572 ejemplos a 522 ejemplos.</p>"},{"location":"tratamiento-de-datos/04-datos-anomalos/#datos-anomalos","title":"Datos An\u00f3malos\u00b6","text":"<p>Un modelo de aprendizaje automatizado aprende a partir de datos conocidos, por lo que es importante que los datos de entrenamiento sean representativos de los datos que se encontrar\u00e1n en el mundo real. Si los datos de entrenamiento contienen valores at\u00edpicos, el modelo puede aprender a predecir valores at\u00edpicos. Esto puede ser un problema si los valores at\u00edpicos son datos incorrectos o si son datos correctos pero poco frecuentes.</p> <p>Para mejorar la calidad de las predicciones de un modelo, es importante identificar y eliminar los valores at\u00edpicos de los datos de entrenamiento. Esto se puede realizar de varias formas, aqu\u00ed veremos como utilizar el m\u00e9todo de <code>Local Outlier Factor</code> para asignar a cada dato un valor de anomal\u00eda. Luego, y a partir de este dato, podemos eliminar los datos que tengan un valor de anomal\u00eda superior a un umbral.</p>"},{"location":"tratamiento-de-datos/04-datos-anomalos/#python","title":"Python\u00b6","text":"<p>Carguemos un conjunto de datos e identifiquemos los valores at\u00edpicos.</p>"},{"location":"tratamiento-de-datos/04-datos-anomalos/#rapidminer","title":"RapidMiner\u00b6","text":"<p>Dentro de RapidMiner, el proceso es similar. Utilizamos primero el operador <code>Detect Outliers (LOF)</code>  para obtener el valor de anomal\u00eda de cada dato.</p> <p></p> <p>Luego utilizamos el operador <code>Filter Examples</code> para eliminar los datos que tengan un valor de anomal\u00eda superior al umbral predeterminado. En nuestro caso 1.3.</p> <p></p> <p>El resultado en este caso es un conjunto de datos de 458 ejemplos.</p> <p></p>"},{"location":"tratamiento-de-datos/05-escalado/","title":"Escalado","text":"In\u00a0[27]: Copied! <pre>import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nimport matplotlib.pyplot as plt\n</pre> import pandas as pd import numpy as np from sklearn.preprocessing import MinMaxScaler, StandardScaler import matplotlib.pyplot as plt In\u00a0[28]: Copied! <pre>dataset: pd.DataFrame = pd.read_csv('./datasets/clean.csv')\ndataset.info()\n</pre> dataset: pd.DataFrame = pd.read_csv('./datasets/clean.csv') dataset.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 572 entries, 0 to 571\nData columns (total 29 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   dataset   572 non-null    object \n 1   id        572 non-null    int64  \n 2   age       572 non-null    int64  \n 3   sex       572 non-null    int64  \n 4   painloc   572 non-null    float64\n 5   painexer  572 non-null    float64\n 6   trestbps  572 non-null    float64\n 7   htn       572 non-null    float64\n 8   chol      572 non-null    float64\n 9   fbs       572 non-null    float64\n 10  restecg   572 non-null    float64\n 11  ekgmo     572 non-null    int64  \n 12  ekgday    572 non-null    int64  \n 13  dig       572 non-null    float64\n 14  prop      572 non-null    float64\n 15  nitr      572 non-null    float64\n 16  pro       572 non-null    float64\n 17  diuretic  572 non-null    float64\n 18  thaldur   572 non-null    float64\n 19  tpeakbpd  572 non-null    float64\n 20  oldpeak   572 non-null    float64\n 21  num       572 non-null    int64  \n 22  lmt       572 non-null    float64\n 23  ladprox   572 non-null    float64\n 24  laddist   572 non-null    float64\n 25  cxmain    572 non-null    float64\n 26  om1       572 non-null    float64\n 27  rcaprox   572 non-null    float64\n 28  rcadist   572 non-null    float64\ndtypes: float64(22), int64(6), object(1)\nmemory usage: 129.7+ KB\n</pre> In\u00a0[29]: Copied! <pre>scaler = MinMaxScaler()\ndataset_scaled = scaler.fit_transform(dataset.drop(['id', 'num', 'dataset'], axis=1))\n\ndataset_scaled = pd.DataFrame(dataset_scaled, columns=dataset.drop(['id', 'num', 'dataset'], axis=1).columns)\ndataset_scaled.head()\n</pre> scaler = MinMaxScaler() dataset_scaled = scaler.fit_transform(dataset.drop(['id', 'num', 'dataset'], axis=1))  dataset_scaled = pd.DataFrame(dataset_scaled, columns=dataset.drop(['id', 'num', 'dataset'], axis=1).columns) dataset_scaled.head() Out[29]: age sex painloc painexer trestbps htn chol fbs restecg ekgmo ... thaldur tpeakbpd oldpeak lmt ladprox laddist cxmain om1 rcaprox rcadist 0 0.708333 1.0 1.0 1.0 0.725 1.0 0.413121 1.0 1.0 0.090909 ... 0.666667 0.724771 0.524390 0.006173 0.0 0.0 0.0 0.0 0.0 0.0 1 0.791667 1.0 1.0 1.0 0.800 1.0 0.507092 0.0 1.0 0.181818 ... 0.592593 0.724771 0.426829 0.006173 1.0 1.0 1.0 0.0 0.0 0.0 2 0.791667 1.0 1.0 1.0 0.600 1.0 0.406028 0.0 1.0 0.090909 ... 0.518519 0.633028 0.560976 0.006173 0.0 0.0 0.0 0.0 1.0 1.0 3 0.166667 1.0 1.0 1.0 0.650 0.0 0.443262 0.0 0.0 0.090909 ... 0.851852 0.522936 0.670732 0.006173 0.0 0.0 0.0 0.0 0.0 0.0 4 0.250000 0.0 1.0 1.0 0.650 1.0 0.361702 0.0 1.0 0.090909 ... 0.407407 0.577982 0.414634 0.006173 0.0 0.0 0.0 0.0 0.0 0.0 <p>5 rows \u00d7 26 columns</p> <p>Grafiquemos histogramas para verificar las escalas.</p> In\u00a0[30]: Copied! <pre>for col in dataset_scaled.columns:\n    if len(dataset_scaled[col].unique()) &lt;= 3:\n        dataset_scaled.drop(col, axis=1, inplace=True)\n\nfig, axs = plt.subplots(1, 5, figsize=(20, 5))\nfor i in range(5):\n    j = np.random.randint(0, len(dataset_scaled.columns))\n    axs[i].hist(dataset_scaled[dataset_scaled.columns[j]])\n    axs[i].set_title(dataset_scaled.columns[j])\nplt.show()\n</pre> for col in dataset_scaled.columns:     if len(dataset_scaled[col].unique()) &lt;= 3:         dataset_scaled.drop(col, axis=1, inplace=True)  fig, axs = plt.subplots(1, 5, figsize=(20, 5)) for i in range(5):     j = np.random.randint(0, len(dataset_scaled.columns))     axs[i].hist(dataset_scaled[dataset_scaled.columns[j]])     axs[i].set_title(dataset_scaled.columns[j]) plt.show() <p>Podemos ver como todas las variables se encuentran en el rango [0, 1]. Provemos ahora con la estandarizaci\u00f3n.</p> In\u00a0[31]: Copied! <pre>dataset: pd.DataFrame = pd.read_csv('./datasets/clean.csv')\n\nscaler = StandardScaler()\ndataset_scaled = scaler.fit_transform(dataset.drop(['id', 'num', 'dataset'], axis=1))\n\ndataset_scaled = pd.DataFrame(dataset_scaled, columns=dataset.drop(['id', 'num', 'dataset'], axis=1).columns)\ndataset_scaled.head()\n</pre> dataset: pd.DataFrame = pd.read_csv('./datasets/clean.csv')  scaler = StandardScaler() dataset_scaled = scaler.fit_transform(dataset.drop(['id', 'num', 'dataset'], axis=1))  dataset_scaled = pd.DataFrame(dataset_scaled, columns=dataset.drop(['id', 'num', 'dataset'], axis=1).columns) dataset_scaled.head() Out[31]: age sex painloc painexer trestbps htn chol fbs restecg ekgmo ... thaldur tpeakbpd oldpeak lmt ladprox laddist cxmain om1 rcaprox rcadist 0 0.739886 0.471405 0.2 0.39736 0.669783 0.909657 0.417507 2.004381 1.368567 -1.089578 ... 1.102557 0.510183 1.200519 -0.048076 -0.612372 -0.528812 -0.571967 -0.434959 -0.661200 -0.411988 1 1.188681 0.471405 0.2 0.39736 1.483742 0.909657 0.879975 -0.498907 1.368567 -0.808079 ... 0.707860 0.510183 0.445150 -0.048076 1.632993 1.891031 1.748353 -0.434959 -0.661200 -0.411988 2 1.188681 0.471405 0.2 0.39736 -0.686815 0.909657 0.382604 -0.498907 1.368567 -1.089578 ... 0.313162 -0.240371 1.483783 -0.048076 -0.612372 -0.528812 -0.571967 -0.434959 1.512401 2.427255 3 -2.177288 0.471405 0.2 0.39736 -0.144176 -1.099315 0.565846 -0.498907 -0.933845 -1.089578 ... 2.089300 -1.141036 2.333573 -0.048076 -0.612372 -0.528812 -0.571967 -0.434959 -0.661200 -0.411988 4 -1.728492 -2.121320 0.2 0.39736 -0.144176 0.909657 0.164458 -0.498907 1.368567 -1.089578 ... -0.278883 -0.690704 0.350729 -0.048076 -0.612372 -0.528812 -0.571967 -0.434959 -0.661200 -0.411988 <p>5 rows \u00d7 26 columns</p> In\u00a0[32]: Copied! <pre>for col in dataset_scaled.columns:\n    if len(dataset_scaled[col].unique()) &lt;= 3:\n        dataset_scaled.drop(col, axis=1, inplace=True)\n\nfig, axs = plt.subplots(1, 5, figsize=(20, 5))\nfor i in range(5):\n    j = np.random.randint(0, len(dataset_scaled.columns))\n    axs[i].hist(dataset_scaled[dataset_scaled.columns[j]])\n    axs[i].set_title(dataset_scaled.columns[j])\nplt.show()\n</pre> for col in dataset_scaled.columns:     if len(dataset_scaled[col].unique()) &lt;= 3:         dataset_scaled.drop(col, axis=1, inplace=True)  fig, axs = plt.subplots(1, 5, figsize=(20, 5)) for i in range(5):     j = np.random.randint(0, len(dataset_scaled.columns))     axs[i].hist(dataset_scaled[dataset_scaled.columns[j]])     axs[i].set_title(dataset_scaled.columns[j]) plt.show() <p>Se puede apreciar en las graficas como la estandarizaci\u00f3n llevo la distribuci\u00f3n de las variables a una distribuci\u00f3n normal con media 0 y varianza 1.</p>"},{"location":"tratamiento-de-datos/05-escalado/#escalado","title":"Escalado\u00b6","text":"<p>Raramente nos encontramos con un conjunto de datos donde las variables compartan su rango. Una edad puede variar de 0 a cerca de 100, mientras la altura de una persona puede variar desde 1 a 2 metros. Esto puede no ser un problema, pero para algunos algoritmos de aprendizaje automatizado, lo es. Por ejemplo, la aplicaci\u00f3n de algoritmos basados en distancias, como k-NN, o aquellos que requieran distribuciones normales con media y varianza similares, puede resultar en un mal rendimiento.</p> <p>Para resolver este problema, se suele utilizar transformaciones que escalan las variables a un rango com\u00fan. Las m\u00e1s comunes son:</p> <ul> <li>Normalizaci\u00f3n Min-Max: Escala las variables entre 0 y 1. Se calcula como:</li> </ul> <p>$$x_{norm} = \\frac{x - x_{min}}{x_{max} - x_{min}}$$</p> <ul> <li>Estandarizaci\u00f3n (Z-score normalization): Escala las variables para que tengan distribuci\u00f3n normal, con media 0 y varianza 1. Se calcula como:</li> </ul> <p>$$x_{std} = \\frac{x - \\mu}{\\sigma}$$</p> <p>Veamos como realizar estas transformaciones sobre un conjunto de datos.</p>"},{"location":"tratamiento-de-datos/05-escalado/#python","title":"Python\u00b6","text":"<p>En Python, podemos utilizar la clase <code>MinMaxScaler</code> del m\u00f3dulo <code>preprocessing</code> de la librer\u00eda <code>scikit-learn</code> para realizar la normalizaci\u00f3n Min-Max. Para la estandarizaci\u00f3n, podemos utilizar la clase <code>StandardScaler</code> del mismo m\u00f3dulo.</p>"},{"location":"tratamiento-de-datos/05-escalado/#rapidminer","title":"RapidMiner\u00b6","text":"<p>En RapidMiner, podemos utilizar el operador <code>Normalize</code> para realizar la normalizaci\u00f3n, tanto Min-Max como Z-score. Para ello, debemos seleccionar las variables a normalizar y el tipo de normalizaci\u00f3n a realizar.</p> <p></p> <p>El resultado se puede observar en los minimos y maximos de la pesta\u00f1a de estad\u00edstica.</p> <p></p> <p>Para escoger el tipo de normalizaci\u00f3n a utilizar, debemos seleccionar el tipo de normalizaci\u00f3n en el par\u00e1metro <code>method</code>.</p> <p></p>"},{"location":"validacion/00-introduccion/","title":"Introducci\u00f3n","text":""},{"location":"validacion/00-introduccion/#introduccion","title":"Introducci\u00f3n\u00b6","text":"<p>Un modelo de aprendizaje automatizado es un modelo predictivo. Se comienza con una tabla de $n$ columnas $X$, y una columna especial $Y$, y el modelo aprende a predecir $Y$ a partir de $X$. En el caso de la regresi\u00f3n, $Y$ es una variable num\u00e9rica, mientras que en la clasificaci\u00f3n $Y$ es una variable categ\u00f3rica.</p> <p>No solo se debe requerir que el modelo no cometa errores en los datos de entrenamiento, sino que tambi\u00e9n se debe requerir que el modelo sea capaz de generalizar a datos nuevos.</p>"},{"location":"validacion/00-introduccion/#contenidos","title":"Contenidos\u00b6","text":"<ol> <li>Error de entrenamiento vs error de prueba</li> <li>Validaci\u00f3n dividida</li> <li>Validaci\u00f3n cruzada</li> </ol>"},{"location":"validacion/01-error-prueba-vs-entrenamiento/","title":"Error de Entrenamiento vs Error de Prueba","text":""},{"location":"validacion/01-error-prueba-vs-entrenamiento/#error-de-entrenamiento-vs-error-de-prueba","title":"Error de Entrenamiento vs Error de Prueba\u00b6","text":"<p>Es importante, luego de entrenar un modelo, evaluar su desempe\u00f1o, y esto lo hacemos midiendo su precision. Con este dato, es posible seleccionar el modelo que mejor se ajuste a los datos y a el problema en cuesti\u00f3n.</p> <p>Para calcular la precision, primero es necesario entrenar el modelo con un conjunto de datos de entrenamiento. Luego, utilizamos el modelo con un conjunto de datos que ya posea determinada la variable de salida, y comparamos el resultado obtenido con el valor real de la variable de salida.</p> <p>Aqui surgen dos conceptos:</p> <ul> <li>Error de Entrenamiento: Es el error que se obtiene al evaluar el modelo con los mismos datos con los que se entreno.</li> <li>Error de Prueba: Es el error que se obtiene al evaluar el modelo con datos que no se utilizaron para entrenarlo. Requiere un segundo conjunto de datos disjunto del conjunto de entrenamiento. Al hacer esta distincion, pasamos a operar con dos conjuntos de datos: el de entrenamiento y el de prueba.</li> </ul> <p>En relaci\u00f3n al error de entrenamiento, podemos decir que la precision del modelo puede ser muy alta, pero esto no significa que el modelo sea bueno. Esto se debe a que el modelo puede estar sobreajustado a los datos de entrenamiento, y por lo tanto, no generalizar bien.</p> <p>Un ejemplo de esto es el algoritmo kNN, que al tener un k = 1, siempre va a predecir el valor de la variable de salida con el valor de la variable de salida del dato mas cercano. Esto hace que el error de entrenamiento sea 0, obteniendo una precision del 100%, sin saber si el modelo es bueno o no.</p> <p>Por esto es importante evaluar el error de prueba e ignorar el error de entrenamiento. Si el error de prueba es alto, significa que el modelo no generaliza bien, y por lo tanto, no es bueno.</p>"},{"location":"validacion/02-validacion-dividida/","title":"Validaci\u00f3n Dividida","text":""},{"location":"validacion/02-validacion-dividida/#validacion-dividida","title":"Validaci\u00f3n Dividida\u00b6","text":"<p>Al momento de validar un modelo, puede no ser posible obtener un conjunto de datos adicional para validar el modelo. En algunos casos, el obtener mas datos puede ser muy costoso o simplemente no es posible. En estos casos, se puede utilizar una t\u00e9cnica llamada validaci\u00f3n dividida.</p> <p>La validaci\u00f3n dividida consiste en dividir el conjunto de datos en dos partes, una para entrenar el modelo y otra para validar el modelo.</p> <p>Para realizar esta division del conjunto, es necesario considerar que el entrenamiento es mas efectivo cuantos mas ejemplos existan en el conjunto de entrenamiento. Debido a esto, se recomienda dividir el conjunto de datos en 70% para entrenamiento y 30% para validaci\u00f3n.</p>"},{"location":"validacion/03-validacion-cruzada/","title":"Validaci\u00f3n Cruzada","text":""},{"location":"validacion/03-validacion-cruzada/#validacion-cruzada","title":"Validaci\u00f3n Cruzada\u00b6","text":"<p>Es posible que al aplicar el m\u00e9todo de validaci\u00f3n dividida, el conjunto de datos de entrenamiento no sea representativo de la poblaci\u00f3n, y que el modelo resulte sobre ajustado. Para prevenir esto, podemos utilizar otro m\u00e9todo de validaci\u00f3n llamado validaci\u00f3n cruzada.</p> <p>La validaci\u00f3n cruzada consiste en dividir el conjunto de datos en $k$ subconjuntos, y realizar $k$ iteraciones. En cada iteraci\u00f3n, se utiliza un subconjunto como conjunto de prueba, y los restantes como conjunto de entrenamiento. Al finalizar las iteraciones, se promedian los resultados obtenidos en cada iteraci\u00f3n.</p> <p>Este metodo se llama validaci\u00f3n cruzada en $k$ iteraciones, o $k$-fold cross validation, y es el m\u00e9todo de validaci\u00f3n mas utilizado cuando se dispone de un conjunto de datos peque\u00f1o, o cuando buscamos prevenir el sobre ajuste.</p> <p>Una desventaja de este m\u00e9todo es que requiere entrenar el modelo $k$ veces, lo cual puede ser costoso computacionalmente.</p>"},{"location":"validacion/04-contaminacion-accidental/","title":"Contaminaci\u00f3n Accidental","text":""},{"location":"validacion/04-contaminacion-accidental/#contaminacion-accidental","title":"Contaminaci\u00f3n Accidental\u00b6","text":"<p>El concepto central detr\u00e1s de dividir el conjunto de datos en dos conjuntos, uno de entrenamiento y uno de prueba, es el prevenir que el conjunto de datos de prueba y el conjunto de datos de entrenamiento compartan informaci\u00f3n que, resultando en sobreajuste, lleve a una evaluaci\u00f3n optimista del modelo.</p> <p>Pero incluso al separar los conjuntos, es posible que se mezcle informacion de uno en el otro. Si no se toman ciertas precauciones, la utilizaci\u00f3n de metodos de validaci\u00f3n dividida o cruzada, todav\u00eda pueden llevar a una evaluaci\u00f3n optimista del modelo.</p> <p>Existen varias formas de contaminar el conjunto de datos de prueba con informaci\u00f3n del conjunto de entrenamiento. Dentro de estas podemos encontrar</p> <ul> <li>Contaminaci\u00f3n por Normalizaci\u00f3n</li> </ul> <p>Este tipo de contaminaci\u00f3n ocurre cuando la normalizaci\u00f3n se realiza sobre el conjunto de datos completo, en lugar de sobre cada conjunto por separado. Informaci\u00f3n de la media y la desviaci\u00f3n estandar del conjunto de entrenamiento se mezclan con el conjunto de prueba, resultando en contaminaci\u00f3n incluso al dividirlos.</p> <p>Para evitar esto, es preciso realizar la normalizaci\u00f3n sobre cada conjunto por separado.</p> <ul> <li>Contaminaci\u00f3n por Optimizaci\u00f3n de Par\u00e1metros</li> </ul> <p>Al realizar optimizaci\u00f3n sobre los parametros utilizados por el modelo, como lo son $k$ en kNN o los arboles en un bosque aleatorio, se puede caer en el error de utilizar el conjunto de prueba para seleccionar los parametros que mejor se ajustan al modelo. Esto lleva a que el modelo posea un sesgo direccionado hacia el conjunto de prueba.</p> <p>Para resolver esto, podemos utilizar validaci\u00f3n cruzada anidada, donde primero se utiliza validaci\u00f3n cruzada para seleccionar los parametros, y luego se utiliza validaci\u00f3n cruzada para evaluar el modelo con los parametros seleccionados.</p> <ul> <li>Contaminaci\u00f3n por Selecci\u00f3n de Variables</li> </ul> <p>Al realizar selecci\u00f3n de variables, se puede caer en el error de utilizar el conjunto de prueba para seleccionar las variables que mejor se ajustan al modelo. Seleccionar las variables es parte misma del proceso de entrenar el modelo, por lo que debe ser propiamente validado.</p> <p>Esto lo podemos realizar nuevamente utilizando validaci\u00f3n cruzada anidada, donde primero se utiliza validaci\u00f3n cruzada para seleccionar las variables, y luego se utiliza validaci\u00f3n cruzada para evaluar el modelo con las variables seleccionadas.</p>"}]}